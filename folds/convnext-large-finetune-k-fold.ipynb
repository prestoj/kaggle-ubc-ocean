{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82500098-49ea-4a7e-bb63-ce5130f954cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>label</th>\n",
       "      <th>image_width</th>\n",
       "      <th>image_height</th>\n",
       "      <th>is_tma</th>\n",
       "      <th>tile_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>HGSC</td>\n",
       "      <td>23785</td>\n",
       "      <td>20008</td>\n",
       "      <td>False</td>\n",
       "      <td>../tiles_768/4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>66</td>\n",
       "      <td>LGSC</td>\n",
       "      <td>48871</td>\n",
       "      <td>48195</td>\n",
       "      <td>False</td>\n",
       "      <td>../tiles_768/66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>91</td>\n",
       "      <td>HGSC</td>\n",
       "      <td>3388</td>\n",
       "      <td>3388</td>\n",
       "      <td>True</td>\n",
       "      <td>../tiles_768/91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>281</td>\n",
       "      <td>LGSC</td>\n",
       "      <td>42309</td>\n",
       "      <td>15545</td>\n",
       "      <td>False</td>\n",
       "      <td>../tiles_768/281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>286</td>\n",
       "      <td>EC</td>\n",
       "      <td>37204</td>\n",
       "      <td>30020</td>\n",
       "      <td>False</td>\n",
       "      <td>../tiles_768/286</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   image_id label  image_width  image_height  is_tma         tile_path\n",
       "0         4  HGSC        23785         20008   False    ../tiles_768/4\n",
       "1        66  LGSC        48871         48195   False   ../tiles_768/66\n",
       "2        91  HGSC         3388          3388    True   ../tiles_768/91\n",
       "3       281  LGSC        42309         15545   False  ../tiles_768/281\n",
       "4       286    EC        37204         30020   False  ../tiles_768/286"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def get_image_path(image_id:int):\n",
    "    return os.path.join('../tiles_768', str(image_id))\n",
    "\n",
    "I_FOLD = 3\n",
    "train = pd.read_csv(f\"train_fold_{I_FOLD}.csv\")\n",
    "validation = pd.read_csv(f\"val_fold_{I_FOLD}.csv\")\n",
    "\n",
    "train['tile_path'] = train['image_id'].apply(lambda x: get_image_path(x))\n",
    "validation['tile_path'] = validation['image_id'].apply(lambda x: get_image_path(x))\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a53ed4b-02fa-451d-b948-08167c29f05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# FOR FOLD_3 I USE DROP PATH RATE = 0.5\n",
    "# \"\"\"\n",
    "\n",
    "# import json\n",
    "\n",
    "# # Open the JSON file for reading\n",
    "# with open('./convnextv2_large_config/config.json', 'r') as file:\n",
    "#     data = json.load(file)\n",
    "    \n",
    "# data['drop_path_rate'] = 0.5\n",
    "\n",
    "# with open('./convnextv2_large_config/config.json', 'w') as file:\n",
    "#     json.dump(data, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d782638-2c6c-4b63-87b3-c7bf291bbde4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Initialize EMA model\u001b[39;00m\n\u001b[1;32m     21\u001b[0m ema_decay \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.999\u001b[39m  \u001b[38;5;66;03m# decay factor for EMA\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m ema_model \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(\u001b[43mmodel\u001b[49m)\n\u001b[1;32m     23\u001b[0m ema_model \u001b[38;5;241m=\u001b[39m ema_model\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import ConvNextV2ForImageClassification\n",
    "import copy\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# model_name = \"facebook/convnextv2-large-22k-384\"\n",
    "# print(f\"Using device {device} and model {model_name}\")\n",
    "\n",
    "model = ConvNextV2ForImageClassification.from_pretrained('./convnextv2_large_config')\n",
    "\n",
    "model.classifier = nn.Linear(model.classifier.in_features, 5)\n",
    "\n",
    "state_dict = torch.load(f'convnextv2_large_models/fold_{I_FOLD}/final.pth', map_location=device)\n",
    "model.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Initialize EMA model\n",
    "ema_decay = 0.999  # decay factor for EMA\n",
    "ema_model = copy.deepcopy(model)\n",
    "ema_model = ema_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9613dea-4edf-47cd-a042-5f3468b19d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "integer_to_label = {\n",
    "    0: 'HGSC',\n",
    "    1: 'CC',\n",
    "    2: 'EC',\n",
    "    3: 'LGSC',\n",
    "    4: 'MC',\n",
    "}\n",
    "\n",
    "label_to_integer = {\n",
    "    'HGSC': 0,\n",
    "    'CC': 1,\n",
    "    'EC': 2,\n",
    "    'LGSC': 3,\n",
    "    'MC': 4,\n",
    "}\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.transform = transform\n",
    "        self.folder_paths = []\n",
    "        self.labels = []\n",
    "        self.image_ids = []\n",
    "        for index, row in dataframe.iterrows():\n",
    "            folder_path = row['tile_path']\n",
    "            label = row['label']\n",
    "            image_id = row['image_id']\n",
    "            if os.path.isdir(folder_path):  # Check if the folder_path is a valid directory\n",
    "                self.folder_paths.append(folder_path)\n",
    "                self.labels.append(label)\n",
    "                self.image_ids.append(image_id)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_paths = os.listdir(self.folder_paths[idx])\n",
    "        image_index = random.randint(0, len(image_paths) - 1)\n",
    "        while not image_paths[image_index].lower().endswith('.png'):  # Check if the file is a PNG\n",
    "            image_index = random.randint(0, len(image_paths) - 1)\n",
    "\n",
    "        image = Image.open(os.path.join(self.folder_paths[idx], image_paths[image_index]))\n",
    "        label = self.labels[idx]\n",
    "        image_id = self.image_ids[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label_to_integer[label], image_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "daab3db2-8fff-4b14-83f9-608c256dea11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "\"\"\"\n",
    "I SHOULD BE USING THESE MEANS AND STDS INSTEAD -- NOTE THIS IN THE FUTURE\n",
    "\"image_mean\": [\n",
    "    0.485,\n",
    "    0.456,\n",
    "    0.406\n",
    "],\n",
    "\"image_std\": [\n",
    "    0.229,\n",
    "    0.224,\n",
    "    0.225\n",
    "],\n",
    "\"\"\"\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.1),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.RandomAffine(degrees=45, translate=(0.25, 0.25), scale=(1, 2), shear=(-30, 30, -30, 30)),\n",
    "    transforms.Resize(384),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "    transforms.RandomErasing(p=0.5, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=0, inplace=False),\n",
    "    transforms.RandomErasing(p=0.5, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=0, inplace=False),\n",
    "    transforms.RandomErasing(p=0.5, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=0, inplace=False),\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize(384),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "])\n",
    "\n",
    "train_dataset = ImageDataset(dataframe=train, transform=train_transform)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, num_workers=3, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0aa0e2bd-4c21-48a1-a0fe-5eb15fb556b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "# Get the root logger\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# Optional: Remove all existing handlers from the logger\n",
    "for handler in logger.handlers[:]:\n",
    "    logger.removeHandler(handler)\n",
    "\n",
    "# Set the logging level\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Create a FileHandler and add it to the logger\n",
    "file_handler = logging.FileHandler(f'logs/convnextv2_large/fold_{I_FOLD}.txt')\n",
    "file_handler.setLevel(logging.INFO)\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "# Create a StreamHandler for stderr and add it to the logger\n",
    "stream_handler = logging.StreamHandler(sys.stderr)\n",
    "stream_handler.setLevel(logging.ERROR)  # Only log ERROR and CRITICAL messages to stderr\n",
    "logger.addHandler(stream_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a2e1e02-2ae6-4f5d-a8d1-1f238c34da88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>image_width</th>\n",
       "      <th>image_height</th>\n",
       "      <th>is_tma</th>\n",
       "      <th>tile_path</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CC</th>\n",
       "      <td>79</td>\n",
       "      <td>79</td>\n",
       "      <td>79</td>\n",
       "      <td>79</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EC</th>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HGSC</th>\n",
       "      <td>178</td>\n",
       "      <td>178</td>\n",
       "      <td>178</td>\n",
       "      <td>178</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGSC</th>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MC</th>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       image_id  image_width  image_height  is_tma  tile_path\n",
       "label                                                        \n",
       "CC           79           79            79      79         79\n",
       "EC          100          100           100     100        100\n",
       "HGSC        178          178           178     178        178\n",
       "LGSC         37           37            37      37         37\n",
       "MC           37           37            37      37         37"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.groupby('label').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08f9b938-9860-4147-be7e-f6f36938f212",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import logging\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "import random\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "initial_lr = 1e-5\n",
    "final_lr = 1e-6\n",
    "num_epochs = 10000\n",
    "\n",
    "# Function for linear warmup\n",
    "def learning_rate(step, warmup_steps=500, max_steps=20000):\n",
    "    if step < warmup_steps:\n",
    "        return initial_lr * (float(step) / float(max(1, warmup_steps)))\n",
    "    elif step < max_steps:\n",
    "        progress = (float(step - warmup_steps) / float(max(1, max_steps - warmup_steps)))\n",
    "        cos_component = 0.5 * (1 + math.cos(math.pi * progress))\n",
    "        return final_lr + (initial_lr - final_lr) * cos_component\n",
    "    else:\n",
    "        return final_lr\n",
    "\n",
    "# Function to calculate weighted accuracy\n",
    "def weighted_accuracy(true_labels, predictions, class_weights):\n",
    "    correct = 0\n",
    "    total_weight = 0\n",
    "\n",
    "    for label, pred in zip(true_labels, predictions):\n",
    "        if label == pred:\n",
    "            correct += class_weights[label]\n",
    "        total_weight += class_weights[label]\n",
    "\n",
    "    return correct / total_weight\n",
    "\n",
    "def update_ema_variables(model, ema_model, alpha, global_step):\n",
    "    # Update the EMA model parameters\n",
    "    with torch.no_grad():\n",
    "        for ema_param, param in zip(ema_model.parameters(), model.parameters()):\n",
    "            ema_param.data.mul_(alpha).add_(param.data, alpha=1 - alpha)\n",
    "\n",
    "scaler = GradScaler()\n",
    "            \n",
    "# Define the optimizer\n",
    "optimizer = optim.AdamW(model.parameters(), lr=initial_lr, weight_decay=5e-2)\n",
    "\n",
    "# Calculate class weights\n",
    "class_counts = np.array([178, 79, 100, 37, 37], dtype=np.float32)\n",
    "class_weights = 1. / class_counts\n",
    "class_weights /= class_weights.sum()\n",
    "\n",
    "# Convert class weights to tensor\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "# Define the loss function with class weights\n",
    "criterion = torch.nn.CrossEntropyLoss(weight=class_weights, label_smoothing=0.1)\n",
    "\n",
    "best_val_accuracy = 0.0\n",
    "step = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # set the model to training mode\n",
    "    \n",
    "    for i, (images, labels, _) in enumerate(train_dataloader, 0):\n",
    "        # Convert images to PIL format\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Linearly increase the learning rate\n",
    "        lr = learning_rate(step)\n",
    "        for g in optimizer.param_groups:\n",
    "            g['lr'] = lr\n",
    "            \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass with autocast\n",
    "        with autocast():\n",
    "            outputs = model(images)\n",
    "            logits_per_image = outputs.logits\n",
    "            loss = criterion(logits_per_image, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        update_ema_variables(model, ema_model, ema_decay, step)\n",
    "\n",
    "        logging.info('[%d, %5d] loss: %.3f' % (epoch + 1, step, loss.item()))\n",
    "\n",
    "        if step % 500 == 0:\n",
    "            ema_model.eval()\n",
    "\n",
    "            tma_preds = []\n",
    "            tma_labels = []\n",
    "            \n",
    "            non_tma_preds = []\n",
    "            non_tma_labels = []\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for _, row in validation.iterrows():\n",
    "                    path = row['tile_path']\n",
    "                    all_files = [f for f in os.listdir(path) if f.lower().endswith('.png')]\n",
    "                    \n",
    "                    probabilities = torch.zeros(5).to(device)\n",
    "\n",
    "                    # Prepare a list to hold image tiles\n",
    "                    batch_tiles = []\n",
    "\n",
    "                    sample_size = min(16, len(all_files))\n",
    "                    sampled_files = random.sample(all_files, sample_size)\n",
    "\n",
    "                    for image_name in sampled_files:\n",
    "                        image_path = os.path.join(path, image_name)\n",
    "                        sub_image = Image.open(image_path)\n",
    "\n",
    "                        tile = val_transform(sub_image).unsqueeze(0).to(device)\n",
    "                        batch_tiles.append(tile)\n",
    "\n",
    "                    outputs = ema_model(torch.concat(batch_tiles, dim=0))\n",
    "                    probs = outputs.logits.softmax(dim=1)\n",
    "                    probabilities += probs[0]\n",
    "\n",
    "                    pred_label = integer_to_label[probabilities.argmax().detach().cpu().item()]\n",
    "                    label = row['label']\n",
    "                    if row['is_tma']:\n",
    "                        tma_preds.append(pred_label)\n",
    "                        tma_labels.append(label)\n",
    "                    else:\n",
    "                        non_tma_preds.append(pred_label)\n",
    "                        non_tma_labels.append(label)\n",
    "\n",
    "            tma_accuracy = balanced_accuracy_score(tma_labels, tma_preds)\n",
    "            non_tma_accuracy = balanced_accuracy_score(non_tma_labels, non_tma_preds)\n",
    "            accuracy = (tma_accuracy + non_tma_accuracy) / 2\n",
    "            logging.info(f'TMA Accuracy: {tma_accuracy} | Non-TMA Accuracy: {non_tma_accuracy} | Overall Accuracy: {accuracy}')\n",
    "\n",
    "            if accuracy > best_val_accuracy:\n",
    "                best_val_accuracy = accuracy\n",
    "                torch.save(ema_model.state_dict(), f'convnextv2_large_models/fold_{I_FOLD}/epoch_{epoch}_step_{step}.pth')\n",
    "                logging.info(f'Model saved after epoch {epoch} and step {step}')\n",
    "                \n",
    "            model.train()\n",
    "\n",
    "        if step == 20000:\n",
    "            torch.save(ema_model.state_dict(), f'convnextv2_large_models/fold_{I_FOLD}/final.pth')\n",
    "\n",
    "        step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e08f94e-b372-4ace-aef2-da0100690f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 5]) LGSC 33976\n",
      "torch.Size([128, 5]) LGSC 34277\n",
      "torch.Size([128, 5]) LGSC 34688\n",
      "torch.Size([16, 5]) MC 35565\n",
      "torch.Size([15, 5]) LGSC 37385\n",
      "torch.Size([128, 5]) HGSC 37655\n",
      "torch.Size([128, 5]) HGSC 38018\n",
      "torch.Size([128, 5]) HGSC 38041\n",
      "torch.Size([128, 5]) HGSC 38048\n",
      "torch.Size([128, 5]) HGSC 38097\n",
      "torch.Size([128, 5]) LGSC 38366\n",
      "torch.Size([128, 5]) HGSC 38535\n",
      "torch.Size([128, 5]) LGSC 38585\n",
      "torch.Size([128, 5]) LGSC 38849\n",
      "torch.Size([128, 5]) LGSC 38959\n",
      "torch.Size([128, 5]) HGSC 39144\n",
      "torch.Size([128, 5]) HGSC 39146\n",
      "torch.Size([128, 5]) HGSC 39172\n",
      "torch.Size([128, 5]) HGSC 39255\n",
      "torch.Size([128, 5]) LGSC 39258\n",
      "torch.Size([128, 5]) HGSC 39297\n",
      "torch.Size([128, 5]) HGSC 39365\n",
      "torch.Size([128, 5]) HGSC 39425\n",
      "torch.Size([128, 5]) LGSC 39466\n",
      "torch.Size([128, 5]) HGSC 39728\n",
      "torch.Size([128, 5]) MC 39872\n",
      "torch.Size([128, 5]) HGSC 39893\n",
      "torch.Size([128, 5]) HGSC 39990\n",
      "torch.Size([128, 5]) CC 40129\n",
      "torch.Size([128, 5]) HGSC 40639\n",
      "torch.Size([128, 5]) CC 40888\n",
      "torch.Size([128, 5]) HGSC 41361\n",
      "torch.Size([128, 5]) HGSC 41801\n",
      "torch.Size([128, 5]) CC 42125\n",
      "torch.Size([128, 5]) MC 42260\n",
      "torch.Size([128, 5]) CC 42296\n",
      "torch.Size([128, 5]) CC 42549\n",
      "torch.Size([128, 5]) HGSC 43390\n",
      "torch.Size([128, 5]) HGSC 43432\n",
      "torch.Size([128, 5]) HGSC 43671\n",
      "torch.Size([128, 5]) CC 43796\n",
      "torch.Size([128, 5]) HGSC 43815\n",
      "torch.Size([128, 5]) HGSC 43875\n",
      "torch.Size([128, 5]) EC 43998\n",
      "torch.Size([128, 5]) EC 44232\n",
      "torch.Size([128, 5]) CC 44283\n",
      "torch.Size([128, 5]) HGSC 44432\n",
      "torch.Size([128, 5]) MC 44530\n",
      "torch.Size([128, 5]) CC 44581\n",
      "torch.Size([17, 5]) CC 44603\n",
      "torch.Size([128, 5]) EC 44700\n",
      "torch.Size([128, 5]) EC 44804\n",
      "torch.Size([128, 5]) EC 44962\n",
      "torch.Size([128, 5]) CC 44976\n",
      "torch.Size([128, 5]) HGSC 45104\n",
      "torch.Size([128, 5]) HGSC 45185\n",
      "torch.Size([128, 5]) EC 45254\n",
      "torch.Size([128, 5]) EC 45578\n",
      "torch.Size([128, 5]) EC 45630\n",
      "torch.Size([128, 5]) CC 45725\n",
      "torch.Size([128, 5]) CC 45990\n",
      "torch.Size([128, 5]) HGSC 46139\n",
      "torch.Size([128, 5]) HGSC 46172\n",
      "torch.Size([128, 5]) HGSC 46435\n",
      "torch.Size([128, 5]) EC 46444\n",
      "torch.Size([128, 5]) HGSC 46469\n",
      "torch.Size([128, 5]) CC 46543\n",
      "torch.Size([128, 5]) EC 46688\n",
      "torch.Size([128, 5]) HGSC 46736\n",
      "torch.Size([128, 5]) CC 46769\n",
      "torch.Size([128, 5]) HGSC 46793\n",
      "torch.Size([128, 5]) MC 46815\n",
      "torch.Size([128, 5]) HGSC 47020\n",
      "torch.Size([128, 5]) EC 47105\n",
      "torch.Size([128, 5]) MC 47431\n",
      "torch.Size([128, 5]) HGSC 47837\n",
      "torch.Size([128, 5]) HGSC 47911\n",
      "torch.Size([128, 5]) CC 47960\n",
      "torch.Size([128, 5]) HGSC 47984\n",
      "torch.Size([128, 5]) MC 48502\n",
      "torch.Size([128, 5]) CC 48506\n",
      "torch.Size([128, 5]) MC 48550\n",
      "torch.Size([16, 5]) EC 48734\n",
      "torch.Size([128, 5]) HGSC 48861\n",
      "torch.Size([128, 5]) HGSC 48973\n",
      "torch.Size([128, 5]) HGSC 49281\n",
      "torch.Size([128, 5]) MC 49587\n",
      "torch.Size([128, 5]) EC 49942\n",
      "torch.Size([128, 5]) EC 49995\n",
      "torch.Size([128, 5]) EC 50048\n",
      "torch.Size([128, 5]) EC 50246\n",
      "torch.Size([128, 5]) EC 50304\n",
      "torch.Size([128, 5]) HGSC 50589\n",
      "torch.Size([128, 5]) HGSC 50712\n",
      "torch.Size([128, 5]) HGSC 50878\n",
      "torch.Size([15, 5]) HGSC 50932\n",
      "torch.Size([128, 5]) EC 50962\n",
      "torch.Size([128, 5]) EC 51032\n",
      "torch.Size([128, 5]) EC 51128\n",
      "torch.Size([62, 5]) EC 51215\n",
      "torch.Size([128, 5]) CC 51346\n",
      "torch.Size([128, 5]) EC 51499\n",
      "torch.Size([128, 5]) EC 51679\n",
      "torch.Size([128, 5]) EC 52108\n",
      "torch.Size([128, 5]) CC 52259\n",
      "torch.Size([128, 5]) CC 52375\n",
      "torch.Size([128, 5]) CC 52420\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "ema_model.eval()\n",
    "\n",
    "image_ids = []\n",
    "logits = []\n",
    "labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for _, row in validation.iterrows():\n",
    "        path = row['tile_path']\n",
    "        all_files = [f for f in os.listdir(path) if f.lower().endswith('.png')]\n",
    "\n",
    "        sum_probabilities = torch.zeros(5).to(device)\n",
    "        sum_log_probabilities = torch.zeros(5).to(device)\n",
    "        sum_log_neg_probabilities = torch.zeros(5).to(device)\n",
    "        \n",
    "        batch_logits = []\n",
    "\n",
    "        # Prepare a list to hold image tiles\n",
    "        batch_tiles = []\n",
    "\n",
    "        sample_size = min(128, len(all_files))\n",
    "        sampled_files = random.sample(all_files, sample_size)\n",
    "\n",
    "        for image_name in sampled_files:\n",
    "            image_path = os.path.join(path, image_name)\n",
    "            sub_image = Image.open(image_path)\n",
    "\n",
    "            tile = val_transform(sub_image).unsqueeze(0).to(device)\n",
    "            batch_tiles.append(tile)\n",
    "        \n",
    "        for i_batch in range(0, len(batch_tiles), 32):\n",
    "            outputs = ema_model(torch.concat(batch_tiles[i_batch:i_batch+32], dim=0))\n",
    "            probs = outputs.logits.softmax(dim=1)\n",
    "            batch_logits.append(outputs.logits)\n",
    "            sum_probabilities += probs.sum(dim=0)\n",
    "            sum_log_probabilities += torch.log(probs).sum(dim=0)\n",
    "            sum_log_neg_probabilities += torch.log(1 - probs).sum(dim=0)\n",
    "        \n",
    "        image_id = row['image_id']\n",
    "        batch_logits = torch.concat(batch_logits, dim=0)\n",
    "        label = row['label']\n",
    "        print(batch_logits.shape, label, image_id)\n",
    "        image_ids.append(image_id)\n",
    "        logits.append(batch_logits)\n",
    "        labels.append(label)\n",
    "        \n",
    "\n",
    "# tma_accuracy = balanced_accuracy_score(tma_labels, tma_preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6fdad573-11dd-45fc-9886-3c5af096a0d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8529292929292929"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = []\n",
    "for image_logits in logits:\n",
    "    argmax_indices = torch.argmax(image_logits, dim=1)\n",
    "\n",
    "    frequency_counts = torch.bincount(argmax_indices, minlength=5)\n",
    "\n",
    "    max_vote_key = frequency_counts.argmax().cpu().item()\n",
    "    predictions.append(integer_to_label[max_vote_key])\n",
    "\n",
    "plurality_accuracy = balanced_accuracy_score(labels, predictions)\n",
    "plurality_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6bf1b109-43ad-4365-8c40-bff4c014d2c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8667171717171718"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = []\n",
    "for image_logits in logits:\n",
    "    summed_logits = image_logits.sum(dim=0)\n",
    "    \n",
    "    max_vote_key = summed_logits.argmax().cpu().item()\n",
    "    predictions.append(integer_to_label[max_vote_key])\n",
    "\n",
    "logit_sum_accuracy = balanced_accuracy_score(labels, predictions)\n",
    "logit_sum_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "55158b97-40e5-47b4-8318-7e75ba5c1329",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8667171717171718"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = []\n",
    "for image_logits in logits:\n",
    "    summed_probs = image_logits.softmax(dim=1).sum(dim=0)\n",
    "    \n",
    "    max_vote_key = summed_probs.argmax().cpu().item()\n",
    "    predictions.append(integer_to_label[max_vote_key])\n",
    "\n",
    "prob_sum_accuracy = balanced_accuracy_score(labels, predictions)\n",
    "prob_sum_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c6b21cfe-e0c7-455c-aab9-8619555895ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8667171717171718"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = []\n",
    "for image_logits in logits:\n",
    "    summed_log_probs = torch.log(image_logits.softmax(dim=1)).sum(dim=0)\n",
    "    \n",
    "    max_vote_key = summed_log_probs.argmax().cpu().item()\n",
    "    predictions.append(integer_to_label[max_vote_key])\n",
    "\n",
    "log_prob_sum_accuracy = balanced_accuracy_score(labels, predictions)\n",
    "log_prob_sum_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cc136b88-b5b8-4fe8-9db8-c89539d7ea0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8667171717171718"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = []\n",
    "for image_logits in logits:\n",
    "    summed_one_minus_log_probs = torch.log(1 - image_logits.softmax(dim=1)).sum(dim=0)\n",
    "    \n",
    "    min_vote_key = summed_one_minus_log_probs.argmin().cpu().item()\n",
    "    predictions.append(integer_to_label[min_vote_key])\n",
    "\n",
    "one_minus_log_prob_sum_accuracy = balanced_accuracy_score(labels, predictions)\n",
    "one_minus_log_prob_sum_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "58c81771-7d17-4200-8a15-ffe857e80b2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.862929292929293"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def borda_count_winner(image_logits):\n",
    "    num_classes = image_logits.size(1)\n",
    "    borda_scores = defaultdict(int)\n",
    "\n",
    "    # Rank each class for each image_logits and assign Borda points\n",
    "    for logits in image_logits:\n",
    "        # Get ranks (in descending order of logits)\n",
    "        ranks = torch.argsort(logits, descending=True)\n",
    "\n",
    "        # Assign Borda points (highest rank gets num_classes - 1 points, next gets num_classes - 2, ...)\n",
    "        for rank, class_index in enumerate(ranks):\n",
    "            borda_scores[class_index.item()] += num_classes - 1 - rank\n",
    "\n",
    "    # Find the class with the highest total Borda score\n",
    "    borda_winner = max(borda_scores, key=borda_scores.get)\n",
    "    return borda_winner\n",
    "\n",
    "predictions = []\n",
    "for image_logits in logits:\n",
    "    borda_winner = borda_count_winner(image_logits)\n",
    "    predictions.append(integer_to_label[borda_winner])\n",
    "    \n",
    "borda_accuracy = balanced_accuracy_score(labels, predictions)\n",
    "borda_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0302ec27-357f-4a0c-ab97-7f202a8bade8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8574747474747475"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def instant_runoff_winner(image_logits):\n",
    "    num_voters, num_classes = image_logits.shape\n",
    "    active_candidates = set(range(num_classes))\n",
    "\n",
    "    while True:\n",
    "        # Count the first-preference votes for each candidate\n",
    "        first_pref_counts = torch.zeros(num_classes)\n",
    "        for logits in image_logits:\n",
    "            for rank in torch.argsort(logits, descending=True):\n",
    "                if rank.item() in active_candidates:\n",
    "                    first_pref_counts[rank] += 1\n",
    "                    break\n",
    "\n",
    "        # Check if any candidate has more than 50% of the votes\n",
    "        if torch.any(first_pref_counts > num_voters / 2):\n",
    "            return torch.argmax(first_pref_counts).item()\n",
    "\n",
    "        # Eliminate the candidate with the fewest votes\n",
    "        min_votes, min_candidate = torch.min(first_pref_counts[list(active_candidates)], 0)\n",
    "        active_candidates.remove(min_candidate.item())\n",
    "        \n",
    "predictions = []\n",
    "for image_logits in logits:\n",
    "    irv_winner = instant_runoff_winner(image_logits)\n",
    "    predictions.append(integer_to_label[irv_winner])\n",
    "    \n",
    "instant_runoff_accuracy = balanced_accuracy_score(labels, predictions)\n",
    "instant_runoff_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f622ea57-832d-497b-848d-e013c9a4232e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8529292929292929"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_minimax_winner(image_logits):\n",
    "    num_voters, num_classes = image_logits.shape\n",
    "    max_regrets = torch.zeros(num_classes)\n",
    "\n",
    "    # Perform pairwise comparisons between all classes\n",
    "    for i in range(num_classes):\n",
    "        for j in range(i + 1, num_classes):\n",
    "            # Count how many voters prefer class i over class j and vice versa\n",
    "            votes_for_i = torch.sum(image_logits[:, i] > image_logits[:, j])\n",
    "            votes_for_j = num_voters - votes_for_i\n",
    "\n",
    "            # Update the maximum regret for each class\n",
    "            max_regrets[i] = max(max_regrets[i], votes_for_j)\n",
    "            max_regrets[j] = max(max_regrets[j], votes_for_i)\n",
    "\n",
    "    # The winner is the class with the smallest maximum regret\n",
    "    return torch.argmin(max_regrets).item()\n",
    "        \n",
    "predictions = []\n",
    "for image_logits in logits:\n",
    "    minimax_winner = calculate_minimax_winner(image_logits)\n",
    "    predictions.append(integer_to_label[minimax_winner])\n",
    "    \n",
    "minimax_accuracy = balanced_accuracy_score(labels, predictions)\n",
    "minimax_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5529ca1f-c7f9-4111-b793-c5cda26a283e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8667171717171718"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def find_winner(graph, source, visited):\n",
    "    \"\"\"Helper function to find the winner in the graph.\"\"\"\n",
    "    if source not in graph:\n",
    "        return False\n",
    "\n",
    "    visited.add(source)\n",
    "    for target in graph[source]:\n",
    "        if target not in visited and find_winner(graph, target, visited):\n",
    "            return True\n",
    "    visited.remove(source)\n",
    "    return False\n",
    "\n",
    "def ranked_pairs_winner(image_logits):\n",
    "    num_voters, num_classes = image_logits.shape\n",
    "    margins = defaultdict(int)\n",
    "\n",
    "    # Perform pairwise comparisons\n",
    "    for i in range(num_classes):\n",
    "        for j in range(num_classes):\n",
    "            if i != j:\n",
    "                votes_for_i = torch.sum(image_logits[:, i] > image_logits[:, j])\n",
    "                votes_for_j = num_voters - votes_for_i\n",
    "                margins[(i, j)] = votes_for_i - votes_for_j\n",
    "\n",
    "    # Sort pairs by margin of victory\n",
    "    sorted_pairs = sorted(margins, key=margins.get, reverse=True)\n",
    "\n",
    "    # Initialize graph for locking pairs\n",
    "    graph = {i: set() for i in range(num_classes)}\n",
    "    for pair in sorted_pairs:\n",
    "        winner, loser = pair\n",
    "        graph[winner].add(loser)\n",
    "        visited = set()\n",
    "\n",
    "        # Check for cycle\n",
    "        if find_winner(graph, loser, visited):\n",
    "            graph[winner].remove(loser)\n",
    "\n",
    "    # Determine the winner\n",
    "    for i in range(num_classes):\n",
    "        if not any(i in targets for targets in graph.values()):\n",
    "            return i\n",
    "\n",
    "    # Fallback: Choose the class with the highest total votes if no clear winner is found\n",
    "    total_votes = torch.sum(image_logits, axis=0)\n",
    "    return torch.argmax(total_votes).item()\n",
    "        \n",
    "predictions = []\n",
    "for image_logits in logits:\n",
    "    rp_winner = ranked_pairs_winner(image_logits)\n",
    "    predictions.append(integer_to_label[rp_winner])\n",
    "    \n",
    "rp_accuracy = balanced_accuracy_score(labels, predictions)\n",
    "rp_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ee601aa5-db28-4c8f-ba9d-f6359f802993",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8667171717171718"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def star_voting_winner(image_logits):\n",
    "    num_voters, num_classes = image_logits.shape\n",
    "\n",
    "    # Step 1: Sum the scores for each candidate\n",
    "    total_scores = torch.sum(image_logits, axis=0)\n",
    "\n",
    "    # Step 2: Find the two candidates with the highest total scores\n",
    "    top_two = torch.topk(total_scores, 2).indices\n",
    "\n",
    "    # Step 3: Runoff between the top two candidates\n",
    "    first_choice_votes = torch.sum(image_logits[:, top_two[0]] > image_logits[:, top_two[1]])\n",
    "    second_choice_votes = num_voters - first_choice_votes\n",
    "\n",
    "    # Determine the winner\n",
    "    if first_choice_votes > second_choice_votes:\n",
    "        return top_two[0].item()\n",
    "    else:\n",
    "        return top_two[1].item()\n",
    "        \n",
    "predictions = []\n",
    "for image_logits in logits:\n",
    "    star_winner = ranked_pairs_winner(image_logits)\n",
    "    predictions.append(integer_to_label[star_winner])\n",
    "    \n",
    "star_accuracy = balanced_accuracy_score(labels, predictions)\n",
    "star_accuracy"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.2-0.m112",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.2-0:m112"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
