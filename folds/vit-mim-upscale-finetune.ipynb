{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82500098-49ea-4a7e-bb63-ce5130f954cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>label</th>\n",
       "      <th>image_width</th>\n",
       "      <th>image_height</th>\n",
       "      <th>is_tma</th>\n",
       "      <th>tile_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8280</td>\n",
       "      <td>HGSC</td>\n",
       "      <td>2964</td>\n",
       "      <td>2964</td>\n",
       "      <td>True</td>\n",
       "      <td>../tiles_768/8280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8985</td>\n",
       "      <td>LGSC</td>\n",
       "      <td>65003</td>\n",
       "      <td>31754</td>\n",
       "      <td>False</td>\n",
       "      <td>../tiles_768/8985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9183</td>\n",
       "      <td>LGSC</td>\n",
       "      <td>74091</td>\n",
       "      <td>34185</td>\n",
       "      <td>False</td>\n",
       "      <td>../tiles_768/9183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9200</td>\n",
       "      <td>MC</td>\n",
       "      <td>3388</td>\n",
       "      <td>3388</td>\n",
       "      <td>True</td>\n",
       "      <td>../tiles_768/9200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10252</td>\n",
       "      <td>LGSC</td>\n",
       "      <td>49053</td>\n",
       "      <td>39794</td>\n",
       "      <td>False</td>\n",
       "      <td>../tiles_768/10252</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   image_id label  image_width  image_height  is_tma           tile_path\n",
       "0      8280  HGSC         2964          2964    True   ../tiles_768/8280\n",
       "1      8985  LGSC        65003         31754   False   ../tiles_768/8985\n",
       "2      9183  LGSC        74091         34185   False   ../tiles_768/9183\n",
       "3      9200    MC         3388          3388    True   ../tiles_768/9200\n",
       "4     10252  LGSC        49053         39794   False  ../tiles_768/10252"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def get_image_path(image_id:int):\n",
    "    return os.path.join('../tiles_768', str(image_id))\n",
    "\n",
    "I_FOLD = 0\n",
    "train = pd.read_csv(f\"train_fold_{I_FOLD}.csv\")\n",
    "validation = pd.read_csv(f\"val_fold_{I_FOLD}.csv\")\n",
    "\n",
    "train['tile_path'] = train['image_id'].apply(lambda x: get_image_path(x))\n",
    "validation['tile_path'] = validation['image_id'].apply(lambda x: get_image_path(x))\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d782638-2c6c-4b63-87b3-c7bf291bbde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import timm\n",
    "from timm.models import VisionTransformer\n",
    "from timm.layers import SwiGLUPacked\n",
    "from timm.models.layers import DropPath\n",
    "import copy\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "class CustomViT(nn.Module):\n",
    "    def __init__(self, n_classes=5, embed_dim=768):\n",
    "        super().__init__()\n",
    "        self.n_classes = n_classes\n",
    "        self.embed_dim = embed_dim\n",
    "        self.patch_size = 16\n",
    "        # Load the base ViT model\n",
    "        self.base_model = VisionTransformer(\n",
    "            img_size=512, \n",
    "            num_classes=self.n_classes, \n",
    "            patch_size=self.patch_size, \n",
    "            embed_dim=self.embed_dim, \n",
    "            depth=12, \n",
    "            num_heads=12, \n",
    "            global_pool='avg', \n",
    "            pre_norm=True, \n",
    "            act_layer=nn.SiLU\n",
    "        )\n",
    "\n",
    "        # Initialize a learnable mask token\n",
    "        self.mask_token = nn.Parameter(torch.randn(1, 1, self.embed_dim))\n",
    "        \n",
    "        max_drop_path_rate = 0.4\n",
    "        dropout_rate = 0.1\n",
    "\n",
    "        drop_path_rates = [x.item() for x in torch.linspace(0, max_drop_path_rate, len(self.base_model.blocks))]\n",
    "\n",
    "        # Assign drop path rates\n",
    "        for i, block in enumerate(self.base_model.blocks):\n",
    "            block.drop_path1 = DropPath(drop_prob=drop_path_rates[i])\n",
    "            block.drop_path2 = DropPath(drop_prob=drop_path_rates[i])\n",
    "            block.attn.attn_drop = nn.Dropout(p=dropout_rate, inplace=False)\n",
    "            block.attn.proj_drop = nn.Dropout(p=dropout_rate, inplace=False)\n",
    "            block.mlp.drop1 = nn.Dropout(p=dropout_rate, inplace=False)\n",
    "            block.mlp.drop2 = nn.Dropout(p=dropout_rate, inplace=False)\n",
    "        self.head_dropout = nn.Dropout(p=dropout_rate, inplace=False)\n",
    "\n",
    "        self.class_token_head = nn.Linear(self.embed_dim, self.n_classes)\n",
    "        self.patch_token_head = nn.Linear(self.embed_dim, self.n_classes) \n",
    "        \n",
    "        self.base_model.patch_embed.img_size = None\n",
    "\n",
    "    def resize(self, new_img_size):\n",
    "        # Calculate the size of the grid of patches\n",
    "        num_patches_side = new_img_size // self.patch_size\n",
    "        num_patches = num_patches_side ** 2\n",
    "\n",
    "        # Extract the original positional embeddings, excluding the class token\n",
    "        pos_embed = self.base_model.pos_embed\n",
    "        old_num_patches_side = int((pos_embed.size(1) - 1) ** 0.5)\n",
    "        pos_grid = pos_embed[:, 1:].reshape(1, old_num_patches_side, old_num_patches_side, -1)\n",
    "        pos_grid = pos_grid.permute(0, 3, 1, 2).contiguous()\n",
    "        \n",
    "        # Resize using bilinear interpolation (make sure to keep the embedding dimension unchanged)\n",
    "        new_pos_grid = F.interpolate(pos_grid, size=(num_patches_side, num_patches_side), mode='bilinear', align_corners=False)\n",
    "\n",
    "        # Flatten the grid back to a sequence and re-add the class token\n",
    "        new_pos_embed = torch.cat([pos_embed[:, :1], new_pos_grid.permute(0, 2, 3, 1).contiguous().view(1, num_patches_side * num_patches_side, -1)], dim=1)\n",
    "\n",
    "        # Update the positional embeddings\n",
    "        self.base_model.pos_embed = nn.Parameter(new_pos_embed)\n",
    "        self.base_model.patch_embed.img_size = (new_img_size, new_img_size)\n",
    "        \n",
    "    def forward_features(self, x, mask=None):\n",
    "        # Get the patch embeddings (excluding the class token)\n",
    "        x = self.base_model.patch_embed(x)\n",
    "        \n",
    "        to_cat = []\n",
    "        if self.base_model.cls_token is not None:\n",
    "            to_cat.append(self.base_model.cls_token.expand(x.shape[0], -1, -1))\n",
    "        x = torch.cat(to_cat + [x], dim=1)\n",
    "\n",
    "        # Handle masked patches if a mask is provided\n",
    "        if mask is not None:\n",
    "            # Adjust mask to account for the class token\n",
    "            mask = torch.cat([torch.zeros(x.shape[0], 1).bool().to(mask.device), mask], dim=1)\n",
    "            # Expand mask token to match the batch size and masked patches\n",
    "            mask_tokens = self.mask_token.expand(x.size(0), -1, -1)\n",
    "            # Apply the mask - replace masked patches with the mask token\n",
    "            x = torch.where(mask.unsqueeze(-1), mask_tokens, x)\n",
    "        \n",
    "        x = self.base_model.pos_drop(x + self.base_model.pos_embed)\n",
    "        x = self.base_model.norm_pre(x)\n",
    "        x = self.base_model.blocks(x)\n",
    "        x = self.base_model.norm(x)\n",
    "\n",
    "        # Exclude the class token and return the patch representations\n",
    "        return x\n",
    "\n",
    "    def forward_head(self, x):\n",
    "        class_token, patch_tokens = x[:, :1], x[:, 1:]\n",
    "\n",
    "        # Apply dropout\n",
    "        class_token = self.head_dropout(class_token)\n",
    "        patch_tokens = self.head_dropout(patch_tokens)\n",
    "\n",
    "        # Process class token and patch tokens through their respective heads\n",
    "        class_token_output = self.class_token_head(class_token)\n",
    "        patch_token_output = self.patch_token_head(patch_tokens)\n",
    "        \n",
    "        x = torch.cat([class_token_output, patch_token_output], dim=1)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        x = self.forward_features(x, mask=mask)\n",
    "        x = self.forward_head(x)\n",
    "        return x\n",
    "\n",
    "IMAGE_SIZE = 512\n",
    "PATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e95575cb-064f-4318-bf0a-c456dd432cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierModel(nn.Module):\n",
    "    def __init__(self, n_classes=5):\n",
    "        super().__init__()\n",
    "        self.d_model = 768\n",
    "        self.n_classes = n_classes\n",
    "        \n",
    "        self.base_model = CustomViT(n_classes=16 * 16 * 3, embed_dim=self.d_model)\n",
    "        \n",
    "        self.layer_norm = nn.LayerNorm(self.d_model)\n",
    "        self.linear = nn.Linear(self.d_model, self.n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.base_model(x)\n",
    "        x = x[:, 1:].mean(dim=1)\n",
    "        x = self.layer_norm(x)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "N_CLASSES = 5\n",
    "classifier_model = ClassifierModel(N_CLASSES)\n",
    "classifier_model = classifier_model.to(device)\n",
    "state_dict = torch.load('vit_mim_upscale_models/fold_0/ema_model_epoch_1_step_25000.pth', map_location=device)\n",
    "classifier_model.base_model.load_state_dict(state_dict, strict=False)\n",
    "classifier_model = classifier_model.to(device)\n",
    "\n",
    "# Initialize EMA model\n",
    "ema_decay = 0.999  # decay factor for EMA\n",
    "ema_classifier_model = copy.deepcopy(classifier_model)\n",
    "# state_dict = torch.load('vit_mim_upscale_models/fold_0/ema_model_epoch_0_step_9000.pth', map_location=device)\n",
    "# ema_model.load_state_dict(state_dict, strict=False)\n",
    "ema_classifier_model = ema_classifier_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9613dea-4edf-47cd-a042-5f3468b19d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "\n",
    "integer_to_label = {\n",
    "    0: 'HGSC',\n",
    "    1: 'CC',\n",
    "    2: 'EC',\n",
    "    3: 'LGSC',\n",
    "    4: 'MC',\n",
    "}\n",
    "\n",
    "label_to_integer = {\n",
    "    'HGSC': 0,\n",
    "    'CC': 1,\n",
    "    'EC': 2,\n",
    "    'LGSC': 3,\n",
    "    'MC': 4,\n",
    "}\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.transform = transform\n",
    "        self.all_images = []  # Store all images in an interlaced fashion\n",
    "        self.folder_images = []  # Temporary storage for images from each folder\n",
    "\n",
    "        # Step 1: Collect all images from each folder\n",
    "        for index, row in dataframe.iterrows():\n",
    "            folder_path = row['tile_path']\n",
    "            label = row['label']\n",
    "            image_id = row['image_id']\n",
    "            if os.path.isdir(folder_path):\n",
    "                image_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.lower().endswith('.png')]\n",
    "                random.shuffle(image_files)\n",
    "                self.folder_images.append([(image_file, label, image_id) for image_file in image_files])\n",
    "\n",
    "        # Step 2: Interlace the images\n",
    "        max_length = max(len(images) for images in self.folder_images)\n",
    "        for i in range(max_length):\n",
    "            for images in self.folder_images:\n",
    "                self.all_images.append(images[i % len(images)])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path, label, image_id = self.all_images[idx]\n",
    "        image = Image.open(image_path)\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label_to_integer[label], image_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "daab3db2-8fff-4b14-83f9-608c256dea11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(size=IMAGE_SIZE, scale=(0.75, 1.0), ratio=(0.75, 1.33)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=(0, 360)),\n",
    "    transforms.RandomAffine(degrees=0, shear=(-20, 20, -20, 20)),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.3, hue=0.3),\n",
    "    transforms.RandomApply([transforms.Grayscale(num_output_channels=3)], p=0.25),\n",
    "    transforms.RandomApply([transforms.GaussianBlur(kernel_size=(5, 5), sigma=(0.1, 1))], p=0.25),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "])\n",
    "\n",
    "# val_transform = transforms.Compose([\n",
    "#     transforms.Resize(448),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "\n",
    "train_dataset = ImageDataset(dataframe=train, transform=transform)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, num_workers=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0aa0e2bd-4c21-48a1-a0fe-5eb15fb556b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "# Get the root logger\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# Optional: Remove all existing handlers from the logger\n",
    "for handler in logger.handlers[:]:\n",
    "    logger.removeHandler(handler)\n",
    "\n",
    "# Set the logging level\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Create a FileHandler and add it to the logger\n",
    "file_handler = logging.FileHandler(f'logs/vit_mim_upscale_finetune/fold_{I_FOLD}.txt')\n",
    "file_handler.setLevel(logging.INFO)\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "# Create a StreamHandler for stderr and add it to the logger\n",
    "stream_handler = logging.StreamHandler(sys.stderr)\n",
    "stream_handler.setLevel(logging.ERROR)  # Only log ERROR and CRITICAL messages to stderr\n",
    "logger.addHandler(stream_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f9b938-9860-4147-be7e-f6f36938f212",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import logging\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "import random\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import torch.nn.functional as F\n",
    "\n",
    "initial_lr = 0.0005 * BATCH_SIZE/256\n",
    "final_lr = initial_lr * 0.01\n",
    "num_epochs = 1000\n",
    "\n",
    "# Function for linear warmup\n",
    "def learning_rate(step, warmup_steps=2500, max_steps=25000):\n",
    "    if step < warmup_steps:\n",
    "        return initial_lr * (float(step) / float(max(1, warmup_steps)))\n",
    "    elif step < max_steps:\n",
    "        progress = (float(step - warmup_steps) / float(max(1, max_steps - warmup_steps)))\n",
    "        cos_component = 0.5 * (1 + math.cos(math.pi * progress))\n",
    "        return final_lr + (initial_lr - final_lr) * cos_component\n",
    "    else:\n",
    "        return final_lr\n",
    "\n",
    "def update_ema_variables(model, ema_model, alpha):\n",
    "    # Update the EMA model parameters\n",
    "    with torch.no_grad():\n",
    "        for ema_param, param in zip(ema_model.parameters(), model.parameters()):\n",
    "            ema_param.data.mul_(alpha).add_(param.data, alpha=1 - alpha)\n",
    "\n",
    "save_dir = f\"vit_mim_upscale_finetune_models/fold_{I_FOLD}\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "scaler = GradScaler()\n",
    "optimizer = optim.AdamW(classifier_model.parameters(), lr=initial_lr, weight_decay=5e-2)\n",
    "# state_dict = torch.load('vit_mim_upscale_models/fold_0/optimizer_epoch_0_step_9000.pth', map_location=device)\n",
    "# optimizer.load_state_dict(state_dict)\n",
    "\n",
    "# Calculate weights for each class\n",
    "class_counts = np.array([train.groupby('label').count().loc[label]['image_id'] for label in label_to_integer], dtype=np.float32)\n",
    "class_weights = 1. / class_counts\n",
    "class_weights /= class_weights.sum()\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss(weight=class_weights, label_smoothing=0.1)\n",
    "\n",
    "best_val_accuracy = 0.0\n",
    "step = 0\n",
    "\n",
    "classifier_model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels, _) in enumerate(train_dataloader, 0):\n",
    "        # Convert images to PIL format\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Linearly increase the learning rate\n",
    "        lr = learning_rate(step)\n",
    "        for g in optimizer.param_groups:\n",
    "            g['lr'] = lr\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass with autocast\n",
    "        with autocast():\n",
    "            outputs = classifier_model(images)\n",
    "            logits_per_image = outputs\n",
    "            loss = criterion(logits_per_image, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        update_ema_variables(classifier_model, ema_classifier_model, ema_decay)\n",
    "\n",
    "        logging.info('[%d, %5d] loss: %.3f' % (epoch + 1, step, loss.item()))\n",
    "\n",
    "        if step % 500 == 0:\n",
    "            # Save the EMA model\n",
    "            ema_model_save_path = os.path.join(save_dir, f'ema_classifier_model_epoch_{epoch}_step_{step}.pth')\n",
    "            torch.save(ema_classifier_model.state_dict(), ema_model_save_path)\n",
    "            \n",
    "            # Save the model\n",
    "            model_save_path = os.path.join(save_dir, f'classifier_model_epoch_{epoch}_step_{step}.pth')\n",
    "            torch.save(classifier_model.state_dict(), model_save_path)\n",
    "\n",
    "            # Save the optimizer\n",
    "            optimizer_save_path = os.path.join(save_dir, f'optimizer_epoch_{epoch}_step_{step}.pth')\n",
    "            torch.save(optimizer.state_dict(), optimizer_save_path)\n",
    "\n",
    "            logging.info(f'Model and optimizer saved after epoch {epoch} and step {step}')\n",
    "\n",
    "        step += 1\n"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.2-0.m112",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.2-0:m112"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
