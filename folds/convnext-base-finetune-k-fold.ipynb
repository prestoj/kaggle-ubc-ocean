{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82500098-49ea-4a7e-bb63-ce5130f954cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>label</th>\n",
       "      <th>image_width</th>\n",
       "      <th>image_height</th>\n",
       "      <th>is_tma</th>\n",
       "      <th>tile_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>HGSC</td>\n",
       "      <td>23785</td>\n",
       "      <td>20008</td>\n",
       "      <td>False</td>\n",
       "      <td>../tiles_768/4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>66</td>\n",
       "      <td>LGSC</td>\n",
       "      <td>48871</td>\n",
       "      <td>48195</td>\n",
       "      <td>False</td>\n",
       "      <td>../tiles_768/66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>91</td>\n",
       "      <td>HGSC</td>\n",
       "      <td>3388</td>\n",
       "      <td>3388</td>\n",
       "      <td>True</td>\n",
       "      <td>../tiles_768/91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>281</td>\n",
       "      <td>LGSC</td>\n",
       "      <td>42309</td>\n",
       "      <td>15545</td>\n",
       "      <td>False</td>\n",
       "      <td>../tiles_768/281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>286</td>\n",
       "      <td>EC</td>\n",
       "      <td>37204</td>\n",
       "      <td>30020</td>\n",
       "      <td>False</td>\n",
       "      <td>../tiles_768/286</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   image_id label  image_width  image_height  is_tma         tile_path\n",
       "0         4  HGSC        23785         20008   False    ../tiles_768/4\n",
       "1        66  LGSC        48871         48195   False   ../tiles_768/66\n",
       "2        91  HGSC         3388          3388    True   ../tiles_768/91\n",
       "3       281  LGSC        42309         15545   False  ../tiles_768/281\n",
       "4       286    EC        37204         30020   False  ../tiles_768/286"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def get_image_path(image_id:int):\n",
    "    return os.path.join('../tiles_768', str(image_id))\n",
    "\n",
    "I_FOLD = 3\n",
    "train = pd.read_csv(f\"train_fold_{I_FOLD}.csv\")\n",
    "validation = pd.read_csv(f\"val_fold_{I_FOLD}.csv\")\n",
    "\n",
    "train['tile_path'] = train['image_id'].apply(lambda x: get_image_path(x))\n",
    "validation['tile_path'] = validation['image_id'].apply(lambda x: get_image_path(x))\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a53ed4b-02fa-451d-b948-08167c29f05d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nFOR FOLD_3 I USE DROP PATH RATE = 0.5\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "FOR FOLD_3 I USE DROP PATH RATE = 0.5\n",
    "\"\"\"\n",
    "\n",
    "# import json\n",
    "\n",
    "# # Open the JSON file for reading\n",
    "# with open('./convnextv2_base_config/config.json', 'r') as file:\n",
    "#     data = json.load(file)\n",
    "    \n",
    "# data['drop_path_rate'] = 0.5\n",
    "\n",
    "# with open('./convnextv2_base_config/config.json', 'w') as file:\n",
    "#     json.dump(data, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d782638-2c6c-4b63-87b3-c7bf291bbde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import ConvNextV2ForImageClassification\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# model_name = \"facebook/convnextv2-base-22k-384\"\n",
    "# print(f\"Using device {device} and model {model_name}\")\n",
    "\n",
    "model = ConvNextV2ForImageClassification.from_pretrained('./convnextv2_base_config')\n",
    "\n",
    "model.classifier = nn.Linear(model.classifier.in_features, 5)\n",
    "\n",
    "# state_dict = torch.load('vit_models_1/epoch_249_step_3500.pth', map_location=device)\n",
    "# model.load_state_dict(state_dict, strict=False)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9613dea-4edf-47cd-a042-5f3468b19d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "integer_to_label = {\n",
    "    0: 'HGSC',\n",
    "    1: 'CC',\n",
    "    2: 'EC',\n",
    "    3: 'LGSC',\n",
    "    4: 'MC',\n",
    "}\n",
    "\n",
    "label_to_integer = {\n",
    "    'HGSC': 0,\n",
    "    'CC': 1,\n",
    "    'EC': 2,\n",
    "    'LGSC': 3,\n",
    "    'MC': 4,\n",
    "}\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.transform = transform\n",
    "        self.folder_paths = []\n",
    "        self.labels = []\n",
    "        self.image_ids = []\n",
    "        for index, row in dataframe.iterrows():\n",
    "            folder_path = row['tile_path']\n",
    "            label = row['label']\n",
    "            image_id = row['image_id']\n",
    "            if os.path.isdir(folder_path):  # Check if the folder_path is a valid directory\n",
    "                self.folder_paths.append(folder_path)\n",
    "                self.labels.append(label)\n",
    "                self.image_ids.append(image_id)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_paths = os.listdir(self.folder_paths[idx])\n",
    "        image_index = random.randint(0, len(image_paths) - 1)\n",
    "        while not image_paths[image_index].lower().endswith('.png'):  # Check if the file is a PNG\n",
    "            image_index = random.randint(0, len(image_paths) - 1)\n",
    "\n",
    "        image = Image.open(os.path.join(self.folder_paths[idx], image_paths[image_index]))\n",
    "        label = self.labels[idx]\n",
    "        image_id = self.image_ids[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label_to_integer[label], image_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "daab3db2-8fff-4b14-83f9-608c256dea11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.1),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.RandomAffine(degrees=45, translate=(0.25, 0.25), scale=(1, 2), shear=(-30, 30, -30, 30)),\n",
    "    transforms.Resize(384),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "    transforms.RandomErasing(p=0.5, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=0, inplace=False),\n",
    "    transforms.RandomErasing(p=0.5, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=0, inplace=False),\n",
    "    transforms.RandomErasing(p=0.5, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=0, inplace=False),\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize(384),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "])\n",
    "\n",
    "train_dataset = ImageDataset(dataframe=train, transform=train_transform)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, num_workers=3, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0aa0e2bd-4c21-48a1-a0fe-5eb15fb556b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "# Get the root logger\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# Optional: Remove all existing handlers from the logger\n",
    "for handler in logger.handlers[:]:\n",
    "    logger.removeHandler(handler)\n",
    "\n",
    "# Set the logging level\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Create a FileHandler and add it to the logger\n",
    "file_handler = logging.FileHandler(f'logs/convnextv2_base/fold_{I_FOLD}.txt')\n",
    "file_handler.setLevel(logging.INFO)\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "# Create a StreamHandler for stderr and add it to the logger\n",
    "stream_handler = logging.StreamHandler(sys.stderr)\n",
    "stream_handler.setLevel(logging.ERROR)  # Only log ERROR and CRITICAL messages to stderr\n",
    "logger.addHandler(stream_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a2e1e02-2ae6-4f5d-a8d1-1f238c34da88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>image_width</th>\n",
       "      <th>image_height</th>\n",
       "      <th>is_tma</th>\n",
       "      <th>tile_path</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CC</th>\n",
       "      <td>79</td>\n",
       "      <td>79</td>\n",
       "      <td>79</td>\n",
       "      <td>79</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EC</th>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HGSC</th>\n",
       "      <td>178</td>\n",
       "      <td>178</td>\n",
       "      <td>178</td>\n",
       "      <td>178</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGSC</th>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MC</th>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       image_id  image_width  image_height  is_tma  tile_path\n",
       "label                                                        \n",
       "CC           79           79            79      79         79\n",
       "EC          100          100           100     100        100\n",
       "HGSC        178          178           178     178        178\n",
       "LGSC         37           37            37      37         37\n",
       "MC           37           37            37      37         37"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.groupby('label').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f9b938-9860-4147-be7e-f6f36938f212",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import logging\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "import random\n",
    "import copy\n",
    "\n",
    "initial_lr = 1e-5\n",
    "final_lr = 1e-6\n",
    "num_epochs = 10000\n",
    "\n",
    "# Function for linear warmup\n",
    "def learning_rate(step, warmup_steps=500, max_steps=20000):\n",
    "    if step < warmup_steps:\n",
    "        return initial_lr * (float(step) / float(max(1, warmup_steps)))\n",
    "    elif step < max_steps:\n",
    "        progress = (float(step - warmup_steps) / float(max(1, max_steps - warmup_steps)))\n",
    "        cos_component = 0.5 * (1 + math.cos(math.pi * progress))\n",
    "        return final_lr + (initial_lr - final_lr) * cos_component\n",
    "    else:\n",
    "        return final_lr\n",
    "\n",
    "# Function to calculate weighted accuracy\n",
    "def weighted_accuracy(true_labels, predictions, class_weights):\n",
    "    correct = 0\n",
    "    total_weight = 0\n",
    "\n",
    "    for label, pred in zip(true_labels, predictions):\n",
    "        if label == pred:\n",
    "            correct += class_weights[label]\n",
    "        total_weight += class_weights[label]\n",
    "\n",
    "    return correct / total_weight\n",
    "\n",
    "def update_ema_variables(model, ema_model, alpha, global_step):\n",
    "    # Update the EMA model parameters\n",
    "    with torch.no_grad():\n",
    "        for ema_param, param in zip(ema_model.parameters(), model.parameters()):\n",
    "            ema_param.data.mul_(alpha).add_(param.data, alpha=1 - alpha)\n",
    "\n",
    "# Initialize EMA model\n",
    "ema_decay = 0.999  # decay factor for EMA\n",
    "ema_model = copy.deepcopy(model)\n",
    "ema_model.to(device)\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.AdamW(model.parameters(), lr=initial_lr, weight_decay=5e-2)\n",
    "\n",
    "# Calculate class weights\n",
    "class_counts = np.array([178, 79, 100, 37, 37], dtype=np.float32)\n",
    "class_weights = 1. / class_counts\n",
    "class_weights /= class_weights.sum()\n",
    "\n",
    "# Convert class weights to tensor\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "# Define the loss function with class weights\n",
    "criterion = torch.nn.CrossEntropyLoss(weight=class_weights, label_smoothing=0.1)\n",
    "\n",
    "best_val_accuracy = 0.0\n",
    "step = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # set the model to training mode\n",
    "    \n",
    "    for i, (images, labels, _) in enumerate(train_dataloader, 0):\n",
    "        # Convert images to PIL format\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Linearly increase the learning rate\n",
    "        lr = learning_rate(step)\n",
    "        for g in optimizer.param_groups:\n",
    "            g['lr'] = lr\n",
    "            \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(images)\n",
    "        logits_per_image = outputs.logits\n",
    "        loss = criterion(logits_per_image, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        update_ema_variables(model, ema_model, ema_decay, step)\n",
    "\n",
    "        logging.info('[%d, %5d] loss: %.3f' % (epoch + 1, step, loss.item()))\n",
    "\n",
    "        if step % 500 == 0:\n",
    "            ema_model.eval()\n",
    "\n",
    "            tma_preds = []\n",
    "            tma_labels = []\n",
    "            \n",
    "            non_tma_preds = []\n",
    "            non_tma_labels = []\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for _, row in validation.iterrows():\n",
    "                    path = row['tile_path']\n",
    "                    all_files = [f for f in os.listdir(path) if f.lower().endswith('.png')]\n",
    "                    \n",
    "                    probabilities = torch.zeros(5).to(device)\n",
    "\n",
    "                    # Prepare a list to hold image tiles\n",
    "                    batch_tiles = []\n",
    "\n",
    "                    sample_size = min(16, len(all_files))\n",
    "                    sampled_files = random.sample(all_files, sample_size)\n",
    "\n",
    "                    for image_name in sampled_files:\n",
    "                        image_path = os.path.join(path, image_name)\n",
    "                        sub_image = Image.open(image_path)\n",
    "\n",
    "                        tile = val_transform(sub_image).unsqueeze(0).to(device)\n",
    "                        batch_tiles.append(tile)\n",
    "\n",
    "                    outputs = ema_model(torch.concat(batch_tiles, dim=0))\n",
    "                    probs = outputs.logits.softmax(dim=1)\n",
    "                    probabilities += probs[0]\n",
    "\n",
    "                    pred_label = integer_to_label[probabilities.argmax().detach().cpu().item()]\n",
    "                    label = row['label']\n",
    "                    if row['is_tma']:\n",
    "                        tma_preds.append(pred_label)\n",
    "                        tma_labels.append(label)\n",
    "                    else:\n",
    "                        non_tma_preds.append(pred_label)\n",
    "                        non_tma_labels.append(label)\n",
    "\n",
    "            tma_accuracy = balanced_accuracy_score(tma_labels, tma_preds)\n",
    "            non_tma_accuracy = balanced_accuracy_score(non_tma_labels, non_tma_preds)\n",
    "            accuracy = (tma_accuracy + non_tma_accuracy) / 2\n",
    "            logging.info(f'TMA Accuracy: {tma_accuracy} | Non-TMA Accuracy: {non_tma_accuracy} | Overall Accuracy: {accuracy}')\n",
    "\n",
    "            if accuracy > best_val_accuracy:\n",
    "                best_val_accuracy = accuracy\n",
    "                torch.save(ema_model.state_dict(), f'convnextv2_base_models/fold_{I_FOLD}/epoch_{epoch}_step_{step}.pth')\n",
    "                logging.info(f'Model saved after epoch {epoch} and step {step}')\n",
    "                \n",
    "            model.train()\n",
    "\n",
    "        if step == 20000:\n",
    "            torch.save(ema_model.state_dict(), f'convnextv2_base_models/fold_{I_FOLD}/final.pth')\n",
    "\n",
    "        step += 1"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.2-0.m112",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.2-0:m112"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
