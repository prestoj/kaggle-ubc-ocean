[1,     0] loss: 0.815
Model saved after epoch 0 and step 0
[1,     1] loss: 0.828
[1,     2] loss: 0.772
[1,     3] loss: 0.701
[1,     4] loss: 0.710
[1,     5] loss: 0.669
[1,     6] loss: 0.631
[1,     7] loss: 0.576
[1,     8] loss: 0.536
[1,     9] loss: 0.557
[1,    10] loss: 0.483
[1,    11] loss: 0.423
[1,    12] loss: 0.393
[1,    13] loss: 0.338
[1,    14] loss: 0.340
[1,    15] loss: 0.292
[1,    16] loss: 0.319
[1,    17] loss: 0.317
[1,    18] loss: 0.307
[1,    19] loss: 0.294
[1,    20] loss: 0.299
[1,    21] loss: 0.299
[1,    22] loss: 0.318
[1,    23] loss: 0.307
[1,    24] loss: 0.303
[1,    25] loss: 0.287
[1,    26] loss: 0.306
[1,    27] loss: 0.296
[1,    28] loss: 0.289
[1,    29] loss: 0.271
[1,    30] loss: 0.263
[1,    31] loss: 0.268
[1,    32] loss: 0.253
[1,    33] loss: 0.248
[1,    34] loss: 0.258
[1,    35] loss: 0.268
[1,    36] loss: 0.251
[1,    37] loss: 0.244
[1,    38] loss: 0.259
[1,    39] loss: 0.251
[1,    40] loss: 0.260
[1,    41] loss: 0.260
[1,    42] loss: 0.246
[1,    43] loss: 0.239
[1,    44] loss: 0.250
[1,    45] loss: 0.238
[1,    46] loss: 0.251
[1,    47] loss: 0.251
[1,    48] loss: 0.244
[1,    49] loss: 0.233
[1,    50] loss: 0.244
[1,    51] loss: 0.260
[1,    52] loss: 0.223
[1,    53] loss: 0.235
[1,    54] loss: 0.249
[1,    55] loss: 0.221
[1,    56] loss: 0.233
[1,    57] loss: 0.217
[1,    58] loss: 0.220
[1,    59] loss: 0.226
[1,    60] loss: 0.228
[1,    61] loss: 0.231
[1,    62] loss: 0.235
[1,    63] loss: 0.219
[1,    64] loss: 0.214
[1,    65] loss: 0.210
[1,    66] loss: 0.215
[1,    67] loss: 0.206
[1,    68] loss: 0.210
[1,    69] loss: 0.194
[1,    70] loss: 0.204
[1,    71] loss: 0.200
[1,    72] loss: 0.199
[1,    73] loss: 0.194
[1,    74] loss: 0.185
[1,    75] loss: 0.202
[1,    76] loss: 0.192
[1,    77] loss: 0.179
[1,    78] loss: 0.181
[1,    79] loss: 0.189
[1,    80] loss: 0.190
[1,    81] loss: 0.189
[1,    82] loss: 0.182
[1,    83] loss: 0.167
[1,    84] loss: 0.178
[1,    85] loss: 0.193
[1,    86] loss: 0.179
[1,    87] loss: 0.163
[1,    88] loss: 0.153
[1,    89] loss: 0.177
[1,    90] loss: 0.161
[1,    91] loss: 0.175
[1,    92] loss: 0.161
[1,    93] loss: 0.176
[1,    94] loss: 0.169
[1,    95] loss: 0.176
[1,    96] loss: 0.171
[1,    97] loss: 0.172
[1,    98] loss: 0.177
[1,    99] loss: 0.184
[1,   100] loss: 0.164
Model saved after epoch 0 and step 100
[1,   101] loss: 0.162
[1,   102] loss: 0.166
[1,   103] loss: 0.171
[1,   104] loss: 0.155
[1,   105] loss: 0.162
[1,   106] loss: 0.163
[1,   107] loss: 0.149
[1,   108] loss: 0.140
[1,   109] loss: 0.145
[1,   110] loss: 0.140
[1,   111] loss: 0.147
[1,   112] loss: 0.136
[1,   113] loss: 0.142
[1,   114] loss: 0.136
[1,   115] loss: 0.157
[1,   116] loss: 0.130
[1,   117] loss: 0.149
[1,   118] loss: 0.179
[1,   119] loss: 0.161
[1,   120] loss: 0.145
[1,   121] loss: 0.136
[1,   122] loss: 0.137
[1,   123] loss: 0.135
[1,   124] loss: 0.144
[1,   125] loss: 0.134
[1,   126] loss: 0.131
[1,   127] loss: 0.132
[1,   128] loss: 0.129
[1,   129] loss: 0.156
[1,   130] loss: 0.127
[1,   131] loss: 0.130
[1,   132] loss: 0.132
[1,   133] loss: 0.134
[1,   134] loss: 0.138
[1,   135] loss: 0.137
[1,   136] loss: 0.131
[1,   137] loss: 0.143
[1,   138] loss: 0.135
[1,   139] loss: 0.128
[1,   140] loss: 0.128
[1,   141] loss: 0.142
[1,   142] loss: 0.139
[1,   143] loss: 0.124
[1,   144] loss: 0.131
[1,   145] loss: 0.127
[1,   146] loss: 0.133
[1,   147] loss: 0.135
[1,   148] loss: 0.135
[1,   149] loss: 0.141
[1,   150] loss: 0.137
[1,   151] loss: 0.140
[1,   152] loss: 0.125
[1,   153] loss: 0.134
[1,   154] loss: 0.129
[1,   155] loss: 0.127
[1,   156] loss: 0.129
[1,   157] loss: 0.114
[1,   158] loss: 0.126
[1,   159] loss: 0.123
[1,   160] loss: 0.129
[1,   161] loss: 0.119
[1,   162] loss: 0.116
[1,   163] loss: 0.108
[1,   164] loss: 0.122
[1,   165] loss: 0.124
[1,   166] loss: 0.130
[1,   167] loss: 0.126
[1,   168] loss: 0.119
[1,   169] loss: 0.100
[1,   170] loss: 0.131
[1,   171] loss: 0.108
[1,   172] loss: 0.113
[1,   173] loss: 0.116
[1,   174] loss: 0.116
[1,   175] loss: 0.116
[1,   176] loss: 0.104
[1,   177] loss: 0.116
[1,   178] loss: 0.113
[1,   179] loss: 0.125
[1,   180] loss: 0.115
[1,   181] loss: 0.122
[1,   182] loss: 0.113
[1,   183] loss: 0.116
[1,   184] loss: 0.113
[1,   185] loss: 0.117
[1,   186] loss: 0.122
[1,   187] loss: 0.103
[1,   188] loss: 0.102
[1,   189] loss: 0.110
[1,   190] loss: 0.103
[1,   191] loss: 0.112
[1,   192] loss: 0.105
[1,   193] loss: 0.104
[1,   194] loss: 0.103
[1,   195] loss: 0.099
[1,   196] loss: 0.093
[1,   197] loss: 0.107
[1,   198] loss: 0.103
[1,   199] loss: 0.100
[1,   200] loss: 0.108
Model saved after epoch 0 and step 200
[1,   201] loss: 0.103
[1,   202] loss: 0.118
[1,   203] loss: 0.115
[1,   204] loss: 0.114
[1,   205] loss: 0.121
[1,   206] loss: 0.110
[1,   207] loss: 0.110
[1,   208] loss: 0.124
[1,   209] loss: 0.107
[1,   210] loss: 0.128
[1,   211] loss: 0.109
[1,   212] loss: 0.113
[1,   213] loss: 0.110
[1,   214] loss: 0.123
[1,   215] loss: 0.114
[1,   216] loss: 0.113
[1,   217] loss: 0.101
[1,   218] loss: 0.105
[1,   219] loss: 0.107
[1,   220] loss: 0.113
[1,   221] loss: 0.111
[1,   222] loss: 0.112
[1,   223] loss: 0.106
[1,   224] loss: 0.110
[1,   225] loss: 0.100
[1,   226] loss: 0.103
[1,   227] loss: 0.090
[1,   228] loss: 0.101
[1,   229] loss: 0.115
[1,   230] loss: 0.109
[1,   231] loss: 0.103
[1,   232] loss: 0.096
[1,   233] loss: 0.098
[1,   234] loss: 0.099
[1,   235] loss: 0.107
[1,   236] loss: 0.101
[1,   237] loss: 0.102
[1,   238] loss: 0.110
[1,   239] loss: 0.098
[1,   240] loss: 0.109
[1,   241] loss: 0.098
[1,   242] loss: 0.115
[1,   243] loss: 0.094
[1,   244] loss: 0.102
[1,   245] loss: 0.102
[1,   246] loss: 0.099
[1,   247] loss: 0.095
[1,   248] loss: 0.106
[1,   249] loss: 0.090
[1,   250] loss: 0.090
[1,   251] loss: 0.091
[1,   252] loss: 0.094
[1,   253] loss: 0.105
[1,   254] loss: 0.107
[1,   255] loss: 0.086
[1,   256] loss: 0.101
[1,   257] loss: 0.096
[1,   258] loss: 0.106
[1,   259] loss: 0.105
[1,   260] loss: 0.099
[1,   261] loss: 0.105
[1,   262] loss: 0.092
[1,   263] loss: 0.103
[1,   264] loss: 0.090
[1,   265] loss: 0.086
[1,   266] loss: 0.098
[1,   267] loss: 0.105
[1,   268] loss: 0.098
[1,   269] loss: 0.107
[1,   270] loss: 0.089
[1,   271] loss: 0.096
[1,   272] loss: 0.092
[1,   273] loss: 0.088
[1,   274] loss: 0.093
[1,   275] loss: 0.099
[1,   276] loss: 0.095
[1,   277] loss: 0.091
[1,   278] loss: 0.094
[1,   279] loss: 0.096
[1,   280] loss: 0.090
[1,   281] loss: 0.090
[1,   282] loss: 0.091
[1,   283] loss: 0.095
[1,   284] loss: 0.090
[1,   285] loss: 0.088
[1,   286] loss: 0.082
[1,   287] loss: 0.090
[1,   288] loss: 0.090
[1,   289] loss: 0.092
[1,   290] loss: 0.088
[1,   291] loss: 0.098
[1,   292] loss: 0.104
[1,   293] loss: 0.104
[1,   294] loss: 0.089
[1,   295] loss: 0.090
[1,   296] loss: 0.091
[1,   297] loss: 0.093
[1,   298] loss: 0.091
[1,   299] loss: 0.087
[1,   300] loss: 0.090
Model saved after epoch 0 and step 300
[1,   301] loss: 0.098
[1,   302] loss: 0.091
[1,   303] loss: 0.085
[1,   304] loss: 0.087
[1,   305] loss: 0.097
[1,   306] loss: 0.102
[1,   307] loss: 0.080
[1,   308] loss: 0.093
[1,   309] loss: 0.087
[1,   310] loss: 0.089
[1,   311] loss: 0.086
[1,   312] loss: 0.088
[1,   313] loss: 0.097
[1,   314] loss: 0.080
[1,   315] loss: 0.079
[1,   316] loss: 0.094
[1,   317] loss: 0.079
[1,   318] loss: 0.080
[1,   319] loss: 0.090
[1,   320] loss: 0.091
[1,   321] loss: 0.092
[1,   322] loss: 0.085
[1,   323] loss: 0.079
[1,   324] loss: 0.088
[1,   325] loss: 0.078
[1,   326] loss: 0.080
[1,   327] loss: 0.087
[1,   328] loss: 0.085
[1,   329] loss: 0.088
[1,   330] loss: 0.075
[1,   331] loss: 0.078
[1,   332] loss: 0.081
[1,   333] loss: 0.079
[1,   334] loss: 0.074
[1,   335] loss: 0.078
[1,   336] loss: 0.081
[1,   337] loss: 0.094
[1,   338] loss: 0.086
[1,   339] loss: 0.084
[1,   340] loss: 0.076
[1,   341] loss: 0.080
[1,   342] loss: 0.075
[1,   343] loss: 0.075
[1,   344] loss: 0.076
[1,   345] loss: 0.088
[1,   346] loss: 0.075
[1,   347] loss: 0.076
[1,   348] loss: 0.080
[1,   349] loss: 0.082
[1,   350] loss: 0.081
[1,   351] loss: 0.070
[1,   352] loss: 0.090
[1,   353] loss: 0.083
[1,   354] loss: 0.081
[1,   355] loss: 0.076
[1,   356] loss: 0.082
[1,   357] loss: 0.083
[1,   358] loss: 0.075
[1,   359] loss: 0.072
[1,   360] loss: 0.075
[1,   361] loss: 0.083
[1,   362] loss: 0.075
[1,   363] loss: 0.076
[1,   364] loss: 0.072
[1,   365] loss: 0.081
[1,   366] loss: 0.083
[1,   367] loss: 0.080
[1,   368] loss: 0.081
[1,   369] loss: 0.087
[1,   370] loss: 0.077
[1,   371] loss: 0.077
[1,   372] loss: 0.083
[1,   373] loss: 0.070
[1,   374] loss: 0.076
[1,   375] loss: 0.087
[1,   376] loss: 0.082
[1,   377] loss: 0.081
[1,   378] loss: 0.076
[1,   379] loss: 0.077
[1,   380] loss: 0.079
[1,   381] loss: 0.073
[1,   382] loss: 0.077
[1,   383] loss: 0.069
[1,   384] loss: 0.076
[1,   385] loss: 0.074
[1,   386] loss: 0.071
[1,   387] loss: 0.071
[1,   388] loss: 0.082
[1,   389] loss: 0.077
[1,   390] loss: 0.077
[1,   391] loss: 0.077
[1,   392] loss: 0.077
[1,   393] loss: 0.070
[1,   394] loss: 0.075
[1,   395] loss: 0.071
[1,   396] loss: 0.073
[1,   397] loss: 0.071
[1,   398] loss: 0.071
[1,   399] loss: 0.076
[1,   400] loss: 0.079
Model saved after epoch 0 and step 400
[1,   401] loss: 0.085
[1,   402] loss: 0.081
[1,   403] loss: 0.079
[1,   404] loss: 0.069
[1,   405] loss: 0.080
[1,   406] loss: 0.080
[1,   407] loss: 0.074
[1,   408] loss: 0.071
[1,   409] loss: 0.076
[1,   410] loss: 0.075
[1,   411] loss: 0.073
[1,   412] loss: 0.081
[1,   413] loss: 0.072
[1,   414] loss: 0.080
[1,   415] loss: 0.070
[1,   416] loss: 0.075
[1,   417] loss: 0.080
[1,   418] loss: 0.067
[1,   419] loss: 0.078
[1,   420] loss: 0.069
[1,   421] loss: 0.081
[1,   422] loss: 0.070
[1,   423] loss: 0.081
[1,   424] loss: 0.075
[1,   425] loss: 0.079
[1,   426] loss: 0.064
[1,   427] loss: 0.067
[1,   428] loss: 0.072
[1,   429] loss: 0.078
[1,   430] loss: 0.072
[1,   431] loss: 0.067
[1,   432] loss: 0.068
[1,   433] loss: 0.080
[1,   434] loss: 0.074
[1,   435] loss: 0.076
[1,   436] loss: 0.071
[1,   437] loss: 0.070
[1,   438] loss: 0.065
[1,   439] loss: 0.072
[1,   440] loss: 0.079
[1,   441] loss: 0.070
[1,   442] loss: 0.070
[1,   443] loss: 0.064
[1,   444] loss: 0.067
[1,   445] loss: 0.063
[1,   446] loss: 0.069
[1,   447] loss: 0.064
[1,   448] loss: 0.070
[1,   449] loss: 0.079
[1,   450] loss: 0.063
[1,   451] loss: 0.068
[1,   452] loss: 0.077
[1,   453] loss: 0.067
[1,   454] loss: 0.060
[1,   455] loss: 0.070
[1,   456] loss: 0.067
[1,   457] loss: 0.065
[1,   458] loss: 0.067
[1,   459] loss: 0.063
[1,   460] loss: 0.061
[1,   461] loss: 0.065
[1,   462] loss: 0.070
[1,   463] loss: 0.063
[1,   464] loss: 0.065
[1,   465] loss: 0.068
[1,   466] loss: 0.061
[1,   467] loss: 0.064
[1,   468] loss: 0.066
[1,   469] loss: 0.063
[1,   470] loss: 0.070
[1,   471] loss: 0.065
[1,   472] loss: 0.071
[1,   473] loss: 0.072
[1,   474] loss: 0.066
[1,   475] loss: 0.064
[1,   476] loss: 0.067
[1,   477] loss: 0.066
[1,   478] loss: 0.071
[1,   479] loss: 0.066
[1,   480] loss: 0.072
[1,   481] loss: 0.064
[1,   482] loss: 0.081
[1,   483] loss: 0.068
[1,   484] loss: 0.064
[1,   485] loss: 0.068
[1,   486] loss: 0.070
[1,   487] loss: 0.068
[1,   488] loss: 0.066
[1,   489] loss: 0.073
[1,   490] loss: 0.065
[1,   491] loss: 0.064
[1,   492] loss: 0.069
[1,   493] loss: 0.068
[1,   494] loss: 0.065
[1,   495] loss: 0.068
[1,   496] loss: 0.059
[1,   497] loss: 0.070
[1,   498] loss: 0.070
[1,   499] loss: 0.059
[1,   500] loss: 0.068
Model saved after epoch 0 and step 500
[1,   501] loss: 0.058
[1,   502] loss: 0.063
[1,   503] loss: 0.063
[1,   504] loss: 0.062
[1,   505] loss: 0.071
[1,   506] loss: 0.068
[1,   507] loss: 0.065
[1,   508] loss: 0.071
[1,   509] loss: 0.066
[1,   510] loss: 0.063
[1,   511] loss: 0.071
[1,   512] loss: 0.070
[1,   513] loss: 0.059
[1,   514] loss: 0.064
[1,   515] loss: 0.063
[1,   516] loss: 0.067
[1,   517] loss: 0.071
[1,   518] loss: 0.070
[1,   519] loss: 0.063
[1,   520] loss: 0.057
[1,   521] loss: 0.058
[1,   522] loss: 0.066
[1,   523] loss: 0.060
[1,   524] loss: 0.057
[1,   525] loss: 0.066
[1,   526] loss: 0.063
[1,   527] loss: 0.062
[1,   528] loss: 0.063
[1,   529] loss: 0.068
[1,   530] loss: 0.062
[1,   531] loss: 0.060
[1,   532] loss: 0.065
[1,   533] loss: 0.062
[1,   534] loss: 0.054
[1,   535] loss: 0.062
[1,   536] loss: 0.062
[1,   537] loss: 0.061
[1,   538] loss: 0.066
[1,   539] loss: 0.066
[1,   540] loss: 0.058
[1,   541] loss: 0.055
[1,   542] loss: 0.055
[1,   543] loss: 0.065
[1,   544] loss: 0.059
[1,   545] loss: 0.058
[1,   546] loss: 0.061
[1,   547] loss: 0.065
[1,   548] loss: 0.062
[1,   549] loss: 0.060
[1,   550] loss: 0.061
[1,   551] loss: 0.062
[1,   552] loss: 0.060
[1,   553] loss: 0.065
[1,   554] loss: 0.069
[1,   555] loss: 0.059
[1,   556] loss: 0.059
[1,   557] loss: 0.067
[1,   558] loss: 0.056
[1,   559] loss: 0.058
[1,   560] loss: 0.058
[1,   561] loss: 0.062
[1,   562] loss: 0.057
[1,   563] loss: 0.059
[1,   564] loss: 0.062
[1,   565] loss: 0.069
[1,   566] loss: 0.056
[1,   567] loss: 0.068
[1,   568] loss: 0.054
[1,   569] loss: 0.052
[1,   570] loss: 0.063
[1,   571] loss: 0.065
[1,   572] loss: 0.064
[1,   573] loss: 0.063
[1,   574] loss: 0.070
[1,   575] loss: 0.065
[1,   576] loss: 0.066
[1,   577] loss: 0.060
[1,   578] loss: 0.063
[1,   579] loss: 0.057
[1,   580] loss: 0.061
[1,   581] loss: 0.060
[1,   582] loss: 0.061
[1,   583] loss: 0.062
[1,   584] loss: 0.059
[1,   585] loss: 0.059
[1,   586] loss: 0.062
[1,   587] loss: 0.054
[1,   588] loss: 0.055
[1,   589] loss: 0.055
[1,   590] loss: 0.059
[1,   591] loss: 0.055
[1,   592] loss: 0.060
[1,   593] loss: 0.060
[1,   594] loss: 0.059
[1,   595] loss: 0.065
[1,   596] loss: 0.069
[1,   597] loss: 0.062
[1,   598] loss: 0.055
[1,   599] loss: 0.057
[1,   600] loss: 0.058
Model saved after epoch 0 and step 600
[1,   601] loss: 0.067
[1,   602] loss: 0.063
[1,   603] loss: 0.063
[1,   604] loss: 0.059
[1,   605] loss: 0.058
[1,   606] loss: 0.056
[1,   607] loss: 0.059
[1,   608] loss: 0.057
[1,   609] loss: 0.067
[1,   610] loss: 0.063
[1,   611] loss: 0.062
[1,   612] loss: 0.053
[1,   613] loss: 0.062
[1,   614] loss: 0.055
[1,   615] loss: 0.057
[1,   616] loss: 0.061
[1,   617] loss: 0.055
[1,   618] loss: 0.055
[1,   619] loss: 0.060
[1,   620] loss: 0.060
[1,   621] loss: 0.060
[1,   622] loss: 0.058
[1,   623] loss: 0.057
[1,   624] loss: 0.056
[1,   625] loss: 0.060
[1,   626] loss: 0.058
[1,   627] loss: 0.053
[1,   628] loss: 0.065
[1,   629] loss: 0.061
[1,   630] loss: 0.057
[1,   631] loss: 0.056
[1,   632] loss: 0.063
[1,   633] loss: 0.057
[1,   634] loss: 0.055
[1,   635] loss: 0.057
[1,   636] loss: 0.055
[1,   637] loss: 0.061
[1,   638] loss: 0.059
[1,   639] loss: 0.058
[1,   640] loss: 0.051
[1,   641] loss: 0.053
[1,   642] loss: 0.050
[1,   643] loss: 0.059
[1,   644] loss: 0.054
[1,   645] loss: 0.054
[1,   646] loss: 0.049
[1,   647] loss: 0.050
[1,   648] loss: 0.057
[1,   649] loss: 0.058
[1,   650] loss: 0.056
[1,   651] loss: 0.060
[1,   652] loss: 0.060
[1,   653] loss: 0.054
[1,   654] loss: 0.052
[1,   655] loss: 0.056
[1,   656] loss: 0.051
[1,   657] loss: 0.054
[1,   658] loss: 0.051
[1,   659] loss: 0.059
[1,   660] loss: 0.057
[1,   661] loss: 0.052
[1,   662] loss: 0.055
[1,   663] loss: 0.056
[1,   664] loss: 0.058
[1,   665] loss: 0.056
[1,   666] loss: 0.058
[1,   667] loss: 0.061
[1,   668] loss: 0.052
[1,   669] loss: 0.059
[1,   670] loss: 0.057
[1,   671] loss: 0.058
[1,   672] loss: 0.051
[1,   673] loss: 0.054
[1,   674] loss: 0.047
[1,   675] loss: 0.052
[1,   676] loss: 0.057
[1,   677] loss: 0.058
[1,   678] loss: 0.057
[1,   679] loss: 0.051
[1,   680] loss: 0.054
[1,   681] loss: 0.053
[1,   682] loss: 0.052
[1,   683] loss: 0.052
[1,   684] loss: 0.054
[1,   685] loss: 0.052
[1,   686] loss: 0.051
[1,   687] loss: 0.058
[1,   688] loss: 0.054
[1,   689] loss: 0.059
[1,   690] loss: 0.056
[1,   691] loss: 0.051
[1,   692] loss: 0.053
[1,   693] loss: 0.054
[1,   694] loss: 0.057
[1,   695] loss: 0.063
[1,   696] loss: 0.053
[1,   697] loss: 0.061
[1,   698] loss: 0.064
[1,   699] loss: 0.052
[1,   700] loss: 0.063
Model saved after epoch 0 and step 700
[1,   701] loss: 0.058
[1,   702] loss: 0.053
[1,   703] loss: 0.052
[1,   704] loss: 0.057
[1,   705] loss: 0.057
[1,   706] loss: 0.056
[1,   707] loss: 0.060
[1,   708] loss: 0.053
[1,   709] loss: 0.051
[1,   710] loss: 0.056
[1,   711] loss: 0.056
[1,   712] loss: 0.057
[1,   713] loss: 0.056
[1,   714] loss: 0.065
[1,   715] loss: 0.056
[1,   716] loss: 0.057
[1,   717] loss: 0.049
[1,   718] loss: 0.061
[1,   719] loss: 0.051
[1,   720] loss: 0.054
[1,   721] loss: 0.054
[1,   722] loss: 0.052
[1,   723] loss: 0.045
[1,   724] loss: 0.052
[1,   725] loss: 0.052
[1,   726] loss: 0.056
[1,   727] loss: 0.060
[1,   728] loss: 0.053
[1,   729] loss: 0.055
[1,   730] loss: 0.056
[1,   731] loss: 0.056
[1,   732] loss: 0.050
[1,   733] loss: 0.053
[1,   734] loss: 0.049
[1,   735] loss: 0.057
[1,   736] loss: 0.052
[1,   737] loss: 0.048
[1,   738] loss: 0.058
[1,   739] loss: 0.059
[1,   740] loss: 0.047
[1,   741] loss: 0.057
[1,   742] loss: 0.051
[1,   743] loss: 0.055
[1,   744] loss: 0.056
[1,   745] loss: 0.046
[1,   746] loss: 0.049
[1,   747] loss: 0.059
[1,   748] loss: 0.059
[1,   749] loss: 0.055
[1,   750] loss: 0.054
[1,   751] loss: 0.059
[1,   752] loss: 0.053
[1,   753] loss: 0.050
[1,   754] loss: 0.048
[1,   755] loss: 0.054
[1,   756] loss: 0.049
[1,   757] loss: 0.055
[1,   758] loss: 0.054
[1,   759] loss: 0.053
[1,   760] loss: 0.051
[1,   761] loss: 0.052
[1,   762] loss: 0.046
[1,   763] loss: 0.051
[1,   764] loss: 0.050
[1,   765] loss: 0.051
[1,   766] loss: 0.057
[1,   767] loss: 0.053
[1,   768] loss: 0.049
[1,   769] loss: 0.057
[1,   770] loss: 0.054
[1,   771] loss: 0.051
[1,   772] loss: 0.056
[1,   773] loss: 0.061
[1,   774] loss: 0.048
[1,   775] loss: 0.056
[1,   776] loss: 0.054
[1,   777] loss: 0.052
[1,   778] loss: 0.050
[1,   779] loss: 0.052
[1,   780] loss: 0.054
[1,   781] loss: 0.050
[1,   782] loss: 0.054
[1,   783] loss: 0.054
[1,   784] loss: 0.054
[1,   785] loss: 0.052
[1,   786] loss: 0.047
[1,   787] loss: 0.049
[1,   788] loss: 0.046
[1,   789] loss: 0.057
[1,   790] loss: 0.050
[1,   791] loss: 0.051
[1,   792] loss: 0.052
[1,   793] loss: 0.052
[1,   794] loss: 0.047
[1,   795] loss: 0.050
[1,   796] loss: 0.048
[1,   797] loss: 0.053
[1,   798] loss: 0.049
[1,   799] loss: 0.045
[1,   800] loss: 0.050
Model saved after epoch 0 and step 800
[1,   801] loss: 0.051
[1,   802] loss: 0.045
[1,   803] loss: 0.048
[1,   804] loss: 0.053
[1,   805] loss: 0.052
[1,   806] loss: 0.051
[1,   807] loss: 0.046
[1,   808] loss: 0.047
[1,   809] loss: 0.047
[1,   810] loss: 0.051
[1,   811] loss: 0.050
[1,   812] loss: 0.054
[1,   813] loss: 0.052
[1,   814] loss: 0.051
[1,   815] loss: 0.052
[1,   816] loss: 0.055
[1,   817] loss: 0.048
[1,   818] loss: 0.049
[1,   819] loss: 0.051
[1,   820] loss: 0.053
[1,   821] loss: 0.052
[1,   822] loss: 0.056
[1,   823] loss: 0.049
[1,   824] loss: 0.052
[1,   825] loss: 0.046
[1,   826] loss: 0.051
[1,   827] loss: 0.047
[1,   828] loss: 0.048
[1,   829] loss: 0.050
[1,   830] loss: 0.052
[1,   831] loss: 0.053
[1,   832] loss: 0.050
[1,   833] loss: 0.051
[1,   834] loss: 0.052
[1,   835] loss: 0.053
[1,   836] loss: 0.049
[1,   837] loss: 0.052
[1,   838] loss: 0.050
[1,   839] loss: 0.047
[1,   840] loss: 0.047
[1,   841] loss: 0.048
[1,   842] loss: 0.045
[1,   843] loss: 0.050
[1,   844] loss: 0.052
[1,   845] loss: 0.055
[1,   846] loss: 0.049
[1,   847] loss: 0.053
[1,   848] loss: 0.051
[1,   849] loss: 0.050
[1,   850] loss: 0.048
[1,   851] loss: 0.058
[1,   852] loss: 0.052
[1,   853] loss: 0.046
[1,   854] loss: 0.046
[1,   855] loss: 0.048
[1,   856] loss: 0.045
[1,   857] loss: 0.044
[1,   858] loss: 0.052
[1,   859] loss: 0.047
[1,   860] loss: 0.055
[1,   861] loss: 0.044
[1,   862] loss: 0.054
[1,   863] loss: 0.055
[1,   864] loss: 0.051
[1,   865] loss: 0.048
[1,   866] loss: 0.056
[1,   867] loss: 0.049
[1,   868] loss: 0.054
[1,   869] loss: 0.045
[1,   870] loss: 0.057
[1,   871] loss: 0.049
[1,   872] loss: 0.049
[1,   873] loss: 0.043
[1,   874] loss: 0.050
[1,   875] loss: 0.046
[1,   876] loss: 0.054
[1,   877] loss: 0.054
[1,   878] loss: 0.050
[1,   879] loss: 0.048
[1,   880] loss: 0.045
[1,   881] loss: 0.050
[1,   882] loss: 0.049
[1,   883] loss: 0.051
[1,   884] loss: 0.049
[1,   885] loss: 0.054
[1,   886] loss: 0.054
[1,   887] loss: 0.046
[1,   888] loss: 0.051
[1,   889] loss: 0.050
[1,   890] loss: 0.055
[1,   891] loss: 0.053
[1,   892] loss: 0.050
[1,   893] loss: 0.053
[1,   894] loss: 0.046
[1,   895] loss: 0.054
[1,   896] loss: 0.047
[1,   897] loss: 0.053
[1,   898] loss: 0.048
[1,   899] loss: 0.053
[1,   900] loss: 0.048
Model saved after epoch 0 and step 900
[1,   901] loss: 0.042
[1,   902] loss: 0.048
[1,   903] loss: 0.048
[1,   904] loss: 0.053
[1,   905] loss: 0.049
[1,   906] loss: 0.055
[1,   907] loss: 0.052
[1,   908] loss: 0.051
[1,   909] loss: 0.049
[1,   910] loss: 0.046
[1,   911] loss: 0.047
[1,   912] loss: 0.052
[1,   913] loss: 0.048
[1,   914] loss: 0.046
[1,   915] loss: 0.056
[1,   916] loss: 0.049
[1,   917] loss: 0.051
[1,   918] loss: 0.047
[1,   919] loss: 0.050
[1,   920] loss: 0.052
[1,   921] loss: 0.060
[1,   922] loss: 0.058
[1,   923] loss: 0.053
[1,   924] loss: 0.051
[1,   925] loss: 0.052
[1,   926] loss: 0.049
[1,   927] loss: 0.047
[1,   928] loss: 0.047
[1,   929] loss: 0.050
[1,   930] loss: 0.055
[1,   931] loss: 0.053
[1,   932] loss: 0.049
[1,   933] loss: 0.051
[1,   934] loss: 0.049
[1,   935] loss: 0.049
[1,   936] loss: 0.052
[1,   937] loss: 0.050
[1,   938] loss: 0.055
[1,   939] loss: 0.051
[1,   940] loss: 0.046
[1,   941] loss: 0.051
[1,   942] loss: 0.055
[1,   943] loss: 0.050
[1,   944] loss: 0.051
[1,   945] loss: 0.050
[1,   946] loss: 0.052
[1,   947] loss: 0.048
[1,   948] loss: 0.052
[1,   949] loss: 0.049
[1,   950] loss: 0.050
[1,   951] loss: 0.046
[1,   952] loss: 0.053
[1,   953] loss: 0.051
[1,   954] loss: 0.048
[1,   955] loss: 0.048
[1,   956] loss: 0.046
[1,   957] loss: 0.050
[1,   958] loss: 0.048
[1,   959] loss: 0.045
[1,   960] loss: 0.051
[1,   961] loss: 0.045
[1,   962] loss: 0.045
[1,   963] loss: 0.047
[1,   964] loss: 0.048
[1,   965] loss: 0.051
[1,   966] loss: 0.047
[1,   967] loss: 0.048
[1,   968] loss: 0.057
[1,   969] loss: 0.042
[1,   970] loss: 0.051
[1,   971] loss: 0.053
[1,   972] loss: 0.046
[1,   973] loss: 0.050
[1,   974] loss: 0.043
[1,   975] loss: 0.051
[1,   976] loss: 0.051
[1,   977] loss: 0.049
[1,   978] loss: 0.050
[1,   979] loss: 0.050
[1,   980] loss: 0.047
[1,   981] loss: 0.045
[1,   982] loss: 0.060
[1,   983] loss: 0.048
[1,   984] loss: 0.050
[1,   985] loss: 0.045
[1,   986] loss: 0.049
[1,   987] loss: 0.052
[1,   988] loss: 0.046
[1,   989] loss: 0.051
[1,   990] loss: 0.049
[1,   991] loss: 0.050
[1,   992] loss: 0.049
[1,   993] loss: 0.050
[1,   994] loss: 0.054
[1,   995] loss: 0.049
[1,   996] loss: 0.049
[1,   997] loss: 0.051
[1,   998] loss: 0.047
[1,   999] loss: 0.048
[1,  1000] loss: 0.050
Model saved after epoch 0 and step 1000
[1,  1001] loss: 0.050
[1,  1002] loss: 0.046
[1,  1003] loss: 0.050
[1,  1004] loss: 0.052
[1,  1005] loss: 0.047
[1,  1006] loss: 0.048
[1,  1007] loss: 0.054
[1,  1008] loss: 0.053
[1,  1009] loss: 0.051
[1,  1010] loss: 0.046
[1,  1011] loss: 0.046
[1,  1012] loss: 0.053
[1,  1013] loss: 0.050
[1,  1014] loss: 0.060
[1,  1015] loss: 0.046
[1,  1016] loss: 0.051
[1,  1017] loss: 0.050
[1,  1018] loss: 0.050
[1,  1019] loss: 0.050
[1,  1020] loss: 0.049
[1,  1021] loss: 0.057
[1,  1022] loss: 0.053
[1,  1023] loss: 0.048
[1,  1024] loss: 0.051
[1,  1025] loss: 0.049
[1,  1026] loss: 0.058
[1,  1027] loss: 0.050
[1,  1028] loss: 0.054
[1,  1029] loss: 0.053
[1,  1030] loss: 0.051
[1,  1031] loss: 0.050
[1,  1032] loss: 0.045
[1,  1033] loss: 0.047
[1,  1034] loss: 0.052
[1,  1035] loss: 0.050
[1,  1036] loss: 0.050
[1,  1037] loss: 0.046
[1,  1038] loss: 0.054
[1,  1039] loss: 0.050
[1,  1040] loss: 0.055
[1,  1041] loss: 0.045
[1,  1042] loss: 0.049
[1,  1043] loss: 0.050
[1,  1044] loss: 0.051
[1,  1045] loss: 0.051
[1,  1046] loss: 0.050
[1,  1047] loss: 0.050
[1,  1048] loss: 0.050
[1,  1049] loss: 0.047
[1,  1050] loss: 0.047
[1,  1051] loss: 0.051
[1,  1052] loss: 0.048
[1,  1053] loss: 0.048
[1,  1054] loss: 0.046
[1,  1055] loss: 0.047
[1,  1056] loss: 0.049
[1,  1057] loss: 0.043
[1,  1058] loss: 0.044
[1,  1059] loss: 0.052
[1,  1060] loss: 0.045
[1,  1061] loss: 0.050
[1,  1062] loss: 0.051
[1,  1063] loss: 0.047
[1,  1064] loss: 0.046
[1,  1065] loss: 0.047
[1,  1066] loss: 0.044
[1,  1067] loss: 0.048
[1,  1068] loss: 0.054
[1,  1069] loss: 0.048
[1,  1070] loss: 0.055
[1,  1071] loss: 0.053
[1,  1072] loss: 0.052
[1,  1073] loss: 0.054
[1,  1074] loss: 0.046
[1,  1075] loss: 0.050
[1,  1076] loss: 0.048
[1,  1077] loss: 0.057
[1,  1078] loss: 0.051
[1,  1079] loss: 0.051
[1,  1080] loss: 0.051
[1,  1081] loss: 0.049
[1,  1082] loss: 0.053
[1,  1083] loss: 0.052
[1,  1084] loss: 0.048
[1,  1085] loss: 0.052
[1,  1086] loss: 0.050
[1,  1087] loss: 0.053
[1,  1088] loss: 0.046
[1,  1089] loss: 0.050
[1,  1090] loss: 0.047
[1,  1091] loss: 0.051
[1,  1092] loss: 0.048
[1,  1093] loss: 0.052
[1,  1094] loss: 0.049
[1,  1095] loss: 0.048
[1,  1096] loss: 0.048
[1,  1097] loss: 0.047
[1,  1098] loss: 0.046
[1,  1099] loss: 0.050
[1,  1100] loss: 0.047
Model saved after epoch 0 and step 1100
[1,  1101] loss: 0.052
[1,  1102] loss: 0.053
[1,  1103] loss: 0.042
[1,  1104] loss: 0.052
[1,  1105] loss: 0.049
[1,  1106] loss: 0.054
[1,  1107] loss: 0.046
[1,  1108] loss: 0.052
[1,  1109] loss: 0.052
[1,  1110] loss: 0.043
[1,  1111] loss: 0.041
[1,  1112] loss: 0.048
[1,  1113] loss: 0.046
[1,  1114] loss: 0.055
[1,  1115] loss: 0.049
[1,  1116] loss: 0.049
[1,  1117] loss: 0.049
[1,  1118] loss: 0.049
[1,  1119] loss: 0.045
[1,  1120] loss: 0.053
[1,  1121] loss: 0.046
[1,  1122] loss: 0.046
[1,  1123] loss: 0.048
[1,  1124] loss: 0.050
[1,  1125] loss: 0.046
[1,  1126] loss: 0.052
[1,  1127] loss: 0.052
[1,  1128] loss: 0.056
[1,  1129] loss: 0.057
[1,  1130] loss: 0.048
[1,  1131] loss: 0.050
[1,  1132] loss: 0.048
[1,  1133] loss: 0.053
[1,  1134] loss: 0.047
[1,  1135] loss: 0.044
[1,  1136] loss: 0.053
[1,  1137] loss: 0.050
[1,  1138] loss: 0.055
[1,  1139] loss: 0.053
[1,  1140] loss: 0.051
[1,  1141] loss: 0.046
[1,  1142] loss: 0.051
[1,  1143] loss: 0.052
[1,  1144] loss: 0.051
[1,  1145] loss: 0.049
[1,  1146] loss: 0.050
[1,  1147] loss: 0.052
[1,  1148] loss: 0.048
[1,  1149] loss: 0.049
[1,  1150] loss: 0.051
[1,  1151] loss: 0.051
[1,  1152] loss: 0.049
[1,  1153] loss: 0.051
[1,  1154] loss: 0.050
[1,  1155] loss: 0.045
[1,  1156] loss: 0.052
[1,  1157] loss: 0.048
[1,  1158] loss: 0.050
[1,  1159] loss: 0.050
[1,  1160] loss: 0.046
[1,  1161] loss: 0.049
[1,  1162] loss: 0.047
[1,  1163] loss: 0.050
[1,  1164] loss: 0.051
[1,  1165] loss: 0.051
[1,  1166] loss: 0.041
[1,  1167] loss: 0.051
[1,  1168] loss: 0.048
[1,  1169] loss: 0.051
[1,  1170] loss: 0.049
[1,  1171] loss: 0.047
[1,  1172] loss: 0.049
[1,  1173] loss: 0.047
[1,  1174] loss: 0.049
[1,  1175] loss: 0.044
[1,  1176] loss: 0.050
[1,  1177] loss: 0.044
[1,  1178] loss: 0.044
[1,  1179] loss: 0.052
[1,  1180] loss: 0.043
[1,  1181] loss: 0.044
[1,  1182] loss: 0.047
[1,  1183] loss: 0.053
[1,  1184] loss: 0.048
[1,  1185] loss: 0.046
[1,  1186] loss: 0.050
[1,  1187] loss: 0.045
[1,  1188] loss: 0.048
[1,  1189] loss: 0.050
[1,  1190] loss: 0.048
[1,  1191] loss: 0.051
[1,  1192] loss: 0.047
[1,  1193] loss: 0.048
[1,  1194] loss: 0.054
[1,  1195] loss: 0.057
[1,  1196] loss: 0.049
[1,  1197] loss: 0.046
[1,  1198] loss: 0.048
[1,  1199] loss: 0.048
[1,  1200] loss: 0.050
Model saved after epoch 0 and step 1200
[1,  1201] loss: 0.049
[1,  1202] loss: 0.057
[1,  1203] loss: 0.049
[1,  1204] loss: 0.049
[1,  1205] loss: 0.054
[1,  1206] loss: 0.049
[1,  1207] loss: 0.056
[1,  1208] loss: 0.043
[1,  1209] loss: 0.047
[1,  1210] loss: 0.047
[1,  1211] loss: 0.054
[1,  1212] loss: 0.051
[1,  1213] loss: 0.052
[1,  1214] loss: 0.045
[1,  1215] loss: 0.055
[1,  1216] loss: 0.046
[1,  1217] loss: 0.051
[1,  1218] loss: 0.050
[1,  1219] loss: 0.049
[1,  1220] loss: 0.049
[1,  1221] loss: 0.048
[1,  1222] loss: 0.051
[1,  1223] loss: 0.049
[1,  1224] loss: 0.050
[1,  1225] loss: 0.050
[1,  1226] loss: 0.048
[1,  1227] loss: 0.055
[1,  1228] loss: 0.047
[1,  1229] loss: 0.046
[1,  1230] loss: 0.049
[1,  1231] loss: 0.049
[1,  1232] loss: 0.047
[1,  1233] loss: 0.053
[1,  1234] loss: 0.049
[1,  1235] loss: 0.052
[1,  1236] loss: 0.055
[1,  1237] loss: 0.051
[1,  1238] loss: 0.050
[1,  1239] loss: 0.047
[1,  1240] loss: 0.052
[1,  1241] loss: 0.049
[1,  1242] loss: 0.050
[1,  1243] loss: 0.051
[1,  1244] loss: 0.043
[1,  1245] loss: 0.049
[1,  1246] loss: 0.046
[1,  1247] loss: 0.047
[1,  1248] loss: 0.050
[1,  1249] loss: 0.053
[1,  1250] loss: 0.051
[1,  1251] loss: 0.054
[1,  1252] loss: 0.051
[1,  1253] loss: 0.045
[1,  1254] loss: 0.044
[1,  1255] loss: 0.045
[1,  1256] loss: 0.050
[1,  1257] loss: 0.052
[1,  1258] loss: 0.051
[1,  1259] loss: 0.049
[1,  1260] loss: 0.046
[1,  1261] loss: 0.048
[1,  1262] loss: 0.047
[1,  1263] loss: 0.046
[1,  1264] loss: 0.044
[1,  1265] loss: 0.048
[1,  1266] loss: 0.052
[1,  1267] loss: 0.050
[1,  1268] loss: 0.043
[1,  1269] loss: 0.050
[1,  1270] loss: 0.049
[1,  1271] loss: 0.046
[1,  1272] loss: 0.050
[1,  1273] loss: 0.046
[1,  1274] loss: 0.057
[1,  1275] loss: 0.046
[1,  1276] loss: 0.045
[1,  1277] loss: 0.046
[1,  1278] loss: 0.046
[1,  1279] loss: 0.050
[1,  1280] loss: 0.051
[1,  1281] loss: 0.048
[1,  1282] loss: 0.044
[1,  1283] loss: 0.050
[1,  1284] loss: 0.050
[1,  1285] loss: 0.049
[1,  1286] loss: 0.053
[1,  1287] loss: 0.054
[1,  1288] loss: 0.047
[1,  1289] loss: 0.050
[1,  1290] loss: 0.051
[1,  1291] loss: 0.048
[1,  1292] loss: 0.052
[1,  1293] loss: 0.053
[1,  1294] loss: 0.049
[1,  1295] loss: 0.048
[1,  1296] loss: 0.047
[1,  1297] loss: 0.046
[1,  1298] loss: 0.057
[1,  1299] loss: 0.047
[1,  1300] loss: 0.047
Model saved after epoch 0 and step 1300
[1,  1301] loss: 0.049
[1,  1302] loss: 0.048
[1,  1303] loss: 0.049
[1,  1304] loss: 0.047
[1,  1305] loss: 0.048
[1,  1306] loss: 0.050
[1,  1307] loss: 0.046
[1,  1308] loss: 0.061
[1,  1309] loss: 0.045
[1,  1310] loss: 0.050
[1,  1311] loss: 0.053
[1,  1312] loss: 0.049
[1,  1313] loss: 0.049
[1,  1314] loss: 0.052
[1,  1315] loss: 0.053
[1,  1316] loss: 0.049
[1,  1317] loss: 0.051
[1,  1318] loss: 0.046
[1,  1319] loss: 0.047
[1,  1320] loss: 0.053
[1,  1321] loss: 0.051
[1,  1322] loss: 0.049
[1,  1323] loss: 0.048
[1,  1324] loss: 0.045
[1,  1325] loss: 0.055
[1,  1326] loss: 0.053
[1,  1327] loss: 0.049
[1,  1328] loss: 0.051
[1,  1329] loss: 0.050
[1,  1330] loss: 0.048
[1,  1331] loss: 0.050
[1,  1332] loss: 0.047
[1,  1333] loss: 0.050
[1,  1334] loss: 0.049
[1,  1335] loss: 0.048
[1,  1336] loss: 0.047
[1,  1337] loss: 0.046
[1,  1338] loss: 0.046
[1,  1339] loss: 0.043
[1,  1340] loss: 0.048
[1,  1341] loss: 0.049
[1,  1342] loss: 0.053
[1,  1343] loss: 0.048
[1,  1344] loss: 0.052
[1,  1345] loss: 0.045
[1,  1346] loss: 0.048
[1,  1347] loss: 0.051
[1,  1348] loss: 0.046
[1,  1349] loss: 0.053
[1,  1350] loss: 0.047
[1,  1351] loss: 0.045
[1,  1352] loss: 0.051
[1,  1353] loss: 0.046
[1,  1354] loss: 0.048
[1,  1355] loss: 0.048
[1,  1356] loss: 0.047
[1,  1357] loss: 0.049
[1,  1358] loss: 0.051
[1,  1359] loss: 0.049
[1,  1360] loss: 0.047
[1,  1361] loss: 0.053
[1,  1362] loss: 0.045
[1,  1363] loss: 0.055
[1,  1364] loss: 0.048
[1,  1365] loss: 0.056
[1,  1366] loss: 0.045
[1,  1367] loss: 0.047
[1,  1368] loss: 0.049
[1,  1369] loss: 0.051
[1,  1370] loss: 0.050
[1,  1371] loss: 0.049
[1,  1372] loss: 0.044
[1,  1373] loss: 0.052
[1,  1374] loss: 0.054
[1,  1375] loss: 0.053
[1,  1376] loss: 0.055
[1,  1377] loss: 0.055
[1,  1378] loss: 0.054
[1,  1379] loss: 0.049
[1,  1380] loss: 0.048
[1,  1381] loss: 0.051
[1,  1382] loss: 0.054
[1,  1383] loss: 0.053
[1,  1384] loss: 0.055
[1,  1385] loss: 0.048
[1,  1386] loss: 0.050
[1,  1387] loss: 0.045
[1,  1388] loss: 0.049
[1,  1389] loss: 0.050
[1,  1390] loss: 0.050
[1,  1391] loss: 0.045
[1,  1392] loss: 0.048
[1,  1393] loss: 0.046
[1,  1394] loss: 0.055
[1,  1395] loss: 0.047
[1,  1396] loss: 0.043
[1,  1397] loss: 0.045
[1,  1398] loss: 0.052
[1,  1399] loss: 0.044
[1,  1400] loss: 0.048
Model saved after epoch 0 and step 1400
[1,  1401] loss: 0.051
[1,  1402] loss: 0.048
[1,  1403] loss: 0.048
[1,  1404] loss: 0.046
[1,  1405] loss: 0.047
[1,  1406] loss: 0.046
[1,  1407] loss: 0.048
[1,  1408] loss: 0.048
[1,  1409] loss: 0.048
[1,  1410] loss: 0.048
[1,  1411] loss: 0.051
[1,  1412] loss: 0.049
[1,  1413] loss: 0.052
[1,  1414] loss: 0.052
[1,  1415] loss: 0.044
[1,  1416] loss: 0.046
[1,  1417] loss: 0.052
[1,  1418] loss: 0.047
[1,  1419] loss: 0.045
[1,  1420] loss: 0.048
[1,  1421] loss: 0.050
[1,  1422] loss: 0.050
[1,  1423] loss: 0.049
[1,  1424] loss: 0.053
[1,  1425] loss: 0.047
[1,  1426] loss: 0.047
[1,  1427] loss: 0.049
[1,  1428] loss: 0.042
[1,  1429] loss: 0.053
[1,  1430] loss: 0.050
[1,  1431] loss: 0.050
[1,  1432] loss: 0.049
[1,  1433] loss: 0.050
[1,  1434] loss: 0.048
[1,  1435] loss: 0.049
[1,  1436] loss: 0.047
[1,  1437] loss: 0.043
[1,  1438] loss: 0.049
[1,  1439] loss: 0.052
[1,  1440] loss: 0.055
[1,  1441] loss: 0.054
[1,  1442] loss: 0.046
[1,  1443] loss: 0.050
[1,  1444] loss: 0.049
[1,  1445] loss: 0.043
[1,  1446] loss: 0.048
[1,  1447] loss: 0.058
[1,  1448] loss: 0.045
[1,  1449] loss: 0.049
[1,  1450] loss: 0.046
[1,  1451] loss: 0.050
[1,  1452] loss: 0.049
[1,  1453] loss: 0.054
[1,  1454] loss: 0.050
[1,  1455] loss: 0.051
[1,  1456] loss: 0.043
[1,  1457] loss: 0.048
[1,  1458] loss: 0.050
[1,  1459] loss: 0.048
[1,  1460] loss: 0.049
[1,  1461] loss: 0.050
[1,  1462] loss: 0.052
[1,  1463] loss: 0.052
[1,  1464] loss: 0.049
[1,  1465] loss: 0.054
[1,  1466] loss: 0.043
[1,  1467] loss: 0.045
[1,  1468] loss: 0.045
[1,  1469] loss: 0.045
[1,  1470] loss: 0.045
[1,  1471] loss: 0.048
[1,  1472] loss: 0.049
[1,  1473] loss: 0.050
[1,  1474] loss: 0.046
[1,  1475] loss: 0.045
[1,  1476] loss: 0.050
[1,  1477] loss: 0.052
[1,  1478] loss: 0.047
[1,  1479] loss: 0.051
[1,  1480] loss: 0.050
[1,  1481] loss: 0.054
[1,  1482] loss: 0.050
[1,  1483] loss: 0.047
[1,  1484] loss: 0.048
[1,  1485] loss: 0.044
[1,  1486] loss: 0.048
[1,  1487] loss: 0.048
[1,  1488] loss: 0.048
[1,  1489] loss: 0.047
[1,  1490] loss: 0.047
[1,  1491] loss: 0.048
[1,  1492] loss: 0.048
[1,  1493] loss: 0.043
[1,  1494] loss: 0.048
[1,  1495] loss: 0.049
[1,  1496] loss: 0.048
[1,  1497] loss: 0.048
[1,  1498] loss: 0.044
[1,  1499] loss: 0.050
[1,  1500] loss: 0.049
Model saved after epoch 0 and step 1500
[1,  1501] loss: 0.052
[1,  1502] loss: 0.051
[1,  1503] loss: 0.051
[1,  1504] loss: 0.046
[1,  1505] loss: 0.047
[1,  1506] loss: 0.044
[1,  1507] loss: 0.050
[1,  1508] loss: 0.044
[1,  1509] loss: 0.048
[1,  1510] loss: 0.048
[1,  1511] loss: 0.050
[1,  1512] loss: 0.054
[1,  1513] loss: 0.046
[1,  1514] loss: 0.049
[1,  1515] loss: 0.054
[1,  1516] loss: 0.052
[1,  1517] loss: 0.049
[1,  1518] loss: 0.045
[1,  1519] loss: 0.049
[1,  1520] loss: 0.049
[1,  1521] loss: 0.049
[1,  1522] loss: 0.045
[1,  1523] loss: 0.049
[1,  1524] loss: 0.050
[1,  1525] loss: 0.051
[1,  1526] loss: 0.048
[1,  1527] loss: 0.046
[1,  1528] loss: 0.050
[1,  1529] loss: 0.048
[1,  1530] loss: 0.045
[1,  1531] loss: 0.049
[1,  1532] loss: 0.044
[1,  1533] loss: 0.047
[1,  1534] loss: 0.050
[1,  1535] loss: 0.047
[1,  1536] loss: 0.049
[1,  1537] loss: 0.049
[1,  1538] loss: 0.040
[1,  1539] loss: 0.053
[1,  1540] loss: 0.052
[1,  1541] loss: 0.051
[1,  1542] loss: 0.051
[1,  1543] loss: 0.045
[1,  1544] loss: 0.051
[1,  1545] loss: 0.055
[1,  1546] loss: 0.049
[1,  1547] loss: 0.051
[1,  1548] loss: 0.051
[1,  1549] loss: 0.047
[1,  1550] loss: 0.046
[1,  1551] loss: 0.048
[1,  1552] loss: 0.058
[1,  1553] loss: 0.049
[1,  1554] loss: 0.051
[1,  1555] loss: 0.049
[1,  1556] loss: 0.043
[1,  1557] loss: 0.052
[1,  1558] loss: 0.055
[1,  1559] loss: 0.048
[1,  1560] loss: 0.052
[1,  1561] loss: 0.050
[1,  1562] loss: 0.047
[1,  1563] loss: 0.043
[1,  1564] loss: 0.048
[1,  1565] loss: 0.058
[1,  1566] loss: 0.050
[1,  1567] loss: 0.052
[1,  1568] loss: 0.048
[1,  1569] loss: 0.045
[1,  1570] loss: 0.044
[1,  1571] loss: 0.048
[1,  1572] loss: 0.045
[1,  1573] loss: 0.048
[1,  1574] loss: 0.050
[1,  1575] loss: 0.053
[1,  1576] loss: 0.048
[1,  1577] loss: 0.043
[1,  1578] loss: 0.050
[1,  1579] loss: 0.050
[1,  1580] loss: 0.051
[1,  1581] loss: 0.047
[1,  1582] loss: 0.055
[1,  1583] loss: 0.046
[1,  1584] loss: 0.052
[1,  1585] loss: 0.057
[1,  1586] loss: 0.045
[1,  1587] loss: 0.051
[1,  1588] loss: 0.045
[1,  1589] loss: 0.051
[1,  1590] loss: 0.053
[1,  1591] loss: 0.046
[1,  1592] loss: 0.052
[1,  1593] loss: 0.043
[1,  1594] loss: 0.048
[1,  1595] loss: 0.050
[1,  1596] loss: 0.044
[1,  1597] loss: 0.053
[1,  1598] loss: 0.047
[1,  1599] loss: 0.047
[1,  1600] loss: 0.050
Model saved after epoch 0 and step 1600
[1,  1601] loss: 0.047
[1,  1602] loss: 0.048
[1,  1603] loss: 0.047
[1,  1604] loss: 0.049
[1,  1605] loss: 0.046
[1,  1606] loss: 0.054
[1,  1607] loss: 0.047
[1,  1608] loss: 0.050
[1,  1609] loss: 0.045
[1,  1610] loss: 0.041
[1,  1611] loss: 0.054
[1,  1612] loss: 0.052
[1,  1613] loss: 0.056
[1,  1614] loss: 0.049
[1,  1615] loss: 0.047
[1,  1616] loss: 0.046
[1,  1617] loss: 0.051
[1,  1618] loss: 0.048
[1,  1619] loss: 0.051
[1,  1620] loss: 0.048
[1,  1621] loss: 0.045
[1,  1622] loss: 0.044
[1,  1623] loss: 0.048
[1,  1624] loss: 0.049
[1,  1625] loss: 0.050
[1,  1626] loss: 0.046
[1,  1627] loss: 0.044
[1,  1628] loss: 0.049
[1,  1629] loss: 0.050
[1,  1630] loss: 0.056
[1,  1631] loss: 0.044
[1,  1632] loss: 0.045
[1,  1633] loss: 0.047
[1,  1634] loss: 0.043
[1,  1635] loss: 0.047
[1,  1636] loss: 0.050
[1,  1637] loss: 0.051
[1,  1638] loss: 0.049
[1,  1639] loss: 0.048
[1,  1640] loss: 0.054
[1,  1641] loss: 0.048
[1,  1642] loss: 0.054
[1,  1643] loss: 0.047
[1,  1644] loss: 0.048
[1,  1645] loss: 0.047
[1,  1646] loss: 0.046
[1,  1647] loss: 0.049
[1,  1648] loss: 0.045
[1,  1649] loss: 0.050
[1,  1650] loss: 0.055
[1,  1651] loss: 0.046
[1,  1652] loss: 0.049
[1,  1653] loss: 0.048
[1,  1654] loss: 0.051
[1,  1655] loss: 0.050
[1,  1656] loss: 0.052
[1,  1657] loss: 0.046
[1,  1658] loss: 0.042
[1,  1659] loss: 0.048
[1,  1660] loss: 0.047
[1,  1661] loss: 0.050
[1,  1662] loss: 0.044
[1,  1663] loss: 0.051
[1,  1664] loss: 0.052
[1,  1665] loss: 0.048
[1,  1666] loss: 0.051
[1,  1667] loss: 0.047
[1,  1668] loss: 0.048
[1,  1669] loss: 0.047
[1,  1670] loss: 0.045
[1,  1671] loss: 0.053
[1,  1672] loss: 0.054
[1,  1673] loss: 0.044
[1,  1674] loss: 0.043
[1,  1675] loss: 0.047
[1,  1676] loss: 0.047
[1,  1677] loss: 0.043
[1,  1678] loss: 0.049
[1,  1679] loss: 0.052
[1,  1680] loss: 0.047
[1,  1681] loss: 0.050
[1,  1682] loss: 0.047
[1,  1683] loss: 0.049
[1,  1684] loss: 0.053
[1,  1685] loss: 0.046
[1,  1686] loss: 0.056
[1,  1687] loss: 0.055
[1,  1688] loss: 0.048
[1,  1689] loss: 0.046
[1,  1690] loss: 0.046
[1,  1691] loss: 0.043
[1,  1692] loss: 0.048
[1,  1693] loss: 0.050
[1,  1694] loss: 0.048
[1,  1695] loss: 0.049
[1,  1696] loss: 0.057
[1,  1697] loss: 0.050
[1,  1698] loss: 0.046
[1,  1699] loss: 0.050
[1,  1700] loss: 0.050
Model saved after epoch 0 and step 1700
[1,  1701] loss: 0.047
[1,  1702] loss: 0.048
[1,  1703] loss: 0.047
[1,  1704] loss: 0.048
[1,  1705] loss: 0.047
[1,  1706] loss: 0.048
[1,  1707] loss: 0.052
[1,  1708] loss: 0.048
[1,  1709] loss: 0.049
[1,  1710] loss: 0.044
[1,  1711] loss: 0.053
[1,  1712] loss: 0.044
[1,  1713] loss: 0.054
[1,  1714] loss: 0.052
[1,  1715] loss: 0.046
[1,  1716] loss: 0.051
[1,  1717] loss: 0.050
[1,  1718] loss: 0.051
[1,  1719] loss: 0.049
[1,  1720] loss: 0.052
[1,  1721] loss: 0.051
[1,  1722] loss: 0.057
[1,  1723] loss: 0.049
[1,  1724] loss: 0.049
[1,  1725] loss: 0.051
[1,  1726] loss: 0.045
[1,  1727] loss: 0.047
[1,  1728] loss: 0.052
[1,  1729] loss: 0.050
[1,  1730] loss: 0.046
[1,  1731] loss: 0.052
[1,  1732] loss: 0.049
[1,  1733] loss: 0.048
[1,  1734] loss: 0.053
[1,  1735] loss: 0.051
[1,  1736] loss: 0.052
[1,  1737] loss: 0.048
[1,  1738] loss: 0.052
[1,  1739] loss: 0.047
[1,  1740] loss: 0.047
[1,  1741] loss: 0.048
[1,  1742] loss: 0.054
[1,  1743] loss: 0.048
[1,  1744] loss: 0.047
[1,  1745] loss: 0.048
[1,  1746] loss: 0.051
[1,  1747] loss: 0.050
[1,  1748] loss: 0.051
[1,  1749] loss: 0.048
[1,  1750] loss: 0.051
[1,  1751] loss: 0.052
[1,  1752] loss: 0.052
[1,  1753] loss: 0.052
[1,  1754] loss: 0.052
[1,  1755] loss: 0.053
[1,  1756] loss: 0.048
[1,  1757] loss: 0.053
[1,  1758] loss: 0.045
[1,  1759] loss: 0.049
[1,  1760] loss: 0.049
[1,  1761] loss: 0.047
[1,  1762] loss: 0.054
[1,  1763] loss: 0.044
[1,  1764] loss: 0.052
[1,  1765] loss: 0.047
[1,  1766] loss: 0.045
[1,  1767] loss: 0.048
[1,  1768] loss: 0.048
[1,  1769] loss: 0.052
[1,  1770] loss: 0.047
[1,  1771] loss: 0.050
[1,  1772] loss: 0.051
[1,  1773] loss: 0.048
[1,  1774] loss: 0.053
[1,  1775] loss: 0.045
[1,  1776] loss: 0.047
[1,  1777] loss: 0.049
[1,  1778] loss: 0.053
[1,  1779] loss: 0.048
[1,  1780] loss: 0.048
[1,  1781] loss: 0.052
[1,  1782] loss: 0.048
[1,  1783] loss: 0.049
[1,  1784] loss: 0.044
[1,  1785] loss: 0.052
[1,  1786] loss: 0.052
[1,  1787] loss: 0.048
[1,  1788] loss: 0.049
[1,  1789] loss: 0.053
[1,  1790] loss: 0.046
[1,  1791] loss: 0.053
[1,  1792] loss: 0.048
[1,  1793] loss: 0.048
[1,  1794] loss: 0.046
[1,  1795] loss: 0.047
[1,  1796] loss: 0.049
[1,  1797] loss: 0.046
[1,  1798] loss: 0.052
[1,  1799] loss: 0.056
[1,  1800] loss: 0.048
Model saved after epoch 0 and step 1800
[1,  1801] loss: 0.053
[1,  1802] loss: 0.047
[1,  1803] loss: 0.048
[1,  1804] loss: 0.052
[1,  1805] loss: 0.044
[1,  1806] loss: 0.051
[1,  1807] loss: 0.055
[1,  1808] loss: 0.045
[1,  1809] loss: 0.051
[1,  1810] loss: 0.048
[1,  1811] loss: 0.050
[1,  1812] loss: 0.049
[1,  1813] loss: 0.046
[1,  1814] loss: 0.049
[1,  1815] loss: 0.044
[1,  1816] loss: 0.047
[1,  1817] loss: 0.048
[1,  1818] loss: 0.048
[1,  1819] loss: 0.046
[1,  1820] loss: 0.046
[1,  1821] loss: 0.048
[1,  1822] loss: 0.047
[1,  1823] loss: 0.047
[1,  1824] loss: 0.050
[1,  1825] loss: 0.048
[1,  1826] loss: 0.046
[1,  1827] loss: 0.044
[1,  1828] loss: 0.048
[1,  1829] loss: 0.050
[1,  1830] loss: 0.048
[1,  1831] loss: 0.052
[1,  1832] loss: 0.050
[1,  1833] loss: 0.053
[1,  1834] loss: 0.045
[1,  1835] loss: 0.051
[1,  1836] loss: 0.052
[1,  1837] loss: 0.051
[1,  1838] loss: 0.052
[1,  1839] loss: 0.042
[1,  1840] loss: 0.049
[1,  1841] loss: 0.048
[1,  1842] loss: 0.045
[1,  1843] loss: 0.048
[1,  1844] loss: 0.045
[1,  1845] loss: 0.047
[1,  1846] loss: 0.047
[1,  1847] loss: 0.050
[1,  1848] loss: 0.047
[1,  1849] loss: 0.047
[1,  1850] loss: 0.045
[1,  1851] loss: 0.050
[1,  1852] loss: 0.048
[1,  1853] loss: 0.043
[1,  1854] loss: 0.048
[1,  1855] loss: 0.047
[1,  1856] loss: 0.050
[1,  1857] loss: 0.045
[1,  1858] loss: 0.046
[1,  1859] loss: 0.048
[1,  1860] loss: 0.049
[1,  1861] loss: 0.051
[1,  1862] loss: 0.050
[1,  1863] loss: 0.051
[1,  1864] loss: 0.048
[1,  1865] loss: 0.050
[1,  1866] loss: 0.047
[1,  1867] loss: 0.052
[1,  1868] loss: 0.048
[1,  1869] loss: 0.045
[1,  1870] loss: 0.048
[1,  1871] loss: 0.053
[1,  1872] loss: 0.047
[1,  1873] loss: 0.048
[1,  1874] loss: 0.044
[1,  1875] loss: 0.047
[1,  1876] loss: 0.045
[1,  1877] loss: 0.055
[1,  1878] loss: 0.045
[1,  1879] loss: 0.049
[1,  1880] loss: 0.044
[1,  1881] loss: 0.051
[1,  1882] loss: 0.051
[1,  1883] loss: 0.049
[1,  1884] loss: 0.052
[1,  1885] loss: 0.049
[1,  1886] loss: 0.050
[1,  1887] loss: 0.050
[1,  1888] loss: 0.048
[1,  1889] loss: 0.046
[1,  1890] loss: 0.051
[1,  1891] loss: 0.048
[1,  1892] loss: 0.049
[1,  1893] loss: 0.048
[1,  1894] loss: 0.054
[1,  1895] loss: 0.047
[1,  1896] loss: 0.047
[1,  1897] loss: 0.044
[1,  1898] loss: 0.048
[1,  1899] loss: 0.053
[1,  1900] loss: 0.046
Model saved after epoch 0 and step 1900
[1,  1901] loss: 0.046
[1,  1902] loss: 0.047
[1,  1903] loss: 0.045
[1,  1904] loss: 0.049
[1,  1905] loss: 0.056
[1,  1906] loss: 0.044
[1,  1907] loss: 0.048
[1,  1908] loss: 0.048
[1,  1909] loss: 0.047
[1,  1910] loss: 0.051
[1,  1911] loss: 0.052
[1,  1912] loss: 0.044
[1,  1913] loss: 0.046
[1,  1914] loss: 0.050
[1,  1915] loss: 0.044
[1,  1916] loss: 0.054
[1,  1917] loss: 0.050
[1,  1918] loss: 0.046
[1,  1919] loss: 0.042
[1,  1920] loss: 0.047
[1,  1921] loss: 0.046
[1,  1922] loss: 0.044
[1,  1923] loss: 0.049
[1,  1924] loss: 0.045
[1,  1925] loss: 0.052
[1,  1926] loss: 0.052
[1,  1927] loss: 0.054
[1,  1928] loss: 0.048
[1,  1929] loss: 0.045
[1,  1930] loss: 0.048
[1,  1931] loss: 0.051
[1,  1932] loss: 0.046
[1,  1933] loss: 0.049
[1,  1934] loss: 0.047
[1,  1935] loss: 0.051
[1,  1936] loss: 0.050
[1,  1937] loss: 0.043
[1,  1938] loss: 0.050
[1,  1939] loss: 0.048
[1,  1940] loss: 0.049
[1,  1941] loss: 0.051
[1,  1942] loss: 0.049
[1,  1943] loss: 0.053
[1,  1944] loss: 0.048
[1,  1945] loss: 0.053
[1,  1946] loss: 0.049
[1,  1947] loss: 0.047
[1,  1948] loss: 0.048
[1,  1949] loss: 0.046
[1,  1950] loss: 0.051
[1,  1951] loss: 0.049
[1,  1952] loss: 0.043
[1,  1953] loss: 0.053
[1,  1954] loss: 0.046
[1,  1955] loss: 0.053
[1,  1956] loss: 0.046
[1,  1957] loss: 0.051
[1,  1958] loss: 0.049
[1,  1959] loss: 0.049
[1,  1960] loss: 0.052
[1,  1961] loss: 0.056
[1,  1962] loss: 0.052
[1,  1963] loss: 0.056
[1,  1964] loss: 0.045
[1,  1965] loss: 0.054
[1,  1966] loss: 0.054
[1,  1967] loss: 0.060
[1,  1968] loss: 0.046
[1,  1969] loss: 0.054
[1,  1970] loss: 0.050
[1,  1971] loss: 0.044
[1,  1972] loss: 0.043
[1,  1973] loss: 0.047
[1,  1974] loss: 0.049
[1,  1975] loss: 0.045
[1,  1976] loss: 0.048
[1,  1977] loss: 0.046
[1,  1978] loss: 0.043
[1,  1979] loss: 0.048
[1,  1980] loss: 0.046
[1,  1981] loss: 0.050
[1,  1982] loss: 0.048
[1,  1983] loss: 0.044
[1,  1984] loss: 0.051
[1,  1985] loss: 0.042
[1,  1986] loss: 0.051
[1,  1987] loss: 0.046
[1,  1988] loss: 0.049
[1,  1989] loss: 0.046
[1,  1990] loss: 0.048
[1,  1991] loss: 0.044
[1,  1992] loss: 0.052
[1,  1993] loss: 0.050
[1,  1994] loss: 0.051
[1,  1995] loss: 0.047
[1,  1996] loss: 0.059
[1,  1997] loss: 0.046
[1,  1998] loss: 0.045
[1,  1999] loss: 0.053
[1,  2000] loss: 0.059
Model saved after epoch 0 and step 2000
[1,  2001] loss: 0.051
[1,  2002] loss: 0.053
[1,  2003] loss: 0.044
[1,  2004] loss: 0.048
[1,  2005] loss: 0.044
[1,  2006] loss: 0.052
[1,  2007] loss: 0.049
[1,  2008] loss: 0.045
[1,  2009] loss: 0.045
[1,  2010] loss: 0.049
[1,  2011] loss: 0.048
[1,  2012] loss: 0.044
[1,  2013] loss: 0.048
[1,  2014] loss: 0.048
[1,  2015] loss: 0.049
[1,  2016] loss: 0.044
[1,  2017] loss: 0.052
[1,  2018] loss: 0.049
[1,  2019] loss: 0.049
[1,  2020] loss: 0.050
[1,  2021] loss: 0.047
[1,  2022] loss: 0.047
[1,  2023] loss: 0.044
[1,  2024] loss: 0.047
[1,  2025] loss: 0.046
[1,  2026] loss: 0.053
[1,  2027] loss: 0.050
[1,  2028] loss: 0.045
[1,  2029] loss: 0.049
[1,  2030] loss: 0.053
[1,  2031] loss: 0.055
[1,  2032] loss: 0.044
[1,  2033] loss: 0.045
[1,  2034] loss: 0.049
[1,  2035] loss: 0.046
[1,  2036] loss: 0.048
[1,  2037] loss: 0.047
[1,  2038] loss: 0.051
[1,  2039] loss: 0.053
[1,  2040] loss: 0.045
[1,  2041] loss: 0.048
[1,  2042] loss: 0.047
[1,  2043] loss: 0.049
[1,  2044] loss: 0.048
[1,  2045] loss: 0.047
[1,  2046] loss: 0.053
[1,  2047] loss: 0.048
[1,  2048] loss: 0.049
[1,  2049] loss: 0.051
[1,  2050] loss: 0.048
[1,  2051] loss: 0.041
[1,  2052] loss: 0.047
[1,  2053] loss: 0.046
[1,  2054] loss: 0.053
[1,  2055] loss: 0.049
[1,  2056] loss: 0.050
[1,  2057] loss: 0.045
[1,  2058] loss: 0.047
[1,  2059] loss: 0.050
[1,  2060] loss: 0.049
[1,  2061] loss: 0.046
[1,  2062] loss: 0.049
[1,  2063] loss: 0.048
[1,  2064] loss: 0.045
[1,  2065] loss: 0.049
[1,  2066] loss: 0.051
[1,  2067] loss: 0.050
[1,  2068] loss: 0.053
[1,  2069] loss: 0.044
[1,  2070] loss: 0.052
[1,  2071] loss: 0.052
[1,  2072] loss: 0.046
[1,  2073] loss: 0.051
[1,  2074] loss: 0.046
[1,  2075] loss: 0.045
[1,  2076] loss: 0.048
[1,  2077] loss: 0.054
[1,  2078] loss: 0.045
[1,  2079] loss: 0.048
[1,  2080] loss: 0.045
[1,  2081] loss: 0.050
[1,  2082] loss: 0.040
[1,  2083] loss: 0.044
[1,  2084] loss: 0.046
[1,  2085] loss: 0.044
[1,  2086] loss: 0.044
[1,  2087] loss: 0.048
[1,  2088] loss: 0.048
[1,  2089] loss: 0.047
[1,  2090] loss: 0.050
[1,  2091] loss: 0.052
[1,  2092] loss: 0.046
[1,  2093] loss: 0.050
[1,  2094] loss: 0.052
[1,  2095] loss: 0.053
[1,  2096] loss: 0.050
[1,  2097] loss: 0.043
[1,  2098] loss: 0.049
[1,  2099] loss: 0.049
[1,  2100] loss: 0.052
Model saved after epoch 0 and step 2100
[1,  2101] loss: 0.052
[1,  2102] loss: 0.051
[1,  2103] loss: 0.052
[1,  2104] loss: 0.051
[1,  2105] loss: 0.046
[1,  2106] loss: 0.044
[1,  2107] loss: 0.048
[1,  2108] loss: 0.046
[1,  2109] loss: 0.051
[1,  2110] loss: 0.052
[1,  2111] loss: 0.049
[1,  2112] loss: 0.049
[1,  2113] loss: 0.060
[1,  2114] loss: 0.045
[1,  2115] loss: 0.053
[1,  2116] loss: 0.047
[1,  2117] loss: 0.049
[1,  2118] loss: 0.048
[1,  2119] loss: 0.057
[1,  2120] loss: 0.050
[1,  2121] loss: 0.046
[1,  2122] loss: 0.048
[1,  2123] loss: 0.045
[1,  2124] loss: 0.047
[1,  2125] loss: 0.048
[1,  2126] loss: 0.048
[1,  2127] loss: 0.046
[1,  2128] loss: 0.054
[1,  2129] loss: 0.048
[1,  2130] loss: 0.048
[1,  2131] loss: 0.045
[1,  2132] loss: 0.047
[1,  2133] loss: 0.047
[1,  2134] loss: 0.046
[1,  2135] loss: 0.051
[1,  2136] loss: 0.052
[1,  2137] loss: 0.047
[1,  2138] loss: 0.049
[1,  2139] loss: 0.050
[1,  2140] loss: 0.045
[1,  2141] loss: 0.043
[1,  2142] loss: 0.044
[1,  2143] loss: 0.045
[1,  2144] loss: 0.049
[1,  2145] loss: 0.049
[1,  2146] loss: 0.052
[1,  2147] loss: 0.046
[1,  2148] loss: 0.049
[1,  2149] loss: 0.051
[1,  2150] loss: 0.049
[1,  2151] loss: 0.047
[1,  2152] loss: 0.050
[1,  2153] loss: 0.049
[1,  2154] loss: 0.049
[1,  2155] loss: 0.048
[1,  2156] loss: 0.049
[1,  2157] loss: 0.052
[1,  2158] loss: 0.053
[1,  2159] loss: 0.047
[1,  2160] loss: 0.050
[1,  2161] loss: 0.050
[1,  2162] loss: 0.051
[1,  2163] loss: 0.049
[1,  2164] loss: 0.050
[1,  2165] loss: 0.050
[1,  2166] loss: 0.049
[1,  2167] loss: 0.042
[1,  2168] loss: 0.047
[1,  2169] loss: 0.050
[1,  2170] loss: 0.048
[1,  2171] loss: 0.046
[1,  2172] loss: 0.052
[1,  2173] loss: 0.055
[1,  2174] loss: 0.052
[1,  2175] loss: 0.049
[1,  2176] loss: 0.051
[1,  2177] loss: 0.048
[1,  2178] loss: 0.051
[1,  2179] loss: 0.046
[1,  2180] loss: 0.046
[1,  2181] loss: 0.048
[1,  2182] loss: 0.048
[1,  2183] loss: 0.044
[1,  2184] loss: 0.051
[1,  2185] loss: 0.047
[1,  2186] loss: 0.046
[1,  2187] loss: 0.046
[1,  2188] loss: 0.049
[1,  2189] loss: 0.044
[1,  2190] loss: 0.047
[1,  2191] loss: 0.048
[1,  2192] loss: 0.050
[1,  2193] loss: 0.044
[1,  2194] loss: 0.049
[1,  2195] loss: 0.046
[1,  2196] loss: 0.041
[1,  2197] loss: 0.046
[1,  2198] loss: 0.046
[1,  2199] loss: 0.046
[1,  2200] loss: 0.046
Model saved after epoch 0 and step 2200
[1,  2201] loss: 0.048
[1,  2202] loss: 0.048
[1,  2203] loss: 0.043
[1,  2204] loss: 0.049
[1,  2205] loss: 0.050
[1,  2206] loss: 0.047
[1,  2207] loss: 0.050
[1,  2208] loss: 0.046
[1,  2209] loss: 0.050
[1,  2210] loss: 0.050
[1,  2211] loss: 0.050
[1,  2212] loss: 0.047
[1,  2213] loss: 0.048
[1,  2214] loss: 0.048
[1,  2215] loss: 0.051
[1,  2216] loss: 0.050
[1,  2217] loss: 0.051
[1,  2218] loss: 0.054
[1,  2219] loss: 0.049
[1,  2220] loss: 0.052
[1,  2221] loss: 0.046
[1,  2222] loss: 0.054
[1,  2223] loss: 0.053
[1,  2224] loss: 0.047
[1,  2225] loss: 0.048
[1,  2226] loss: 0.047
[1,  2227] loss: 0.046
[1,  2228] loss: 0.047
[1,  2229] loss: 0.047
[1,  2230] loss: 0.048
[1,  2231] loss: 0.049
[1,  2232] loss: 0.047
[1,  2233] loss: 0.047
[1,  2234] loss: 0.047
[1,  2235] loss: 0.052
[1,  2236] loss: 0.045
[1,  2237] loss: 0.049
[1,  2238] loss: 0.047
[1,  2239] loss: 0.051
[1,  2240] loss: 0.043
[1,  2241] loss: 0.053
[1,  2242] loss: 0.052
[1,  2243] loss: 0.048
[1,  2244] loss: 0.055
[1,  2245] loss: 0.048
[1,  2246] loss: 0.042
[1,  2247] loss: 0.049
[1,  2248] loss: 0.048
[1,  2249] loss: 0.048
[1,  2250] loss: 0.046
[1,  2251] loss: 0.056
[1,  2252] loss: 0.044
[1,  2253] loss: 0.047
[1,  2254] loss: 0.047
[1,  2255] loss: 0.048
[1,  2256] loss: 0.046
[1,  2257] loss: 0.046
[1,  2258] loss: 0.041
[1,  2259] loss: 0.047
[1,  2260] loss: 0.047
[1,  2261] loss: 0.050
[1,  2262] loss: 0.050
[1,  2263] loss: 0.052
[1,  2264] loss: 0.049
[1,  2265] loss: 0.051
[1,  2266] loss: 0.048
[1,  2267] loss: 0.055
[1,  2268] loss: 0.050
[1,  2269] loss: 0.044
[1,  2270] loss: 0.044
[1,  2271] loss: 0.051
[1,  2272] loss: 0.044
[1,  2273] loss: 0.047
[1,  2274] loss: 0.050
[1,  2275] loss: 0.049
[1,  2276] loss: 0.049
[1,  2277] loss: 0.047
[1,  2278] loss: 0.045
[1,  2279] loss: 0.049
[1,  2280] loss: 0.047
[1,  2281] loss: 0.042
[1,  2282] loss: 0.049
[1,  2283] loss: 0.049
[1,  2284] loss: 0.053
[1,  2285] loss: 0.044
[1,  2286] loss: 0.046
[1,  2287] loss: 0.046
[1,  2288] loss: 0.051
[1,  2289] loss: 0.045
[1,  2290] loss: 0.043
[1,  2291] loss: 0.043
[1,  2292] loss: 0.040
[1,  2293] loss: 0.047
[1,  2294] loss: 0.049
[1,  2295] loss: 0.045
[1,  2296] loss: 0.046
[1,  2297] loss: 0.053
[1,  2298] loss: 0.056
[1,  2299] loss: 0.046
[1,  2300] loss: 0.047
Model saved after epoch 0 and step 2300
[1,  2301] loss: 0.050
[1,  2302] loss: 0.047
[1,  2303] loss: 0.047
[1,  2304] loss: 0.046
[1,  2305] loss: 0.052
[1,  2306] loss: 0.047
[1,  2307] loss: 0.044
[1,  2308] loss: 0.046
[1,  2309] loss: 0.049
[1,  2310] loss: 0.057
[1,  2311] loss: 0.047
[1,  2312] loss: 0.057
[1,  2313] loss: 0.049
[1,  2314] loss: 0.046
[1,  2315] loss: 0.045
[1,  2316] loss: 0.048
[1,  2317] loss: 0.050
[1,  2318] loss: 0.047
[1,  2319] loss: 0.047
[1,  2320] loss: 0.047
[1,  2321] loss: 0.049
[1,  2322] loss: 0.042
[1,  2323] loss: 0.045
[1,  2324] loss: 0.050
[1,  2325] loss: 0.048
[1,  2326] loss: 0.049
[1,  2327] loss: 0.049
[1,  2328] loss: 0.050
[1,  2329] loss: 0.045
[1,  2330] loss: 0.050
[1,  2331] loss: 0.046
[1,  2332] loss: 0.050
[1,  2333] loss: 0.046
[1,  2334] loss: 0.049
[1,  2335] loss: 0.043
[1,  2336] loss: 0.047
[1,  2337] loss: 0.052
[1,  2338] loss: 0.043
[1,  2339] loss: 0.045
[1,  2340] loss: 0.049
[1,  2341] loss: 0.046
[1,  2342] loss: 0.043
[1,  2343] loss: 0.051
[1,  2344] loss: 0.048
[1,  2345] loss: 0.052
[1,  2346] loss: 0.043
[1,  2347] loss: 0.052
[1,  2348] loss: 0.055
[1,  2349] loss: 0.045
[1,  2350] loss: 0.053
[1,  2351] loss: 0.046
[1,  2352] loss: 0.050
[1,  2353] loss: 0.042
[1,  2354] loss: 0.042
[1,  2355] loss: 0.045
[1,  2356] loss: 0.048
[1,  2357] loss: 0.049
[1,  2358] loss: 0.053
[1,  2359] loss: 0.048
[1,  2360] loss: 0.044
[1,  2361] loss: 0.050
[1,  2362] loss: 0.047
[1,  2363] loss: 0.047
[1,  2364] loss: 0.050
[1,  2365] loss: 0.046
[1,  2366] loss: 0.052
[1,  2367] loss: 0.047
[1,  2368] loss: 0.047
[1,  2369] loss: 0.044
[1,  2370] loss: 0.048
[1,  2371] loss: 0.047
[1,  2372] loss: 0.051
[1,  2373] loss: 0.039
[1,  2374] loss: 0.049
[1,  2375] loss: 0.046
[1,  2376] loss: 0.050
[1,  2377] loss: 0.048
[1,  2378] loss: 0.046
[1,  2379] loss: 0.056
[1,  2380] loss: 0.047
[1,  2381] loss: 0.046
[1,  2382] loss: 0.046
[1,  2383] loss: 0.051
[1,  2384] loss: 0.056
[1,  2385] loss: 0.046
[1,  2386] loss: 0.044
[1,  2387] loss: 0.047
[1,  2388] loss: 0.047
[1,  2389] loss: 0.046
[1,  2390] loss: 0.047
[1,  2391] loss: 0.050
[1,  2392] loss: 0.046
[1,  2393] loss: 0.049
[1,  2394] loss: 0.059
[1,  2395] loss: 0.045
[1,  2396] loss: 0.044
[1,  2397] loss: 0.045
[1,  2398] loss: 0.045
[1,  2399] loss: 0.051
[1,  2400] loss: 0.052
Model saved after epoch 0 and step 2400
[1,  2401] loss: 0.047
[1,  2402] loss: 0.047
[1,  2403] loss: 0.045
[1,  2404] loss: 0.050
[1,  2405] loss: 0.050
[1,  2406] loss: 0.050
[1,  2407] loss: 0.046
[1,  2408] loss: 0.055
[1,  2409] loss: 0.048
[1,  2410] loss: 0.043
[1,  2411] loss: 0.044
[1,  2412] loss: 0.047
[1,  2413] loss: 0.053
[1,  2414] loss: 0.048
[1,  2415] loss: 0.048
[1,  2416] loss: 0.047
[1,  2417] loss: 0.055
[1,  2418] loss: 0.044
[1,  2419] loss: 0.049
[1,  2420] loss: 0.043
[1,  2421] loss: 0.047
[1,  2422] loss: 0.052
[1,  2423] loss: 0.046
[1,  2424] loss: 0.045
[1,  2425] loss: 0.048
[1,  2426] loss: 0.049
[1,  2427] loss: 0.046
[1,  2428] loss: 0.049
[1,  2429] loss: 0.047
[1,  2430] loss: 0.050
[1,  2431] loss: 0.050
[1,  2432] loss: 0.052
[1,  2433] loss: 0.047
[1,  2434] loss: 0.049
[1,  2435] loss: 0.056
[1,  2436] loss: 0.046
[1,  2437] loss: 0.052
[1,  2438] loss: 0.048
[1,  2439] loss: 0.054
[1,  2440] loss: 0.047
[1,  2441] loss: 0.046
[1,  2442] loss: 0.050
[1,  2443] loss: 0.049
[1,  2444] loss: 0.046
[1,  2445] loss: 0.045
[1,  2446] loss: 0.045
[1,  2447] loss: 0.046
[1,  2448] loss: 0.047
[1,  2449] loss: 0.051
[1,  2450] loss: 0.046
[1,  2451] loss: 0.048
[1,  2452] loss: 0.048
[1,  2453] loss: 0.050
[1,  2454] loss: 0.048
[1,  2455] loss: 0.048
[1,  2456] loss: 0.047
[1,  2457] loss: 0.049
[1,  2458] loss: 0.051
[1,  2459] loss: 0.049
[1,  2460] loss: 0.045
[1,  2461] loss: 0.048
[1,  2462] loss: 0.045
[1,  2463] loss: 0.048
[1,  2464] loss: 0.047
[1,  2465] loss: 0.050
[1,  2466] loss: 0.049
[1,  2467] loss: 0.052
[1,  2468] loss: 0.049
[1,  2469] loss: 0.046
[1,  2470] loss: 0.051
[1,  2471] loss: 0.051
[1,  2472] loss: 0.043
[1,  2473] loss: 0.044
[1,  2474] loss: 0.050
[1,  2475] loss: 0.048
[1,  2476] loss: 0.050
[1,  2477] loss: 0.058
[1,  2478] loss: 0.049
[1,  2479] loss: 0.056
[1,  2480] loss: 0.048
