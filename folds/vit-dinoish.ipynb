{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82500098-49ea-4a7e-bb63-ce5130f954cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>label</th>\n",
       "      <th>image_width</th>\n",
       "      <th>image_height</th>\n",
       "      <th>is_tma</th>\n",
       "      <th>tile_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>HGSC</td>\n",
       "      <td>23785</td>\n",
       "      <td>20008</td>\n",
       "      <td>False</td>\n",
       "      <td>../tiles_768/4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>66</td>\n",
       "      <td>LGSC</td>\n",
       "      <td>48871</td>\n",
       "      <td>48195</td>\n",
       "      <td>False</td>\n",
       "      <td>../tiles_768/66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>91</td>\n",
       "      <td>HGSC</td>\n",
       "      <td>3388</td>\n",
       "      <td>3388</td>\n",
       "      <td>True</td>\n",
       "      <td>../tiles_768/91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>281</td>\n",
       "      <td>LGSC</td>\n",
       "      <td>42309</td>\n",
       "      <td>15545</td>\n",
       "      <td>False</td>\n",
       "      <td>../tiles_768/281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>286</td>\n",
       "      <td>EC</td>\n",
       "      <td>37204</td>\n",
       "      <td>30020</td>\n",
       "      <td>False</td>\n",
       "      <td>../tiles_768/286</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   image_id label  image_width  image_height  is_tma         tile_path\n",
       "0         4  HGSC        23785         20008   False    ../tiles_768/4\n",
       "1        66  LGSC        48871         48195   False   ../tiles_768/66\n",
       "2        91  HGSC         3388          3388    True   ../tiles_768/91\n",
       "3       281  LGSC        42309         15545   False  ../tiles_768/281\n",
       "4       286    EC        37204         30020   False  ../tiles_768/286"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def get_image_path(image_id:int):\n",
    "    return os.path.join('../tiles_768', str(image_id))\n",
    "\n",
    "I_FOLD = 4\n",
    "train = pd.read_csv(f\"train_fold_{I_FOLD}.csv\")\n",
    "validation = pd.read_csv(f\"val_fold_{I_FOLD}.csv\")\n",
    "\n",
    "train['tile_path'] = train['image_id'].apply(lambda x: get_image_path(x))\n",
    "validation['tile_path'] = validation['image_id'].apply(lambda x: get_image_path(x))\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d782638-2c6c-4b63-87b3-c7bf291bbde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import timm\n",
    "from timm.models import VisionTransformer\n",
    "from timm.layers import SwiGLUPacked\n",
    "from timm.models.layers import DropPath\n",
    "import copy\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "class CustomViT(nn.Module):\n",
    "    def __init__(self, n_classes=5, embed_dim=384):\n",
    "        super().__init__()\n",
    "        self.n_classes = n_classes\n",
    "        self.embed_dim = embed_dim\n",
    "        # Load the base ViT model\n",
    "        self.base_model = VisionTransformer(img_size=224, num_classes=self.n_classes, patch_size=16, embed_dim=self.embed_dim, depth=12, num_heads=6, global_pool='avg', pre_norm=True)\n",
    "\n",
    "        # Initialize a learnable mask token\n",
    "        self.mask_token = nn.Parameter(torch.randn(1, 1, self.embed_dim))\n",
    "        \n",
    "        max_drop_path_rate = 0.4\n",
    "        dropout_rate = 0.1\n",
    "\n",
    "        drop_path_rates = [x.item() for x in torch.linspace(0, max_drop_path_rate, len(self.base_model.blocks))]\n",
    "\n",
    "        # Assign drop path rates\n",
    "        for i, block in enumerate(self.base_model.blocks):\n",
    "            block.drop_path1 = DropPath(drop_prob=drop_path_rates[i])\n",
    "            block.drop_path2 = DropPath(drop_prob=drop_path_rates[i])\n",
    "            block.attn.attn_drop = nn.Dropout(p=dropout_rate, inplace=False)\n",
    "            block.attn.proj_drop = nn.Dropout(p=dropout_rate, inplace=False)\n",
    "            block.mlp.drop1 = nn.Dropout(p=dropout_rate, inplace=False)\n",
    "            block.mlp.drop2 = nn.Dropout(p=dropout_rate, inplace=False)\n",
    "        self.head_dropout = nn.Dropout(p=dropout_rate, inplace=False)\n",
    "\n",
    "        self.class_token_head = nn.Linear(self.embed_dim, self.n_classes)\n",
    "        self.patch_token_head = nn.Linear(self.embed_dim, self.n_classes) \n",
    "\n",
    "    def forward_features(self, x, mask=None):\n",
    "        # Get the patch embeddings (excluding the class token)\n",
    "        x = self.base_model.patch_embed(x)\n",
    "        \n",
    "        to_cat = []\n",
    "        if self.base_model.cls_token is not None:\n",
    "            to_cat.append(self.base_model.cls_token.expand(x.shape[0], -1, -1))\n",
    "        x = torch.cat(to_cat + [x], dim=1)\n",
    "\n",
    "        # Handle masked patches if a mask is provided\n",
    "        if mask is not None:\n",
    "            # Adjust mask to account for the class token\n",
    "            mask = torch.cat([torch.zeros(1, 1).bool().to(mask.device), mask], dim=1)\n",
    "            # Expand mask token to match the batch size and masked patches\n",
    "            mask_tokens = self.mask_token.expand(x.size(0), -1, -1)\n",
    "            # Apply the mask - replace masked patches with the mask token\n",
    "            x = torch.where(mask.unsqueeze(-1), mask_tokens, x)\n",
    "        \n",
    "        x = self.base_model.pos_drop(x + self.base_model.pos_embed)\n",
    "        x = self.base_model.norm_pre(x)\n",
    "        x = self.base_model.blocks(x)\n",
    "        x = self.base_model.norm(x)\n",
    "\n",
    "        # Exclude the class token and return the patch representations\n",
    "        return x\n",
    "\n",
    "    def forward_head(self, x):\n",
    "        class_token, patch_tokens = x[:, :1], x[:, 1:]\n",
    "\n",
    "        # Apply dropout\n",
    "        class_token = self.head_dropout(class_token)\n",
    "        patch_tokens = self.head_dropout(patch_tokens)\n",
    "\n",
    "        # Process class token and patch tokens through their respective heads\n",
    "        class_token_output = self.class_token_head(class_token)\n",
    "        patch_token_output = self.patch_token_head(patch_tokens)\n",
    "        \n",
    "        x = torch.cat([class_token_output, patch_token_output], dim=1)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        x = self.forward_features(x, mask=mask)\n",
    "        x = self.forward_head(x)\n",
    "        return x\n",
    "\n",
    "D_MODEL = 384\n",
    "N_CLASSES = 8192\n",
    "model = CustomViT(n_classes=N_CLASSES, embed_dim=D_MODEL)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Initialize EMA model\n",
    "ema_decay = 0.9995  # decay factor for EMA\n",
    "ema_model = copy.deepcopy(model)\n",
    "ema_model = ema_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9613dea-4edf-47cd-a042-5f3468b19d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "\n",
    "integer_to_label = {\n",
    "    0: 'HGSC',\n",
    "    1: 'CC',\n",
    "    2: 'EC',\n",
    "    3: 'LGSC',\n",
    "    4: 'MC',\n",
    "}\n",
    "\n",
    "label_to_integer = {\n",
    "    'HGSC': 0,\n",
    "    'CC': 1,\n",
    "    'EC': 2,\n",
    "    'LGSC': 3,\n",
    "    'MC': 4,\n",
    "}\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.transform = transform\n",
    "        self.all_images = []  # Store all images in an interlaced fashion\n",
    "        self.folder_images = []  # Temporary storage for images from each folder\n",
    "\n",
    "        # Step 1: Collect all images from each folder\n",
    "        for index, row in dataframe.iterrows():\n",
    "            folder_path = row['tile_path']\n",
    "            label = row['label']\n",
    "            image_id = row['image_id']\n",
    "            if os.path.isdir(folder_path):\n",
    "                image_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.lower().endswith('.png')]\n",
    "                random.shuffle(image_files)\n",
    "                self.folder_images.append([(image_file, label, image_id) for image_file in image_files])\n",
    "\n",
    "        # Step 2: Interlace the images\n",
    "        max_length = max(len(images) for images in self.folder_images)\n",
    "        for i in range(max_length):\n",
    "            for images in self.folder_images:\n",
    "                self.all_images.append(images[i % len(images)])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path, label, image_id = self.all_images[idx]\n",
    "        image = Image.open(image_path)\n",
    "        \n",
    "        augment_a = self.transform(image)\n",
    "        augment_b = self.transform(image)\n",
    "\n",
    "        return augment_a, augment_b, label_to_integer[label], image_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "daab3db2-8fff-4b14-83f9-608c256dea11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(size=224, scale=(0.25, 1.0), ratio=(0.75, 1.33)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=(0, 360)),\n",
    "    transforms.RandomAffine(degrees=0, shear=(-20, 20, -20, 20)),\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.3),\n",
    "    transforms.RandomApply([transforms.Grayscale(num_output_channels=3)], p=0.5),\n",
    "    transforms.RandomApply([transforms.GaussianBlur(kernel_size=(5, 5), sigma=(0.1, 2))], p=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "])\n",
    "\n",
    "train_dataset = ImageDataset(dataframe=train, transform=transform)\n",
    "\n",
    "# Calculate weights for each class\n",
    "class_counts = [1 for label in label_to_integer] # equally weighted\n",
    "num_samples = sum(class_counts)\n",
    "class_weights = [num_samples / class_count for class_count in class_counts]\n",
    "\n",
    "# Assign a weight to each sample in the dataset based on its class\n",
    "sample_weights = [class_weights[label_to_integer[label]] for _, label, _ in train_dataset.all_images]\n",
    "\n",
    "# Create WeightedRandomSampler\n",
    "sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)\n",
    "\n",
    "# DataLoader with WeightedRandomSampler\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=sampler, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0aa0e2bd-4c21-48a1-a0fe-5eb15fb556b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "# Get the root logger\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# Optional: Remove all existing handlers from the logger\n",
    "for handler in logger.handlers[:]:\n",
    "    logger.removeHandler(handler)\n",
    "\n",
    "# Set the logging level\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Create a FileHandler and add it to the logger\n",
    "file_handler = logging.FileHandler(f'logs/vit_self_supervised/fold_{I_FOLD}.txt')\n",
    "file_handler.setLevel(logging.INFO)\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "# Create a StreamHandler for stderr and add it to the logger\n",
    "stream_handler = logging.StreamHandler(sys.stderr)\n",
    "stream_handler.setLevel(logging.ERROR)  # Only log ERROR and CRITICAL messages to stderr\n",
    "logger.addHandler(stream_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ebabdca3-7c43-4571-96ac-2fd20d66775e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KoLeoLoss(nn.Module):\n",
    "    \"\"\"Kozachenko-Leonenko entropic loss regularizer from Sablayrolles et al. - 2018 - Spreading vectors for similarity search\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.pdist = nn.PairwiseDistance(2, eps=1e-8)\n",
    "\n",
    "    def pairwise_NNs_inner(self, x):\n",
    "        \"\"\"\n",
    "        Pairwise nearest neighbors for L2-normalized vectors.\n",
    "        Uses Torch rather than Faiss to remain on GPU.\n",
    "        \"\"\"\n",
    "        # parwise dot products (= inverse distance)\n",
    "        dots = torch.mm(x, x.t())\n",
    "        n = x.shape[0]\n",
    "        dots.view(-1)[:: (n + 1)].fill_(-1)  # Trick to fill diagonal with -1\n",
    "        # max inner prod -> min distance\n",
    "        _, I = torch.max(dots, dim=1)  # noqa: E741\n",
    "        return I\n",
    "\n",
    "    def forward(self, student_output, eps=1e-8):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            student_output (BxD): backbone output of student\n",
    "        \"\"\"\n",
    "        with torch.cuda.amp.autocast(enabled=False):\n",
    "            student_output = F.normalize(student_output, eps=eps, p=2, dim=-1)\n",
    "            I = self.pairwise_NNs_inner(student_output)  # noqa: E741\n",
    "            distances = self.pdist(student_output, student_output[I])  # BxD, BxD -> B\n",
    "            loss = -torch.log(distances + eps).mean()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f9b938-9860-4147-be7e-f6f36938f212",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import logging\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "import random\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import torch.nn.functional as F\n",
    "\n",
    "initial_lr = 0.0005 * BATCH_SIZE/256\n",
    "final_lr = initial_lr * 0.01\n",
    "num_epochs = 10000\n",
    "\n",
    "# Function for linear warmup\n",
    "def learning_rate(step, warmup_steps=10000, max_steps=100000):\n",
    "    if step < warmup_steps:\n",
    "        return initial_lr * (float(step) / float(max(1, warmup_steps)))\n",
    "    elif step < max_steps:\n",
    "        progress = (float(step - warmup_steps) / float(max(1, max_steps - warmup_steps)))\n",
    "        cos_component = 0.5 * (1 + math.cos(math.pi * progress))\n",
    "        return final_lr + (initial_lr - final_lr) * cos_component\n",
    "    else:\n",
    "        return final_lr\n",
    "\n",
    "def update_ema_variables(model, ema_model, alpha, global_step):\n",
    "    # Update the EMA model parameters\n",
    "    with torch.no_grad():\n",
    "        for ema_param, param in zip(ema_model.parameters(), model.parameters()):\n",
    "            ema_param.data.mul_(alpha).add_(param.data, alpha=1 - alpha)\n",
    "\n",
    "scaler = GradScaler()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=initial_lr, weight_decay=5e-2)\n",
    "koleo_loss = KoLeoLoss()\n",
    "\n",
    "best_val_accuracy = 0.0\n",
    "step = 0\n",
    "\n",
    "student_temperature = 0.1\n",
    "teacher_temperature = 0.04\n",
    "\n",
    "center_momentum = 0.9\n",
    "center_class = torch.zeros(1, N_CLASSES).to(device)\n",
    "center_patch = torch.zeros(1, N_CLASSES).to(device)\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    ema_model.eval()\n",
    "    \n",
    "    for i, (augment_a_images, augment_b_images, _, _) in enumerate(train_dataloader, 0):\n",
    "        # Convert images to PIL format\n",
    "        augment_a_images = augment_a_images.to(device)\n",
    "        augment_b_images = augment_b_images.to(device)\n",
    "\n",
    "        # Linearly increase the learning rate\n",
    "        lr = learning_rate(step)\n",
    "        for g in optimizer.param_groups:\n",
    "            g['lr'] = lr\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass with autocast\n",
    "        with autocast():\n",
    "            mask = torch.rand((1, (224 // 16) ** 2)) < 0.5\n",
    "            mask = mask.to(device)\n",
    "            student_a_outputs = model(augment_a_images, mask=mask)\n",
    "            student_b_outputs = model(augment_b_images, mask=mask)\n",
    "            with torch.no_grad():\n",
    "                teacher_a_outputs = ema_model(augment_a_images)\n",
    "                teacher_b_outputs = ema_model(augment_b_images)\n",
    "            \n",
    "            teacher_a_outputs = teacher_a_outputs.detach()\n",
    "            teacher_b_outputs = teacher_b_outputs.detach()\n",
    "\n",
    "            student_a_probs = F.softmax(student_a_outputs / student_temperature, dim=2)\n",
    "            student_b_probs = F.softmax(student_b_outputs / student_temperature, dim=2)\n",
    "            teacher_a_class_probs = F.softmax((teacher_a_outputs[:, :1] - center_class.unsqueeze(1)) / teacher_temperature, dim=2)\n",
    "            teacher_a_patch_probs = F.softmax((teacher_a_outputs[:, 1:] - center_patch.unsqueeze(1)) / teacher_temperature, dim=2)\n",
    "            teacher_b_class_probs = F.softmax((teacher_b_outputs[:, :1] - center_class.unsqueeze(1)) / teacher_temperature, dim=2)\n",
    "            teacher_b_patch_probs = F.softmax((teacher_b_outputs[:, 1:] - center_patch.unsqueeze(1)) / teacher_temperature, dim=2)\n",
    "            \n",
    "            class_loss = - (teacher_a_class_probs * torch.log(student_b_probs[:, :1] + 1e-9)).sum(dim=2).mean()\n",
    "            class_loss += - (teacher_b_class_probs * torch.log(student_a_probs[:, :1] + 1e-9)).sum(dim=2).mean()\n",
    "            class_loss /= 2\n",
    "            \n",
    "            patch_loss = - (teacher_a_patch_probs * torch.log(student_a_probs[:, 1:] + 1e-9)).sum(dim=2).mean()\n",
    "            patch_loss += - (teacher_b_patch_probs * torch.log(student_b_probs[:, 1:] + 1e-9)).sum(dim=2).mean()\n",
    "            patch_loss /= 2\n",
    "\n",
    "            loss = 0.495 * class_loss\n",
    "            loss += 0.495 * patch_loss\n",
    "            loss += 0.01 * (koleo_loss(student_a_outputs[:, 0]) + koleo_loss(student_a_outputs[:, 1:].reshape((224 // 16) ** 2 * BATCH_SIZE, N_CLASSES)))\n",
    "            \n",
    "            teacher_class_means = torch.cat([teacher_a_outputs[:, :1], teacher_b_outputs[:, :1]], dim=1).mean(dim=(0, 1))\n",
    "            teacher_patch_means = torch.cat([teacher_a_outputs[:, 1:], teacher_b_outputs[:, 1:]], dim=1).mean(dim=(0, 1))\n",
    "            \n",
    "            center_class = center_momentum * center_class + (1 - center_momentum) * teacher_class_means\n",
    "            center_patch = center_momentum * center_patch + (1 - center_momentum) * teacher_patch_means\n",
    "            \n",
    "            class_entropy = -torch.sum(teacher_a_class_probs * torch.log(teacher_a_class_probs + 1e-9), dim=2).mean()\n",
    "            class_entropy += -torch.sum(teacher_b_class_probs * torch.log(teacher_b_class_probs + 1e-9), dim=2).mean()\n",
    "            class_entropy /= 2\n",
    "        \n",
    "        # Backward pass\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        update_ema_variables(model, ema_model, ema_decay, step)\n",
    "\n",
    "        # logging.info('[%d, %5d] loss: %.3f | class_loss: %.3f | patch_loss: %.3f | mean_koleo_distances: %.3f' % (epoch + 1, step, loss.item(), class_loss.item(), patch_loss.item(), mean_koleo_distances.item()))\n",
    "        logging.info('[%d, %5d] loss: %.3f | class_loss: %.3f | patch_loss: %.3f | class_entropy: %.3f' % (epoch + 1, step, loss.item(), class_loss.item(), patch_loss.item(), class_entropy.item()))\n",
    "\n",
    "        if step % 10000 == 0:\n",
    "            ema_model.eval()\n",
    "            torch.save(ema_model.state_dict(), f'vit_self_supervised_models/fold_{I_FOLD}/epoch_{epoch}_step_{step}.pth')\n",
    "            logging.info(f'Model saved after epoch {epoch} and step {step}')\\\n",
    "\n",
    "            model.train()\n",
    "\n",
    "        step += 1"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.2-0.m112",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.2-0:m112"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
