{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67bc7557-86fb-4b69-9043-6f8abbcea1eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>label</th>\n",
       "      <th>image_width</th>\n",
       "      <th>image_height</th>\n",
       "      <th>is_tma</th>\n",
       "      <th>tile_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33976</td>\n",
       "      <td>LGSC</td>\n",
       "      <td>38146</td>\n",
       "      <td>21220</td>\n",
       "      <td>False</td>\n",
       "      <td>../tiles_768/33976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>34277</td>\n",
       "      <td>LGSC</td>\n",
       "      <td>65805</td>\n",
       "      <td>35570</td>\n",
       "      <td>False</td>\n",
       "      <td>../tiles_768/34277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34688</td>\n",
       "      <td>LGSC</td>\n",
       "      <td>27441</td>\n",
       "      <td>19507</td>\n",
       "      <td>False</td>\n",
       "      <td>../tiles_768/34688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>35565</td>\n",
       "      <td>MC</td>\n",
       "      <td>2964</td>\n",
       "      <td>2964</td>\n",
       "      <td>True</td>\n",
       "      <td>../tiles_768/35565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>37385</td>\n",
       "      <td>LGSC</td>\n",
       "      <td>3388</td>\n",
       "      <td>3388</td>\n",
       "      <td>True</td>\n",
       "      <td>../tiles_768/37385</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   image_id label  image_width  image_height  is_tma           tile_path\n",
       "0     33976  LGSC        38146         21220   False  ../tiles_768/33976\n",
       "1     34277  LGSC        65805         35570   False  ../tiles_768/34277\n",
       "2     34688  LGSC        27441         19507   False  ../tiles_768/34688\n",
       "3     35565    MC         2964          2964    True  ../tiles_768/35565\n",
       "4     37385  LGSC         3388          3388    True  ../tiles_768/37385"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def get_image_path(image_id:int):\n",
    "    return os.path.join('../tiles_768', str(image_id))\n",
    "\n",
    "I_FOLD = 3\n",
    "validation = pd.read_csv(f\"val_fold_{I_FOLD}.csv\")\n",
    "\n",
    "validation['tile_path'] = validation['image_id'].apply(lambda x: get_image_path(x))\n",
    "validation.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca4693f0-2863-488b-b459-bbd12d56fd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import timm\n",
    "from timm.models import VisionTransformer\n",
    "from timm.models.layers import DropPath\n",
    "import copy\n",
    "\n",
    "class CustomViT(nn.Module):\n",
    "    def __init__(self, n_classes=5, embed_dim=768):\n",
    "        super().__init__()\n",
    "        self.n_classes = n_classes\n",
    "        self.embed_dim = embed_dim\n",
    "        # Load the base ViT model\n",
    "        self.base_model = VisionTransformer(img_size=384, num_classes=self.n_classes, patch_size=16, embed_dim=self.embed_dim, depth=12, num_heads=12, global_pool='avg', pre_norm=True)\n",
    "\n",
    "        # Initialize a learnable mask token\n",
    "        self.mask_token = nn.Parameter(torch.randn(1, 1, self.embed_dim))\n",
    "\n",
    "        max_drop_path_rate = 0.3\n",
    "        dropout_rate = 0.1\n",
    "\n",
    "        drop_path_rates = [x.item() for x in torch.linspace(0, max_drop_path_rate, len(self.base_model.blocks))]\n",
    "\n",
    "        # Assign drop path rates\n",
    "        for i, block in enumerate(self.base_model.blocks):\n",
    "            block.drop_path1 = DropPath(drop_prob=drop_path_rates[i])\n",
    "            block.drop_path2 = DropPath(drop_prob=drop_path_rates[i])\n",
    "            block.attn.attn_drop = nn.Dropout(p=dropout_rate, inplace=False)\n",
    "            block.attn.proj_drop = nn.Dropout(p=dropout_rate, inplace=False)\n",
    "            block.mlp.drop1 = nn.Dropout(p=dropout_rate, inplace=False)\n",
    "            block.mlp.drop2 = nn.Dropout(p=dropout_rate, inplace=False)\n",
    "        self.head_dropout = nn.Dropout(p=dropout_rate, inplace=False)\n",
    "\n",
    "        self.class_token_head = nn.Linear(self.embed_dim, self.n_classes)\n",
    "        self.patch_token_head = nn.Linear(self.embed_dim, self.embed_dim) \n",
    "\n",
    "    def forward_features(self, x, mask=None):\n",
    "        # Get the patch embeddings (excluding the class token)\n",
    "        x = self.base_model.patch_embed(x)\n",
    "\n",
    "        # Handle masked patches if a mask is provided\n",
    "        if mask is not None:\n",
    "            # Adjust mask to account for the class token\n",
    "            mask = torch.cat((torch.zeros(x.shape[0], 1).bool().to(mask.device), mask), dim=1)\n",
    "            # Expand mask token to match the batch size and masked patches\n",
    "            mask_tokens = self.mask_token.expand(x.size(0), -1, -1)\n",
    "            # Apply the mask - replace masked patches with the mask token\n",
    "            x = torch.where(mask.unsqueeze(-1), mask_tokens, x)\n",
    "\n",
    "        to_cat = []\n",
    "        if self.base_model.cls_token is not None:\n",
    "            to_cat.append(self.base_model.cls_token.expand(x.shape[0], -1, -1))\n",
    "        x = torch.cat(to_cat + [x], dim=1)\n",
    "\n",
    "        x = self.base_model.pos_drop(x + self.base_model.pos_embed)\n",
    "        x = self.base_model.norm_pre(x)\n",
    "        x = self.base_model.blocks(x)\n",
    "        x = self.base_model.norm(x)\n",
    "\n",
    "        # Exclude the class token and return the patch representations\n",
    "        return x\n",
    "\n",
    "    def forward_head(self, x):\n",
    "        class_token, patch_tokens = x[:, 0], x[:, 1:]\n",
    "\n",
    "        # Apply dropout\n",
    "        class_token = self.head_dropout(class_token)\n",
    "        patch_tokens = self.head_dropout(patch_tokens)\n",
    "\n",
    "        # Process class token and patch tokens through their respective heads\n",
    "        class_token_output = self.class_token_head(class_token)\n",
    "        patch_token_output = self.patch_token_head(patch_tokens)\n",
    "\n",
    "        return {\"class_token_output\": class_token_output, \"patch_token_output\": patch_token_output}\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.forward_features(x)\n",
    "        x = self.forward_head(x)\n",
    "        return x['class_token_output']\n",
    "\n",
    "def load_model(model_location):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = CustomViT(n_classes=5, embed_dim=768)\n",
    "\n",
    "    state_dict = torch.load(model_location, map_location=device)\n",
    "    model.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "070a7f0a-067b-4801-b049-308dce410b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import cv2  # Required for CLAHE\n",
    "\n",
    "integer_to_label = {\n",
    "    0: 'HGSC',\n",
    "    1: 'CC',\n",
    "    2: 'EC',\n",
    "    3: 'LGSC',\n",
    "    4: 'MC',\n",
    "}\n",
    "\n",
    "label_to_integer = {\n",
    "    'HGSC': 0,\n",
    "    'CC': 1,\n",
    "    'EC': 2,\n",
    "    'LGSC': 3,\n",
    "    'MC': 4,\n",
    "}\n",
    "\n",
    "def apply_clahe_to_color_image(img):\n",
    "    # Convert PIL Image to OpenCV format\n",
    "    img_cv = np.array(img)\n",
    "    img_cv = img_cv[:, :, ::-1]  # Convert RGB to BGR\n",
    "\n",
    "    # Split the image into its B, G, R channels\n",
    "    b, g, r = cv2.split(img_cv)\n",
    "\n",
    "    # Apply CLAHE to each channel\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    b_clahe = clahe.apply(b)\n",
    "    g_clahe = clahe.apply(g)\n",
    "    r_clahe = clahe.apply(r)\n",
    "\n",
    "    # Merge the CLAHE enhanced channels back together\n",
    "    img_clahe = cv2.merge([b_clahe, g_clahe, r_clahe])\n",
    "    img_clahe = cv2.cvtColor(img_clahe, cv2.COLOR_BGR2RGB)  # Convert BGR back to RGB\n",
    "\n",
    "    # Convert back to PIL Image\n",
    "    img_clahe_pil = Image.fromarray(img_clahe)\n",
    "\n",
    "    return img_clahe_pil\n",
    "\n",
    "# val_transform = transforms.Compose([\n",
    "#     transforms.Resize(448),\n",
    "#     # transforms.Lambda(lambda img: apply_clahe_to_color_image(img)),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=[\n",
    "#         0.48145466,\n",
    "#         0.4578275,\n",
    "#         0.40821073\n",
    "#     ], std=[\n",
    "#         0.26862954,\n",
    "#         0.26130258,\n",
    "#         0.27577711\n",
    "#     ]),\n",
    "# ])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize(384),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "663ccbda-c441-4951-9ced-9a9409b6a0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "# Get the root logger\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# Optional: Remove all existing handlers from the logger\n",
    "for handler in logger.handlers[:]:\n",
    "    logger.removeHandler(handler)\n",
    "\n",
    "# Set the logging level\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Create a FileHandler and add it to the logger\n",
    "file_handler = logging.FileHandler(f'validation_logs/vit_base_pretrained/fold_{I_FOLD}.txt')\n",
    "file_handler.setLevel(logging.INFO)\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "# Create a StreamHandler for stderr and add it to the logger\n",
    "stream_handler = logging.StreamHandler(sys.stderr)\n",
    "stream_handler.setLevel(logging.ERROR)  # Only log ERROR and CRITICAL messages to stderr\n",
    "logger.addHandler(stream_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0edfd8d9-132d-4966-a3fc-6c263a52e828",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def find_winner(graph, source, visited):\n",
    "    \"\"\"Helper function to find the winner in the graph.\"\"\"\n",
    "    if source not in graph:\n",
    "        return False\n",
    "\n",
    "    visited.add(source)\n",
    "    for target in graph[source]:\n",
    "        if target not in visited and find_winner(graph, target, visited):\n",
    "            return True\n",
    "    visited.remove(source)\n",
    "    return False\n",
    "\n",
    "def ranked_pairs_winner(image_logits):\n",
    "    num_voters, num_classes = image_logits.shape\n",
    "    margins = defaultdict(int)\n",
    "\n",
    "    # Perform pairwise comparisons\n",
    "    for i in range(num_classes):\n",
    "        for j in range(num_classes):\n",
    "            if i != j:\n",
    "                votes_for_i = torch.sum(image_logits[:, i] > image_logits[:, j])\n",
    "                votes_for_j = num_voters - votes_for_i\n",
    "                margins[(i, j)] = votes_for_i - votes_for_j\n",
    "\n",
    "    # Sort pairs by margin of victory\n",
    "    sorted_pairs = sorted(margins, key=margins.get, reverse=True)\n",
    "\n",
    "    # Initialize graph for locking pairs\n",
    "    graph = {i: set() for i in range(num_classes)}\n",
    "    for pair in sorted_pairs:\n",
    "        winner, loser = pair\n",
    "        graph[winner].add(loser)\n",
    "        visited = set()\n",
    "\n",
    "        # Check for cycle\n",
    "        if find_winner(graph, loser, visited):\n",
    "            graph[winner].remove(loser)\n",
    "\n",
    "    # Determine the winner\n",
    "    for i in range(num_classes):\n",
    "        if not any(i in targets for targets in graph.values()):\n",
    "            return i\n",
    "\n",
    "    # Fallback: Choose the class with the highest total votes if no clear winner is found\n",
    "    total_votes = torch.sum(image_logits, axis=0)\n",
    "    return torch.argmax(total_votes).item()\n",
    "\n",
    "def star_voting_winner(image_logits):\n",
    "    num_voters, num_classes = image_logits.shape\n",
    "\n",
    "    # Step 1: Sum the scores for each candidate\n",
    "    total_scores = torch.sum(image_logits, axis=0)\n",
    "\n",
    "    # Step 2: Find the two candidates with the highest total scores\n",
    "    top_two = torch.topk(total_scores, 2).indices\n",
    "\n",
    "    # Step 3: Runoff between the top two candidates\n",
    "    first_choice_votes = torch.sum(image_logits[:, top_two[0]] > image_logits[:, top_two[1]])\n",
    "    second_choice_votes = num_voters - first_choice_votes\n",
    "\n",
    "    # Determine the winner\n",
    "    if first_choice_votes > second_choice_votes:\n",
    "        return top_two[0].item()\n",
    "    else:\n",
    "        return top_two[1].item()\n",
    "\n",
    "def instant_runoff_winner(image_logits):\n",
    "    num_voters, num_classes = image_logits.shape\n",
    "    active_candidates = set(range(num_classes))\n",
    "\n",
    "    while True:\n",
    "        # Count the first-preference votes for each candidate\n",
    "        first_pref_counts = torch.zeros(num_classes)\n",
    "        for logits in image_logits:\n",
    "            for rank in torch.argsort(logits, descending=True):\n",
    "                if rank.item() in active_candidates:\n",
    "                    first_pref_counts[rank] += 1\n",
    "                    break\n",
    "\n",
    "        # Check if any candidate has more than 50% of the votes\n",
    "        if torch.any(first_pref_counts > num_voters / 2):\n",
    "            return torch.argmax(first_pref_counts).item()\n",
    "\n",
    "        # Find the candidate with the fewest votes among active candidates\n",
    "        active_candidates_votes = first_pref_counts[list(active_candidates)]\n",
    "        min_votes, min_index = torch.min(active_candidates_votes, 0)\n",
    "        min_candidate = list(active_candidates)[min_index.item()]\n",
    "\n",
    "        # Eliminate the candidate with the fewest votes\n",
    "        active_candidates.remove(min_candidate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c127a8d-18a9-43ab-b261-38782adf8a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import random\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "with torch.no_grad():\n",
    "    for step in range(30000, -2000, -2000):\n",
    "        model = load_model(f'vit_base_pretrained_models/fold_3/epoch_0_step_{step}.pth')\n",
    "        model.eval()\n",
    "        \n",
    "        image_ids = []\n",
    "        logits = []\n",
    "        labels = []\n",
    "        for idx, row in validation.iterrows():\n",
    "            if idx % 10 == 0:\n",
    "                logging.info(f'idx: {idx}')\n",
    "            random.seed(0)\n",
    "            path = row['tile_path']\n",
    "            all_files = [f for f in os.listdir(path) if f.lower().endswith('.png')]\n",
    "\n",
    "            batch_logits = []\n",
    "\n",
    "            # Prepare a list to hold image tiles\n",
    "            batch_tiles = []\n",
    "\n",
    "            sample_size = min(32, len(all_files))\n",
    "            sampled_files = random.sample(all_files, sample_size)\n",
    "\n",
    "            for image_name in sampled_files:\n",
    "                image_path = os.path.join(path, image_name)\n",
    "                sub_image = Image.open(image_path)\n",
    "\n",
    "                tile = val_transform(sub_image).unsqueeze(0).to(device)\n",
    "                batch_tiles.append(tile)\n",
    "\n",
    "            for i_batch in range(0, len(batch_tiles), 32):\n",
    "                outputs = model(torch.concat(batch_tiles[i_batch:i_batch+32], dim=0))\n",
    "                probs = outputs.softmax(dim=1)\n",
    "                batch_logits.append(outputs)\n",
    "\n",
    "            image_id = row['image_id']\n",
    "            batch_logits = torch.concat(batch_logits, dim=0)\n",
    "            label = row['label']\n",
    "            image_ids.append(image_id)\n",
    "            logits.append(batch_logits)\n",
    "            labels.append(label)\n",
    "            \n",
    "        logging.info(f'-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_')\n",
    "        logging.info(f'step: {step}')\n",
    "        \n",
    "        predictions = []\n",
    "        for image_logits in logits:\n",
    "            argmax_indices = torch.argmax(image_logits, dim=1)\n",
    "\n",
    "            frequency_counts = torch.bincount(argmax_indices, minlength=5)\n",
    "\n",
    "            max_vote_key = frequency_counts.argmax().cpu().item()\n",
    "            predictions.append(integer_to_label[max_vote_key])\n",
    "\n",
    "        plurality_accuracy = balanced_accuracy_score(labels, predictions)\n",
    "        logging.info(f'plurality_accuracy: {plurality_accuracy}')\n",
    "        \n",
    "        predictions = []\n",
    "        for image_logits in logits:\n",
    "            summed_logits = image_logits.sum(dim=0)\n",
    "\n",
    "            max_vote_key = summed_logits.argmax().cpu().item()\n",
    "            predictions.append(integer_to_label[max_vote_key])\n",
    "\n",
    "        logit_sum_accuracy = balanced_accuracy_score(labels, predictions)\n",
    "        logging.info(f'logit_sum_accuracy: {logit_sum_accuracy}')\n",
    "        \n",
    "        predictions = []\n",
    "        for image_logits in logits:\n",
    "            summed_probs = image_logits.softmax(dim=1).sum(dim=0)\n",
    "\n",
    "            max_vote_key = summed_probs.argmax().cpu().item()\n",
    "            predictions.append(integer_to_label[max_vote_key])\n",
    "\n",
    "        prob_sum_accuracy = balanced_accuracy_score(labels, predictions)\n",
    "        logging.info(f'prob_sum_accuracy: {logit_sum_accuracy}')\n",
    "\n",
    "        predictions = []\n",
    "        for image_logits in logits:\n",
    "            summed_log_probs = torch.log(image_logits.softmax(dim=1)).sum(dim=0)\n",
    "\n",
    "            max_vote_key = summed_log_probs.argmax().cpu().item()\n",
    "            predictions.append(integer_to_label[max_vote_key])\n",
    "\n",
    "        log_prob_sum_accuracy = balanced_accuracy_score(labels, predictions)\n",
    "        logging.info(f'log_prob_sum_accuracy: {log_prob_sum_accuracy}')\n",
    "        \n",
    "        predictions = []\n",
    "        for image_logits in logits:\n",
    "            summed_one_minus_log_probs = torch.log(1 - image_logits.softmax(dim=1)).sum(dim=0)\n",
    "\n",
    "            min_vote_key = summed_one_minus_log_probs.argmin().cpu().item()\n",
    "            predictions.append(integer_to_label[min_vote_key])\n",
    "\n",
    "        one_minus_log_prob_sum_accuracy = balanced_accuracy_score(labels, predictions)\n",
    "        logging.info(f'one_minus_log_prob_sum_accuracy: {one_minus_log_prob_sum_accuracy}')\n",
    "        \n",
    "        predictions = []\n",
    "        for image_logits in logits:\n",
    "            rp_winner = ranked_pairs_winner(image_logits)\n",
    "            predictions.append(integer_to_label[rp_winner])\n",
    "\n",
    "        rp_accuracy = balanced_accuracy_score(labels, predictions)\n",
    "        logging.info(f'rp_accuracy: {rp_accuracy}')\n",
    "        \n",
    "        predictions = []\n",
    "        for image_logits in logits:\n",
    "            star_winner = star_voting_winner(image_logits)\n",
    "            predictions.append(integer_to_label[star_winner])\n",
    "\n",
    "        star_accuracy = balanced_accuracy_score(labels, predictions)\n",
    "        logging.info(f'star_accuracy: {star_accuracy}')\n",
    "        \n",
    "        predictions = []\n",
    "        for image_logits in logits:\n",
    "            irv_winner = instant_runoff_winner(image_logits)\n",
    "            predictions.append(integer_to_label[irv_winner])\n",
    "\n",
    "        instant_runoff_accuracy = balanced_accuracy_score(labels, predictions)\n",
    "        logging.info(f'instant_runoff_accuracy: {instant_runoff_accuracy}')\n",
    "\n",
    "        logging.info(f'-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_')\n"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.2-0.m112",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.2-0:m112"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
