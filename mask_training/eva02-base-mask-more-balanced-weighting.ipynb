{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82500098-49ea-4a7e-bb63-ce5130f954cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "whole_df = pd.read_csv('../tile_masks_768.csv')\n",
    "\n",
    "whole_df['image_path'] = whole_df['image_path'].apply(lambda x: f'../{x}')\n",
    "train, validation = train_test_split(whole_df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d782638-2c6c-4b63-87b3-c7bf291bbde4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda and model timm/eva02_base_patch14_448.mim_in22k_ft_in22k_in1k\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import timm\n",
    "from timm.models.layers import DropPath\n",
    "import copy\n",
    "\n",
    "MODEL_NAME = 'even-more-balanced'\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_name = \"timm/eva02_base_patch14_448.mim_in22k_ft_in22k_in1k\"\n",
    "\n",
    "print(f\"Using device {device} and model {model_name}\")\n",
    "\n",
    "model = timm.create_model(model_name, pretrained=True)\n",
    "\n",
    "# Define the maximum drop path rate\n",
    "max_drop_path_rate = 0.3\n",
    "dropout_rate = 0.1\n",
    "\n",
    "drop_path_rates = [x.item() for x in torch.linspace(0, max_drop_path_rate, len(model.blocks))]\n",
    "\n",
    "# Assign drop path rates\n",
    "for i, block in enumerate(model.blocks):\n",
    "    block.drop_path1 = DropPath(drop_prob=drop_path_rates[i])\n",
    "    block.drop_path2 = DropPath(drop_prob=drop_path_rates[i])\n",
    "    block.attn.attn_drop = nn.Dropout(p=dropout_rate, inplace=False)\n",
    "    block.attn.proj_drop = nn.Dropout(p=dropout_rate, inplace=False)\n",
    "    block.mlp.drop1 = nn.Dropout(p=dropout_rate, inplace=False)\n",
    "    block.mlp.drop2 = nn.Dropout(p=dropout_rate, inplace=False)\n",
    "\n",
    "model.head = nn.Linear(model.head.in_features, 3)\n",
    "\n",
    "# state_dict = torch.load('vit_models_1/epoch_249_step_3500.pth', map_location=device)\n",
    "# model.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Initialize EMA model\n",
    "ema_decay = 0.999  # decay factor for EMA\n",
    "ema_model = copy.deepcopy(model)\n",
    "ema_model = ema_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9613dea-4edf-47cd-a042-5f3468b19d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "\n",
    "integer_to_label = {\n",
    "    0: 'tumor',\n",
    "    1: 'stroma',\n",
    "    2: 'necrosis',\n",
    "}\n",
    "\n",
    "label_to_integer = {\n",
    "    'tumor': 0,\n",
    "    'stroma': 1,\n",
    "    'necrosis': 2,\n",
    "}\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.transform = transform\n",
    "        self.all_images = []\n",
    "\n",
    "        # Step 1: Collect all images from each folder\n",
    "        for index, row in dataframe.iterrows():\n",
    "            image_file = row['image_path']\n",
    "            label = row['mask_label']\n",
    "            self.all_images.append((image_file, label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path, label = self.all_images[idx]\n",
    "        image = Image.open(image_path)\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label_to_integer[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fb8bd5c-47c2-4e2e-8182-55d17f1c8a43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_path</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mask_label</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>necrosis</th>\n",
       "      <td>505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stroma</th>\n",
       "      <td>2438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tumor</th>\n",
       "      <td>39464</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            image_path\n",
       "mask_label            \n",
       "necrosis           505\n",
       "stroma            2438\n",
       "tumor            39464"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.groupby('mask_label').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "daab3db2-8fff-4b14-83f9-608c256dea11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.1),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.RandomAffine(degrees=45, translate=(0.25, 0.25), scale=(1, 2), shear=(-30, 30, -30, 30)),\n",
    "    transforms.Resize(448),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[\n",
    "        0.48145466,\n",
    "        0.4578275,\n",
    "        0.40821073\n",
    "    ], std=[\n",
    "        0.26862954,\n",
    "        0.26130258,\n",
    "        0.27577711\n",
    "    ]),\n",
    "    transforms.RandomErasing(p=0.5, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=0, inplace=False),\n",
    "    transforms.RandomErasing(p=0.5, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=0, inplace=False),\n",
    "    transforms.RandomErasing(p=0.5, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=0, inplace=False),\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize(448),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[\n",
    "        0.48145466,\n",
    "        0.4578275,\n",
    "        0.40821073\n",
    "    ], std=[\n",
    "        0.26862954,\n",
    "        0.26130258,\n",
    "        0.27577711\n",
    "    ]),\n",
    "])\n",
    "\n",
    "train_dataset = ImageDataset(dataframe=train, transform=train_transform)\n",
    "\n",
    "# Calculate weights for each class\n",
    "class_counts = [39464, 2438, 505]  # Example class counts\n",
    "num_samples = sum(class_counts)\n",
    "class_weights = [num_samples / class_count for class_count in class_counts]\n",
    "\n",
    "# Assign a weight to each sample in the dataset based on its class\n",
    "sample_weights = [class_weights[label_to_integer[label]] for _, label in train_dataset.all_images]\n",
    "\n",
    "# Create WeightedRandomSampler\n",
    "sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)\n",
    "\n",
    "# DataLoader with WeightedRandomSampler\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=sampler, num_workers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0aa0e2bd-4c21-48a1-a0fe-5eb15fb556b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "# Get the root logger\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# Optional: Remove all existing handlers from the logger\n",
    "for handler in logger.handlers[:]:\n",
    "    logger.removeHandler(handler)\n",
    "\n",
    "# Set the logging level\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Create a FileHandler and add it to the logger\n",
    "file_handler = logging.FileHandler(f'logs/{MODEL_NAME}.txt')\n",
    "file_handler.setLevel(logging.INFO)\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "# Create a StreamHandler for stderr and add it to the logger\n",
    "stream_handler = logging.StreamHandler(sys.stderr)\n",
    "stream_handler.setLevel(logging.ERROR)  # Only log ERROR and CRITICAL messages to stderr\n",
    "logger.addHandler(stream_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08f9b938-9860-4147-be7e-f6f36938f212",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import logging\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "import random\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "initial_lr = 1e-5\n",
    "final_lr = 1e-6\n",
    "num_epochs = 10000\n",
    "\n",
    "# Function for linear warmup\n",
    "def learning_rate(step, warmup_steps=2000, max_steps=20000):\n",
    "    if step < warmup_steps:\n",
    "        return initial_lr * (float(step) / float(max(1, warmup_steps)))\n",
    "    elif step < max_steps:\n",
    "        progress = (float(step - warmup_steps) / float(max(1, max_steps - warmup_steps)))\n",
    "        cos_component = 0.5 * (1 + math.cos(math.pi * progress))\n",
    "        return final_lr + (initial_lr - final_lr) * cos_component\n",
    "    else:\n",
    "        return final_lr\n",
    "\n",
    "# Function to calculate weighted accuracy\n",
    "def weighted_accuracy(true_labels, predictions, class_weights):\n",
    "    correct = 0\n",
    "    total_weight = 0\n",
    "\n",
    "    for label, pred in zip(true_labels, predictions):\n",
    "        if label == pred:\n",
    "            correct += class_weights[label]\n",
    "        total_weight += class_weights[label]\n",
    "\n",
    "    return correct / total_weight\n",
    "\n",
    "def update_ema_variables(model, ema_model, alpha, global_step):\n",
    "    # Update the EMA model parameters\n",
    "    with torch.no_grad():\n",
    "        for ema_param, param in zip(ema_model.parameters(), model.parameters()):\n",
    "            ema_param.data.mul_(alpha).add_(param.data, alpha=1 - alpha)\n",
    "\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.AdamW(model.parameters(), lr=initial_lr, weight_decay=5e-2)\n",
    "\n",
    "# # Calculate class weights\n",
    "# class_counts = np.array([39464, 2438, 505], dtype=np.float32)\n",
    "# class_weights = 1. / class_counts\n",
    "# class_weights /= class_weights.sum()\n",
    "\n",
    "# # Convert class weights to tensor\n",
    "# class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "# Define the loss function with class weights\n",
    "criterion = torch.nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "\n",
    "best_val_accuracy = 0.0\n",
    "step = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # set the model to training mode\n",
    "    \n",
    "    for i, (images, labels) in enumerate(train_dataloader, 0):\n",
    "        # Convert images to PIL format\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Linearly increase the learning rate\n",
    "        lr = learning_rate(step)\n",
    "        for g in optimizer.param_groups:\n",
    "            # g['lr'] = g['lr'] * lr / initial_lr\n",
    "            g['lr'] = lr\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass with autocast\n",
    "        with autocast():\n",
    "            outputs = model(images)\n",
    "            logits_per_image = outputs\n",
    "            loss = criterion(logits_per_image, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        update_ema_variables(model, ema_model, ema_decay, step)\n",
    "\n",
    "        logging.info('[%d, %5d] loss: %.3f' % (epoch + 1, step, loss.item()))\n",
    "\n",
    "        if step % 2000 == 0:\n",
    "            ema_model.eval()\n",
    "            torch.save(ema_model.state_dict(), f'eva02_base_models/{MODEL_NAME}/epoch_{epoch}_step_{step}.pth')\n",
    "            logging.info(f'Model saved after epoch {epoch} and step {step}')\\\n",
    "\n",
    "            model.train()\n",
    "\n",
    "        if step == 20000:\n",
    "            torch.save(ema_model.state_dict(), f'eva02_base_models/{MODEL_NAME}/final.pth')\n",
    "\n",
    "        step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f148c4f-3aec-4956-ad4e-6d0f4618f5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ema_model.eval()\n",
    "\n",
    "tma_preds = []\n",
    "tma_labels = []\n",
    "\n",
    "non_tma_preds = []\n",
    "non_tma_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for _, row in validation.iterrows():\n",
    "        path = row['tile_path']\n",
    "        all_files = [f for f in os.listdir(path) if f.lower().endswith('.png')]\n",
    "\n",
    "        sum_probabilities = torch.zeros(5).to(device)\n",
    "        sum_log_probabilities = torch.zeros(5).to(device)\n",
    "        sum_log_neg_probabilities = torch.zeros(5).to(device)\n",
    "\n",
    "        # Prepare a list to hold image tiles\n",
    "        batch_tiles = []\n",
    "\n",
    "        sample_size = min(16, len(all_files))\n",
    "        sampled_files = random.sample(all_files, sample_size)\n",
    "\n",
    "        for image_name in sampled_files:\n",
    "            image_path = os.path.join(path, image_name)\n",
    "            sub_image = Image.open(image_path)\n",
    "\n",
    "            tile = val_transform(sub_image).unsqueeze(0).to(device)\n",
    "            batch_tiles.append(tile)\n",
    "\n",
    "        outputs = ema_model(torch.concat(batch_tiles, dim=0))\n",
    "        probs = outputs.softmax(dim=1)\n",
    "        sum_probabilities += probs.sum(dim=0)\n",
    "        sum_log_probabilities += torch.log(probs).sum(dim=0)\n",
    "        sum_log_neg_probabilities += torch.log(1 - probs).sum(dim=0)\n",
    "        if sum_probabilities.argmax().detach().cpu().item() != (-sum_log_neg_probabilities).argmax().detach().cpu().item():\n",
    "            print(sum_probabilities / 16, sum_log_probabilities / 16, sum_log_neg_probabilities / 16, row)\n",
    "\n",
    "        pred_label = integer_to_label[sum_probabilities.argmax().detach().cpu().item()]\n",
    "        label = row['label']\n",
    "        print(pred_label, label)\n",
    "        if row['is_tma']:\n",
    "            tma_preds.append(pred_label)\n",
    "            tma_labels.append(label)\n",
    "        else:\n",
    "            non_tma_preds.append(pred_label)\n",
    "            non_tma_labels.append(label)\n",
    "\n",
    "tma_accuracy = balanced_accuracy_score(tma_labels, tma_preds)\n",
    "non_tma_accuracy = balanced_accuracy_score(non_tma_labels, non_tma_preds)\n",
    "accuracy = (tma_accuracy + non_tma_accuracy) / 2\n",
    "print(f'TMA Accuracy: {tma_accuracy} | Non-TMA Accuracy: {non_tma_accuracy} | Overall Accuracy: {accuracy}')"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.2-0.m112",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.2-0:m112"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
