{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72d6e1d8-b7d3-4d68-9d35-d23a64c12587",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>label</th>\n",
       "      <th>image_width</th>\n",
       "      <th>image_height</th>\n",
       "      <th>is_tma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>HGSC</td>\n",
       "      <td>23785</td>\n",
       "      <td>20008</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>66</td>\n",
       "      <td>LGSC</td>\n",
       "      <td>48871</td>\n",
       "      <td>48195</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>91</td>\n",
       "      <td>HGSC</td>\n",
       "      <td>3388</td>\n",
       "      <td>3388</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>281</td>\n",
       "      <td>LGSC</td>\n",
       "      <td>42309</td>\n",
       "      <td>15545</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>286</td>\n",
       "      <td>EC</td>\n",
       "      <td>37204</td>\n",
       "      <td>30020</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   image_id label  image_width  image_height  is_tma\n",
       "0         4  HGSC        23785         20008   False\n",
       "1        66  LGSC        48871         48195   False\n",
       "2        91  HGSC         3388          3388    True\n",
       "3       281  LGSC        42309         15545   False\n",
       "4       286    EC        37204         30020   False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# train = pd.read_csv(\"train-yes-tma.csv\")\n",
    "# validation = pd.read_csv(\"validation-yes-tma.csv\")\n",
    "train = pd.read_csv(\"data/train.csv\")\n",
    "validation = pd.read_csv(\"data/train.csv\")\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "806316f9-a0d8-4f7d-b373-512dfa16a854",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>label</th>\n",
       "      <th>image_width</th>\n",
       "      <th>image_height</th>\n",
       "      <th>is_tma</th>\n",
       "      <th>tile_path_0</th>\n",
       "      <th>tile_path_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>HGSC</td>\n",
       "      <td>23785</td>\n",
       "      <td>20008</td>\n",
       "      <td>False</td>\n",
       "      <td>tiles/4</td>\n",
       "      <td>tiles_2964/4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>66</td>\n",
       "      <td>LGSC</td>\n",
       "      <td>48871</td>\n",
       "      <td>48195</td>\n",
       "      <td>False</td>\n",
       "      <td>tiles/66</td>\n",
       "      <td>tiles_2964/66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>91</td>\n",
       "      <td>HGSC</td>\n",
       "      <td>3388</td>\n",
       "      <td>3388</td>\n",
       "      <td>True</td>\n",
       "      <td>tiles/91</td>\n",
       "      <td>tiles_2964/91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>281</td>\n",
       "      <td>LGSC</td>\n",
       "      <td>42309</td>\n",
       "      <td>15545</td>\n",
       "      <td>False</td>\n",
       "      <td>tiles/281</td>\n",
       "      <td>tiles_2964/281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>286</td>\n",
       "      <td>EC</td>\n",
       "      <td>37204</td>\n",
       "      <td>30020</td>\n",
       "      <td>False</td>\n",
       "      <td>tiles/286</td>\n",
       "      <td>tiles_2964/286</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   image_id label  image_width  image_height  is_tma tile_path_0  \\\n",
       "0         4  HGSC        23785         20008   False     tiles/4   \n",
       "1        66  LGSC        48871         48195   False    tiles/66   \n",
       "2        91  HGSC         3388          3388    True    tiles/91   \n",
       "3       281  LGSC        42309         15545   False   tiles/281   \n",
       "4       286    EC        37204         30020   False   tiles/286   \n",
       "\n",
       "      tile_path_1  \n",
       "0    tiles_2964/4  \n",
       "1   tiles_2964/66  \n",
       "2   tiles_2964/91  \n",
       "3  tiles_2964/281  \n",
       "4  tiles_2964/286  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def get_image_path(image_id:int, directory:str):\n",
    "    return os.path.join(directory, str(image_id))\n",
    "\n",
    "train['tile_path_0'] = train['image_id'].apply(lambda x: get_image_path(x, 'tiles'))\n",
    "validation['tile_path_0'] = validation['image_id'].apply(lambda x: get_image_path(x, 'tiles'))\n",
    "train['tile_path_1'] = train['image_id'].apply(lambda x: get_image_path(x, 'tiles_2964'))\n",
    "validation['tile_path_1'] = validation['image_id'].apply(lambda x: get_image_path(x, 'tiles_2964'))\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5427a934-ac9e-4f6f-8df8-d26feb0c668b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda and model google/vit-base-patch16-224\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import ViTImageProcessor, ViTForImageClassification\n",
    "\n",
    "N_MODELS = 2\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_name = \"google/vit-base-patch16-224\"\n",
    "print(f\"Using device {device} and model {model_name}\")\n",
    "\n",
    "processor = ViTImageProcessor.from_pretrained(model_name)\n",
    "models = [ViTForImageClassification.from_pretrained(model_name) for _ in range(N_MODELS)]\n",
    "classifier = nn.Linear(models[0].classifier.in_features * N_MODELS, 5)\n",
    "\n",
    "# model.classifier = nn.Linear(model.classifier.in_features, 5)\n",
    "for model in models:\n",
    "    model.classifier = nn.Identity()\n",
    "    model = model.to(device)\n",
    "\n",
    "classifier = classifier.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "29637b27-0132-4f5c-afcb-a89879d94ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# epoch = 780\n",
    "# step = 50000\n",
    "\n",
    "# models = [ViTForImageClassification.from_pretrained(model_name) for _ in range(N_MODELS)]\n",
    "# classifier = nn.Linear(models[0].classifier.in_features * N_MODELS, 5)\n",
    "# for i_model, model in enumerate(models):\n",
    "#     model.classifier = nn.Identity()\n",
    "#     model = model.to(device)\n",
    "\n",
    "#     state_dict = torch.load(f'vit-finetune-big-and-small-models-pt-3/model_{i_model}_epoch_{epoch}_step_{step}.pth', map_location=device)\n",
    "#     model.load_state_dict(state_dict)\n",
    "\n",
    "# state_dict = torch.load(f'vit-finetune-big-and-small-models-pt-3/classifier_epoch_{epoch}_step_{step}.pth', map_location=device)\n",
    "# classifier.load_state_dict(state_dict)\n",
    "# classifier = classifier.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d347ec39-862e-49df-8c07-37c7c5f189d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "from PIL import Image, ImageOps\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "integer_to_label = {\n",
    "    0: 'HGSC',\n",
    "    1: 'CC',\n",
    "    2: 'EC',\n",
    "    3: 'LGSC',\n",
    "    4: 'MC',\n",
    "}\n",
    "\n",
    "label_to_integer = {\n",
    "    'HGSC': 0,\n",
    "    'CC': 1,\n",
    "    'EC': 2,\n",
    "    'LGSC': 3,\n",
    "    'MC': 4,\n",
    "}\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.transform = transform\n",
    "        self.image_info = []\n",
    "        for index, row in dataframe.iterrows():\n",
    "            image_info = (row['label'], [])\n",
    "            for i_model in range(N_MODELS):\n",
    "                image_info[1].append(row[f'tile_path_{i_model}'])\n",
    "            self.image_info.append(image_info)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_info)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label, tile_paths = self.image_info[idx]\n",
    "        image_paths = []\n",
    "        for tile_path in tile_paths:\n",
    "            if os.path.isdir(tile_path):\n",
    "                image_names = [img for img in os.listdir(tile_path) if img.lower().endswith('.png')]\n",
    "                if len(image_names) == 1:\n",
    "                    selected_images = image_names * 2\n",
    "                else:\n",
    "                    selected_images = random.sample(image_names, 2)\n",
    "                image_paths.extend(os.path.join(tile_path, img) for img in selected_images)\n",
    "        images = [self.transform(Image.open(path)) if self.transform else Image.open(path) for path in image_paths]\n",
    "        return images, label_to_integer[label]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d250491-bb63-4740-9481-40fc06ad04f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation((-45, 45)),\n",
    "    transforms.RandomResizedCrop((224, 224), scale=(0.4, 1)),\n",
    "    transforms.RandAugment(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "train_dataset = ImageDataset(dataframe=train, transform=train_transform)\n",
    "val_dataset = ImageDataset(dataframe=validation, transform=val_transform)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, num_workers=3, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, num_workers=3, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01a6c212-387c-4dd8-b6e8-68cf33fa02c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "# Get the root logger\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# Optional: Remove all existing handlers from the logger\n",
    "for handler in logger.handlers[:]:\n",
    "    logger.removeHandler(handler)\n",
    "\n",
    "# Set the logging level\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Create a FileHandler and add it to the logger\n",
    "file_handler = logging.FileHandler('training_log_finetune_vit_non_tma_big_and_small_pt_5.txt')\n",
    "file_handler.setLevel(logging.INFO)\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "# Create a StreamHandler for stderr and add it to the logger\n",
    "stream_handler = logging.StreamHandler(sys.stderr)\n",
    "stream_handler.setLevel(logging.ERROR)  # Only log ERROR and CRITICAL messages to stderr\n",
    "logger.addHandler(stream_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecefa89-5445-4e6b-ad12-58e4990c7e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import logging\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "initial_lr = 1e-5\n",
    "num_epochs = 1000\n",
    "\n",
    "# Function for linear warmup\n",
    "def warmup_linear(step, warmup_steps=1000):\n",
    "    if step < warmup_steps:\n",
    "        return float(step) / float(max(1, warmup_steps))\n",
    "    progress = float(step - warmup_steps) / float(max(1, 20000 - warmup_steps))\n",
    "    return 0.5 * (1.0 + math.cos(math.pi * progress))\n",
    "\n",
    "all_parameters = []\n",
    "for model in models:\n",
    "    all_parameters += list(model.parameters())\n",
    "all_parameters += list(classifier.parameters())\n",
    "\n",
    "optimizer = optim.Adam(all_parameters, lr=initial_lr, weight_decay=1e-2)\n",
    "\n",
    "scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda step: warmup_linear(step))\n",
    "\n",
    "# Calculate class weights\n",
    "# class_counts = np.array([216, 93, 118, 41, 40], dtype=np.float32)\n",
    "class_counts = np.array([222, 99, 124, 47, 46], dtype=np.float32)\n",
    "class_weights = 1. / class_counts\n",
    "class_weights /= class_weights.sum()\n",
    "\n",
    "# Convert class weights to tensor\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "# Define the loss function with class weights\n",
    "criterion = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "best_val_accuracy = 0.0\n",
    "step = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for model in models:\n",
    "        model.train()  # set the model to training mode\n",
    "    classifier.train()\n",
    "    \n",
    "    for i, (images, labels) in enumerate(train_dataloader, 0):\n",
    "        \n",
    "        if step == 20000:\n",
    "            break\n",
    "        \n",
    "        # Calculate the size of each chunk\n",
    "        chunk_size = len(images) // N_MODELS\n",
    "\n",
    "        # Initialize a list to hold the stacked tensors\n",
    "        stacked_chunks = []\n",
    "\n",
    "        # Split the images into chunks and stack them\n",
    "        for i in range(0, len(images), chunk_size):\n",
    "            # Ensure that we don't go out of bounds on the last chunk\n",
    "            chunk = images[i:i + chunk_size]\n",
    "            # Stack the chunk along a new dimension\n",
    "            stacked_chunk = torch.stack(chunk).to(device)\n",
    "            stacked_chunks.append(stacked_chunk)\n",
    "\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Linearly increase the learning rate\n",
    "        lr_scale = warmup_linear(step)\n",
    "        for g in optimizer.param_groups:\n",
    "            g['lr'] = lr_scale * initial_lr\n",
    "            \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        all_outputs = []\n",
    "\n",
    "        # Forward pass\n",
    "        for i_model in range(N_MODELS):\n",
    "            # inputs = processor(images=stacked_chunks[i_model].view(-1, 3, 224, 224), return_tensors=\"pt\", do_rescale=False)\n",
    "            # for key in inputs.keys():\n",
    "            #     inputs[key] = inputs[key].to(device)\n",
    "\n",
    "            # outputs = models[i_model](**inputs).logits.view(2, -1, 768)\n",
    "            outputs = models[i_model](stacked_chunks[i_model].view(-1, 3, 224, 224)).logits.view(2, -1, 768)\n",
    "            outputs = outputs.mean(dim=0).view(-1, 768)\n",
    "            all_outputs.append(outputs)\n",
    "\n",
    "        all_outputs = torch.cat(all_outputs, dim=1)\n",
    "        all_outputs = classifier(all_outputs)\n",
    "        \n",
    "        loss = criterion(all_outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        step += 1\n",
    "\n",
    "        logging.info('[%d, %5d] loss: %.3f' % (epoch + 1, step, loss.item()))\n",
    "        \n",
    "        # Validation accuracy \n",
    "        if step % 1000 == 0:\n",
    "            for model in models:\n",
    "                model.eval()  # set the model to eval mode\n",
    "            classifier.eval()\n",
    "\n",
    "            all_labels = []\n",
    "            all_preds = []\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for _ in range(100):\n",
    "                    for images, labels in val_dataloader:\n",
    "                        # Calculate the size of each chunk\n",
    "                        chunk_size = len(images) // N_MODELS\n",
    "\n",
    "                        # Initialize a list to hold the stacked tensors\n",
    "                        stacked_chunks = []\n",
    "\n",
    "                        # Split the images into chunks and stack them\n",
    "                        for i in range(0, len(images), chunk_size):\n",
    "                            # Ensure that we don't go out of bounds on the last chunk\n",
    "                            chunk = images[i:i + chunk_size]\n",
    "                            # Stack the chunk along a new dimension\n",
    "                            stacked_chunk = torch.stack(chunk).to(device)\n",
    "                            stacked_chunks.append(stacked_chunk)\n",
    "\n",
    "                        labels = labels.numpy()  # Convert labels to numpy array for later use in accuracy calculation\n",
    "\n",
    "                        all_outputs = []\n",
    "\n",
    "                        # Forward pass\n",
    "                        for i_model in range(N_MODELS):\n",
    "#                             inputs = processor(images=stacked_chunks[i_model].view(-1, 3, 224, 224), return_tensors=\"pt\", do_rescale=False)\n",
    "#                             for key in inputs.keys():\n",
    "#                                 inputs[key] = inputs[key].to(device)\n",
    "\n",
    "#                             outputs = models[i_model](**inputs).logits\n",
    "                            outputs = models[i_model](stacked_chunks[i_model].view(-1, 3, 224, 224)).logits\n",
    "                            outputs = outputs.view(2, -1, 768)\n",
    "                            outputs = outputs.mean(dim=0).view(-1, 768)\n",
    "                            all_outputs.append(outputs)\n",
    "\n",
    "                        all_outputs = torch.cat(all_outputs, dim=1)\n",
    "                        all_outputs = classifier(all_outputs)\n",
    "                        probs = all_outputs.softmax(dim=1)\n",
    "\n",
    "                        # Get predicted labels\n",
    "                        preds = torch.argmax(probs, dim=1).cpu().numpy()\n",
    "\n",
    "                        # Store predictions and labels\n",
    "                        all_preds.extend(preds)\n",
    "                        all_labels.extend(labels)\n",
    "\n",
    "                        if len(all_preds) > 100:\n",
    "                            break\n",
    "        \n",
    "            # Calculate accuracy\n",
    "            accuracy = accuracy_score(all_labels, all_preds)\n",
    "            logging.info(\"Validation Accuracy: %s\" % accuracy)\n",
    "            \n",
    "            for model in models:\n",
    "                model.train()  # set the model to training mode\n",
    "            classifier.train()\n",
    "\n",
    "        if step % 1000 == 0:\n",
    "            # Assuming 'model' is defined\n",
    "            for i_model, model in enumerate(models):\n",
    "                torch.save(model.state_dict(), f'vit-finetune-big-and-small-models-pt-5/model_{i_model}_epoch_{epoch}_step_{step}.pth')\n",
    "            torch.save(classifier.state_dict(), f'vit-finetune-big-and-small-models-pt-5/classifier_epoch_{epoch}_step_{step}.pth')\n",
    "\n",
    "    # # Save model after each epoch\n",
    "    # torch.save(model.state_dict(), f'vit-finetune-big-and-small-models/model_epoch_{epoch+1}.pth')\n",
    "    # logging.info(f'Model saved after epoch {epoch+1}')\n",
    "\n",
    "logging.info('Finished Training')\n"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.2-0.m112",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.2-0:m112"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
