{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "130095c9-7341-4a68-9468-55180d2dd817",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'cutmix_mixup_final_try_all_the_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82500098-49ea-4a7e-bb63-ce5130f954cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>label</th>\n",
       "      <th>image_width</th>\n",
       "      <th>image_height</th>\n",
       "      <th>is_tma</th>\n",
       "      <th>tile_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>HGSC</td>\n",
       "      <td>23785</td>\n",
       "      <td>20008</td>\n",
       "      <td>False</td>\n",
       "      <td>../tiles_768/4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>66</td>\n",
       "      <td>LGSC</td>\n",
       "      <td>48871</td>\n",
       "      <td>48195</td>\n",
       "      <td>False</td>\n",
       "      <td>../tiles_768/66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>91</td>\n",
       "      <td>HGSC</td>\n",
       "      <td>3388</td>\n",
       "      <td>3388</td>\n",
       "      <td>True</td>\n",
       "      <td>../tiles_768/91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>281</td>\n",
       "      <td>LGSC</td>\n",
       "      <td>42309</td>\n",
       "      <td>15545</td>\n",
       "      <td>False</td>\n",
       "      <td>../tiles_768/281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>286</td>\n",
       "      <td>EC</td>\n",
       "      <td>37204</td>\n",
       "      <td>30020</td>\n",
       "      <td>False</td>\n",
       "      <td>../tiles_768/286</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   image_id label  image_width  image_height  is_tma         tile_path\n",
       "0         4  HGSC        23785         20008   False    ../tiles_768/4\n",
       "1        66  LGSC        48871         48195   False   ../tiles_768/66\n",
       "2        91  HGSC         3388          3388    True   ../tiles_768/91\n",
       "3       281  LGSC        42309         15545   False  ../tiles_768/281\n",
       "4       286    EC        37204         30020   False  ../tiles_768/286"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def get_image_path(image_id:int):\n",
    "    return os.path.join('../tiles_768', str(image_id))\n",
    "\n",
    "train = pd.read_csv(f\"../data/train.csv\")\n",
    "\n",
    "train['tile_path'] = train['image_id'].apply(lambda x: get_image_path(x))\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d782638-2c6c-4b63-87b3-c7bf291bbde4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda and model timm/eva02_base_patch14_448.mim_in22k_ft_in22k_in1k\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import timm\n",
    "from timm.models.layers import DropPath\n",
    "import copy\n",
    "from itertools import cycle\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_name = \"timm/eva02_base_patch14_448.mim_in22k_ft_in22k_in1k\"\n",
    "\n",
    "print(f\"Using device {device} and model {model_name}\")\n",
    "\n",
    "model = timm.create_model(model_name, pretrained=True)\n",
    "\n",
    "# 25000 cutmix_mixup was 0.3 and 0.1\n",
    "drop_path_rate = 0.5\n",
    "dropout_rate = 0.\n",
    "head_dropout_rate = 0.3\n",
    "drop_path_rates = [x.item() for x in torch.linspace(0, drop_path_rate, len(model.blocks))]\n",
    "\n",
    "# Assign drop path rates\n",
    "for i, block in enumerate(model.blocks):\n",
    "    block.drop_path1 = DropPath(drop_prob=drop_path_rates[i])\n",
    "    block.drop_path2 = DropPath(drop_prob=drop_path_rates[i])\n",
    "    block.attn.attn_drop = nn.Dropout(p=dropout_rate, inplace=False)\n",
    "    block.attn.proj_drop = nn.Dropout(p=dropout_rate, inplace=False)\n",
    "    block.mlp.drop1 = nn.Dropout(p=dropout_rate, inplace=False)\n",
    "    block.mlp.drop2 = nn.Dropout(p=dropout_rate, inplace=False)\n",
    "\n",
    "model.head = nn.Linear(model.head.in_features, 5)\n",
    "model.pos_drop = nn.Dropout(dropout_rate)\n",
    "model.head_drop = nn.Dropout(head_dropout_rate)\n",
    "\n",
    "model_location = 'eva02_base_models_cutmix_mixup_final_try_all_the_data/ema_0.999_step_54000.pth'\n",
    "state_dict = torch.load(model_location, map_location=device)\n",
    "model.load_state_dict(state_dict, strict=False)\n",
    "model = model.to(device)\n",
    "\n",
    "# Initialize EMA model\n",
    "ema_decays = [0.999, 0.9995, 0.9998, 0.9999, 0.99995, 0.99998, 0.99999]\n",
    "ema_models = [copy.deepcopy(model) for _ in range(len(ema_decays))]\n",
    "for i_ema, ema_model in enumerate(ema_models):\n",
    "    model_location = f'eva02_base_models_cutmix_mixup_final_try_all_the_data/ema_{ema_decays[i_ema]}_step_54000.pth'\n",
    "    state_dict = torch.load(model_location, map_location=device)\n",
    "    ema_model.load_state_dict(state_dict, strict=False)\n",
    "    ema_model = ema_model.to(device)\n",
    "    ema_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82668844-c38f-42a1-8ed9-17098321ad5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "integer_to_label = {\n",
    "    0: 'HGSC',\n",
    "    1: 'CC',\n",
    "    2: 'EC',\n",
    "    3: 'LGSC',\n",
    "    4: 'MC',\n",
    "}\n",
    "\n",
    "label_to_integer = {\n",
    "    'HGSC': 0,\n",
    "    'CC': 1,\n",
    "    'EC': 2,\n",
    "    'LGSC': 3,\n",
    "    'MC': 4,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9613dea-4edf-47cd-a042-5f3468b19d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "    \n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.transform = transform\n",
    "        self.all_images = []  # Store all images in an interlaced fashion\n",
    "        self.label_images = [[] for _ in range(5)]  # Temporary storage for images from each folder\n",
    "\n",
    "        # Step 1: Collect all images from each folder\n",
    "        for index, row in dataframe.iterrows():\n",
    "            folder_path = row['tile_path']\n",
    "            label = row['label']\n",
    "            image_id = row['image_id']\n",
    "            if os.path.isdir(folder_path):\n",
    "                image_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.lower().endswith('.png')]\n",
    "                random.shuffle(image_files)\n",
    "                self.all_images.extend([(image_file, label, image_id) for image_file in image_files])\n",
    "        \n",
    "        random.shuffle(self.all_images)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path, label, image_id = self.all_images[idx]\n",
    "        image = Image.open(image_path)\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label_to_integer[label], image_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "daab3db2-8fff-4b14-83f9-608c256dea11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.v2 as v2\n",
    "from torch.utils.data import default_collate\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(size=448, scale=(0.5, 1.0), ratio=(0.75, 1.33)),\n",
    "    transforms.RandAugment(9, 15, 31),\n",
    "    transforms.Resize(448),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[\n",
    "        0.48145466,\n",
    "        0.4578275,\n",
    "        0.40821073\n",
    "    ], std=[\n",
    "        0.26862954,\n",
    "        0.26130258,\n",
    "        0.27577711\n",
    "    ]),\n",
    "    transforms.RandomErasing(p=0.5, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=0, inplace=False),\n",
    "    transforms.RandomErasing(p=0.5, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=0, inplace=False),\n",
    "    transforms.RandomErasing(p=0.5, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=0, inplace=False),\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize(448),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[\n",
    "        0.48145466,\n",
    "        0.4578275,\n",
    "        0.40821073\n",
    "    ], std=[\n",
    "        0.26862954,\n",
    "        0.26130258,\n",
    "        0.27577711\n",
    "    ]),\n",
    "])\n",
    "\n",
    "train_dataset = ImageDataset(dataframe=train, transform=train_transform)\n",
    "\n",
    "cutmix = v2.CutMix(num_classes=5)\n",
    "mixup = v2.MixUp(num_classes=5)\n",
    "cutmix_or_mixup = v2.RandomChoice([cutmix, mixup])\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return cutmix_or_mixup(*default_collate(batch))\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, num_workers=7, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0aa0e2bd-4c21-48a1-a0fe-5eb15fb556b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "# Get the root logger\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# Optional: Remove all existing handlers from the logger\n",
    "for handler in logger.handlers[:]:\n",
    "    logger.removeHandler(handler)\n",
    "\n",
    "# Set the logging level\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Create a FileHandler and add it to the logger\n",
    "file_handler = logging.FileHandler(f'logs/eva02_base_train_{MODEL_NAME}.txt')\n",
    "file_handler.setLevel(logging.INFO)\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "# Create a StreamHandler for stderr and add it to the logger\n",
    "stream_handler = logging.StreamHandler(sys.stderr)\n",
    "stream_handler.setLevel(logging.ERROR)  # Only log ERROR and CRITICAL messages to stderr\n",
    "logger.addHandler(stream_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f9b938-9860-4147-be7e-f6f36938f212",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import logging\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "import random\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "initial_lr = 1e-5\n",
    "final_lr = 1e-7\n",
    "num_epochs = 10000\n",
    "\n",
    "# Function for linear warmup\n",
    "def learning_rate(step, warmup_steps=5000, max_steps=200000):\n",
    "    if step < warmup_steps:\n",
    "        return initial_lr * (float(step) / float(max(1, warmup_steps)))\n",
    "    elif step < max_steps:\n",
    "        progress = (float(step - warmup_steps) / float(max(1, max_steps - warmup_steps)))\n",
    "        cos_component = 0.5 * (1 + math.cos(math.pi * progress))\n",
    "        return final_lr + (initial_lr - final_lr) * cos_component\n",
    "    else:\n",
    "        return final_lr\n",
    "\n",
    "def update_ema_variables(model, ema_model, alpha, global_step):\n",
    "    # Update the EMA model parameters\n",
    "    with torch.no_grad():\n",
    "        for ema_param, param in zip(ema_model.parameters(), model.parameters()):\n",
    "            ema_param.data.mul_(alpha).add_(param.data, alpha=1 - alpha)\n",
    "\n",
    "scaler = GradScaler()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=initial_lr, weight_decay=1e-7)\n",
    "\n",
    "# Class counts: {'CC': 146576, 'EC': 165037, 'HGSC': 272477, 'LGSC': 46346, 'MC': 79036}\n",
    "class_counts = torch.tensor([272477, 146576, 165037, 46346, 79036], dtype=torch.float32)\n",
    "\n",
    "# Calculate weights: Inverse of class frequencies\n",
    "weights = 1.0 / class_counts\n",
    "weights = weights / weights.sum()  # Normalize to make the sum of weights equal to 1\n",
    "weights = weights.to(device)  # Move weights to the device (CPU/GPU)\n",
    "\n",
    "# Define the weighted loss function\n",
    "criterion = torch.nn.CrossEntropyLoss(weight=weights)\n",
    "\n",
    "best_val_accuracy = 0.0\n",
    "step = 54001\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # set the model to training mode\n",
    "    \n",
    "    for i, (images, labels, _) in enumerate(train_dataloader, 0):\n",
    "        # Convert images to PIL format\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Linearly increase the learning rate\n",
    "        lr = learning_rate(step)\n",
    "        for g in optimizer.param_groups:\n",
    "            # g['lr'] = g['lr'] * lr / initial_lr\n",
    "            g['lr'] = lr\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass with autocast\n",
    "        with autocast():\n",
    "            outputs = model(images)\n",
    "            logits_per_image = outputs\n",
    "            loss = criterion(logits_per_image, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        for i_ema, ema_model in enumerate(ema_models):\n",
    "            update_ema_variables(model, ema_model, ema_decays[i_ema], step)\n",
    "\n",
    "        logging.info('[%d, %5d] loss: %.3f' % (epoch + 1, step, loss.item()))\n",
    "\n",
    "        if step % 1000 == 0:\n",
    "            for i_ema, ema_model in enumerate(ema_models):\n",
    "                torch.save(ema_model.state_dict(), f'eva02_base_models_{MODEL_NAME}/ema_{ema_decays[i_ema]}_step_{step}.pth')\n",
    "            logging.info(f'Models saved after epoch {epoch} and step {step}')\\\n",
    "\n",
    "            model.train()\n",
    "\n",
    "        if step == 200000:\n",
    "            break\n",
    "    \n",
    "        step += 1\n",
    "\n",
    "    if step >= 200000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02074fd9-314d-47c1-97be-b73dcdb26d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "def duplicate_ipynb_with_new_name(src_file_path, dest_dir, new_name):\n",
    "    \"\"\"\n",
    "    Duplicate an IPython notebook file to a new directory with a new file name.\n",
    "\n",
    "    Parameters:\n",
    "    src_file_path (str): The path of the source IPython notebook file.\n",
    "    dest_dir (str): The destination directory where the file will be copied.\n",
    "    new_name (str): The new file name for the duplicated notebook.\n",
    "\n",
    "    Returns:\n",
    "    str: The path of the duplicated file with the new name.\n",
    "    \"\"\"\n",
    "    # Check if the new name contains the '.ipynb' extension, add if not\n",
    "    if not new_name.endswith('.ipynb'):\n",
    "        new_name += '.ipynb'\n",
    "\n",
    "    # Creating the destination file path with the new name\n",
    "    dest_file_path = os.path.join(dest_dir, new_name)\n",
    "\n",
    "    # Copying the file to the new directory\n",
    "    shutil.copy(src_file_path, dest_file_path)\n",
    "\n",
    "    return dest_file_path\n",
    "\n",
    "src_file = \"eva02-base-finetune.ipynb\"\n",
    "dest_directory = \"notebook_history\"\n",
    "new_filename = f\"{MODEL_NAME}.ipynb\"\n",
    "duplicate_ipynb_with_new_name(src_file, dest_directory, new_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e262eef-93e9-46c1-a243-7b8ace18dbc2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd395355-ce58-405c-b6d8-bd889ca98fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "ema_model = model\n",
    "state_dict = torch.load('eva02_base_models_25000_cutmix_mixup/ema_0.9998_step_50000.pth', map_location=device)\n",
    "ema_model.load_state_dict(state_dict, strict=False)\n",
    "ema_model = ema_model.to(device)\n",
    "ema_model.eval()\n",
    "\n",
    "# Maximum number of tiles per image\n",
    "MAX_TILES_PER_IMAGE = 8\n",
    "\n",
    "# Maximum number of images per batch\n",
    "MAX_IMAGES_PER_BATCH = 8  # Adjust based on model capacity and memory constraints\n",
    "\n",
    "image_ids = []\n",
    "logits = []\n",
    "labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Temporary storage for the current batch\n",
    "    batch_tiles = []\n",
    "    batch_image_ids = []\n",
    "    batch_labels = []\n",
    "\n",
    "    for _, row in train.iterrows():\n",
    "        print(row['image_id'])\n",
    "        path = row['tile_path']\n",
    "        all_files = [f for f in os.listdir(path) if f.lower().endswith('.png')]\n",
    "        \n",
    "        # Randomly sample tiles from this image\n",
    "        sample_size = min(MAX_TILES_PER_IMAGE, len(all_files))\n",
    "        sampled_files = random.sample(all_files, sample_size)\n",
    "\n",
    "        image_tiles = []\n",
    "        for image_name in sampled_files:\n",
    "            image_path = os.path.join(path, image_name)\n",
    "            sub_image = Image.open(image_path)\n",
    "            tile = val_transform(sub_image).unsqueeze(0)\n",
    "            image_tiles.append(tile)\n",
    "\n",
    "        # Add this image's tiles to the batch\n",
    "        batch_tiles.append(torch.concat(image_tiles, dim=0))\n",
    "        batch_image_ids.append(row['image_id'])\n",
    "        batch_labels.append(row['label'])\n",
    "\n",
    "        # Process the batch if it's full or this is the last row\n",
    "        if len(batch_tiles) == MAX_IMAGES_PER_BATCH or row.equals(train.iloc[-1]):\n",
    "            # Concatenate tiles from each image in the batch\n",
    "            batch_input = torch.concat(batch_tiles, dim=0).to(device)\n",
    "\n",
    "            # Run the batch through the model\n",
    "            batch_output = ema_model(batch_input)\n",
    "\n",
    "            # Split the outputs back into per-image groups and store them\n",
    "            start = 0\n",
    "            for i, tiles in enumerate(batch_tiles):\n",
    "                end = start + tiles.shape[0]\n",
    "                logits.append(batch_output[start:end])\n",
    "                start = end\n",
    "\n",
    "            image_ids.extend(batch_image_ids)\n",
    "            labels.extend(batch_labels)\n",
    "\n",
    "            # Reset the batch\n",
    "            batch_tiles = []\n",
    "            batch_image_ids = []\n",
    "            batch_labels = []\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c55aaf6-5619-437a-9157-da54f33baa3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for image_logits in logits:\n",
    "    argmax_indices = torch.argmax(image_logits, dim=1)\n",
    "\n",
    "    frequency_counts = torch.bincount(argmax_indices, minlength=5)\n",
    "\n",
    "    max_vote_key = frequency_counts.argmax().cpu().item()\n",
    "    predictions.append(integer_to_label[max_vote_key])\n",
    "\n",
    "plurality_accuracy = balanced_accuracy_score(labels, predictions)\n",
    "plurality_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766c361f-6940-4713-a8ec-932dfee65307",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for image_logits in logits:\n",
    "    summed_logits = image_logits.sum(dim=0)\n",
    "    \n",
    "    max_vote_key = summed_logits.argmax().cpu().item()\n",
    "    predictions.append(integer_to_label[max_vote_key])\n",
    "\n",
    "logit_sum_accuracy = balanced_accuracy_score(labels, predictions)\n",
    "logit_sum_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a1c420-4be4-4ddd-8852-55062d5f829e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for image_logits in logits:\n",
    "    summed_probs = image_logits.softmax(dim=1).sum(dim=0)\n",
    "    \n",
    "    max_vote_key = summed_probs.argmax().cpu().item()\n",
    "    predictions.append(integer_to_label[max_vote_key])\n",
    "\n",
    "prob_sum_accuracy = balanced_accuracy_score(labels, predictions)\n",
    "prob_sum_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bde38a-f9a1-4f8b-8602-94c4aba2d579",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for image_logits in logits:\n",
    "    summed_log_probs = torch.log(image_logits.softmax(dim=1)).sum(dim=0)\n",
    "    \n",
    "    max_vote_key = summed_log_probs.argmax().cpu().item()\n",
    "    predictions.append(integer_to_label[max_vote_key])\n",
    "\n",
    "log_prob_sum_accuracy = balanced_accuracy_score(labels, predictions)\n",
    "log_prob_sum_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb69f74-3c0f-462a-a8a9-96bbd5c30a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for image_logits in logits:\n",
    "    summed_one_minus_log_probs = torch.log(1 - image_logits.softmax(dim=1)).sum(dim=0)\n",
    "    \n",
    "    min_vote_key = summed_one_minus_log_probs.argmin().cpu().item()\n",
    "    predictions.append(integer_to_label[min_vote_key])\n",
    "\n",
    "one_minus_log_prob_sum_accuracy = balanced_accuracy_score(labels, predictions)\n",
    "one_minus_log_prob_sum_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69f8cdc-8c52-411d-855a-c173bed2dd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for i, image_logits in enumerate(logits):\n",
    "    summed_log_probs = torch.log(image_logits.softmax(dim=1)).sum(dim=0)\n",
    "    summed_one_minus_log_probs = torch.log(1 - image_logits.softmax(dim=1)).sum(dim=0)\n",
    "    if summed_log_probs.argmax().cpu().item() != summed_one_minus_log_probs.argmin().cpu().item():\n",
    "        print(i, labels[i], integer_to_label[summed_log_probs.argmax().cpu().item()], integer_to_label[summed_one_minus_log_probs.argmin().cpu().item()], summed_log_probs, summed_one_minus_log_probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda47dbb-859b-4d3d-b7a9-93d119a1d2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def borda_count_winner(image_logits):\n",
    "    num_classes = image_logits.size(1)\n",
    "    borda_scores = defaultdict(int)\n",
    "\n",
    "    # Rank each class for each image_logits and assign Borda points\n",
    "    for logits in image_logits:\n",
    "        # Get ranks (in descending order of logits)\n",
    "        ranks = torch.argsort(logits, descending=True)\n",
    "\n",
    "        # Assign Borda points (highest rank gets num_classes - 1 points, next gets num_classes - 2, ...)\n",
    "        for rank, class_index in enumerate(ranks):\n",
    "            borda_scores[class_index.item()] += num_classes - 1 - rank\n",
    "\n",
    "    # Find the class with the highest total Borda score\n",
    "    borda_winner = max(borda_scores, key=borda_scores.get)\n",
    "    return borda_winner\n",
    "\n",
    "predictions = []\n",
    "for image_logits in logits:\n",
    "    borda_winner = borda_count_winner(image_logits)\n",
    "    predictions.append(integer_to_label[borda_winner])\n",
    "    \n",
    "borda_accuracy = balanced_accuracy_score(labels, predictions)\n",
    "borda_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96aeb972-4944-4e92-908a-be7425aef8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def instant_runoff_winner(image_logits):\n",
    "    num_voters, num_classes = image_logits.shape\n",
    "    active_candidates = set(range(num_classes))\n",
    "\n",
    "    while True:\n",
    "        # Count the first-preference votes for each candidate\n",
    "        first_pref_counts = torch.zeros(num_classes)\n",
    "        for logits in image_logits:\n",
    "            for rank in torch.argsort(logits, descending=True):\n",
    "                if rank.item() in active_candidates:\n",
    "                    first_pref_counts[rank] += 1\n",
    "                    break\n",
    "\n",
    "        # Check if any candidate has more than 50% of the votes\n",
    "        if torch.any(first_pref_counts > num_voters / 2):\n",
    "            return torch.argmax(first_pref_counts).item()\n",
    "\n",
    "        # Find the candidate with the fewest votes among active candidates\n",
    "        active_candidates_votes = first_pref_counts[list(active_candidates)]\n",
    "        min_votes, min_index = torch.min(active_candidates_votes, 0)\n",
    "        min_candidate = list(active_candidates)[min_index.item()]\n",
    "\n",
    "        # Eliminate the candidate with the fewest votes\n",
    "        active_candidates.remove(min_candidate)\n",
    "        \n",
    "predictions = []\n",
    "for image_logits in logits:\n",
    "    irv_winner = instant_runoff_winner(image_logits)\n",
    "    predictions.append(integer_to_label[irv_winner])\n",
    "    \n",
    "instant_runoff_accuracy = balanced_accuracy_score(labels, predictions)\n",
    "instant_runoff_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70c52a4-9a34-4b5d-959f-6bcda9871326",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_minimax_winner(image_logits):\n",
    "    num_voters, num_classes = image_logits.shape\n",
    "    max_regrets = torch.zeros(num_classes)\n",
    "\n",
    "    # Perform pairwise comparisons between all classes\n",
    "    for i in range(num_classes):\n",
    "        for j in range(i + 1, num_classes):\n",
    "            # Count how many voters prefer class i over class j and vice versa\n",
    "            votes_for_i = torch.sum(image_logits[:, i] > image_logits[:, j])\n",
    "            votes_for_j = num_voters - votes_for_i\n",
    "\n",
    "            # Update the maximum regret for each class\n",
    "            max_regrets[i] = max(max_regrets[i], votes_for_j)\n",
    "            max_regrets[j] = max(max_regrets[j], votes_for_i)\n",
    "\n",
    "    # The winner is the class with the smallest maximum regret\n",
    "    return torch.argmin(max_regrets).item()\n",
    "        \n",
    "predictions = []\n",
    "for image_logits in logits:\n",
    "    minimax_winner = calculate_minimax_winner(image_logits)\n",
    "    predictions.append(integer_to_label[minimax_winner])\n",
    "    \n",
    "minimax_accuracy = balanced_accuracy_score(labels, predictions)\n",
    "minimax_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd8f5a2-325d-4570-a923-79c23bb312b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def find_winner(graph, source, visited):\n",
    "    \"\"\"Helper function to find the winner in the graph.\"\"\"\n",
    "    if source not in graph:\n",
    "        return False\n",
    "\n",
    "    visited.add(source)\n",
    "    for target in graph[source]:\n",
    "        if target not in visited and find_winner(graph, target, visited):\n",
    "            return True\n",
    "    visited.remove(source)\n",
    "    return False\n",
    "\n",
    "def ranked_pairs_winner(image_logits):\n",
    "    num_voters, num_classes = image_logits.shape\n",
    "    margins = defaultdict(int)\n",
    "\n",
    "    # Perform pairwise comparisons\n",
    "    for i in range(num_classes):\n",
    "        for j in range(num_classes):\n",
    "            if i != j:\n",
    "                votes_for_i = torch.sum(image_logits[:, i] > image_logits[:, j])\n",
    "                votes_for_j = num_voters - votes_for_i\n",
    "                margins[(i, j)] = votes_for_i - votes_for_j\n",
    "\n",
    "    # Sort pairs by margin of victory\n",
    "    sorted_pairs = sorted(margins, key=margins.get, reverse=True)\n",
    "\n",
    "    # Initialize graph for locking pairs\n",
    "    graph = {i: set() for i in range(num_classes)}\n",
    "    for pair in sorted_pairs:\n",
    "        winner, loser = pair\n",
    "        graph[winner].add(loser)\n",
    "        visited = set()\n",
    "\n",
    "        # Check for cycle\n",
    "        if find_winner(graph, loser, visited):\n",
    "            graph[winner].remove(loser)\n",
    "\n",
    "    # Determine the winner\n",
    "    for i in range(num_classes):\n",
    "        if not any(i in targets for targets in graph.values()):\n",
    "            return i\n",
    "\n",
    "    # Fallback: Choose the class with the highest total votes if no clear winner is found\n",
    "    total_votes = torch.sum(image_logits, axis=0)\n",
    "    return torch.argmax(total_votes).item()\n",
    "        \n",
    "predictions = []\n",
    "for image_logits in logits:\n",
    "    rp_winner = ranked_pairs_winner(image_logits)\n",
    "    predictions.append(integer_to_label[rp_winner])\n",
    "    \n",
    "rp_accuracy = balanced_accuracy_score(labels, predictions)\n",
    "rp_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893ceed5-3c48-49cf-a620-48048d13c4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def star_voting_winner(image_logits):\n",
    "    num_voters, num_classes = image_logits.shape\n",
    "\n",
    "    # Step 1: Sum the scores for each candidate\n",
    "    total_scores = torch.sum(image_logits, axis=0)\n",
    "\n",
    "    # Step 2: Find the two candidates with the highest total scores\n",
    "    top_two = torch.topk(total_scores, 2).indices\n",
    "\n",
    "    # Step 3: Runoff between the top two candidates\n",
    "    first_choice_votes = torch.sum(image_logits[:, top_two[0]] > image_logits[:, top_two[1]])\n",
    "    second_choice_votes = num_voters - first_choice_votes\n",
    "\n",
    "    # Determine the winner\n",
    "    if first_choice_votes > second_choice_votes:\n",
    "        return top_two[0].item()\n",
    "    else:\n",
    "        return top_two[1].item()\n",
    "        \n",
    "predictions = []\n",
    "for image_logits in logits:\n",
    "    star_winner = ranked_pairs_winner(image_logits)\n",
    "    predictions.append(integer_to_label[star_winner])\n",
    "    \n",
    "star_accuracy = balanced_accuracy_score(labels, predictions)\n",
    "star_accuracy"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": ".m114",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/:m114"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
