{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "130095c9-7341-4a68-9468-55180d2dd817",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'tma_from_start'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82500098-49ea-4a7e-bb63-ce5130f954cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>label</th>\n",
       "      <th>image_width</th>\n",
       "      <th>image_height</th>\n",
       "      <th>is_tma</th>\n",
       "      <th>tile_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>HGSC</td>\n",
       "      <td>23785</td>\n",
       "      <td>20008</td>\n",
       "      <td>False</td>\n",
       "      <td>../tiles_768/4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>66</td>\n",
       "      <td>LGSC</td>\n",
       "      <td>48871</td>\n",
       "      <td>48195</td>\n",
       "      <td>False</td>\n",
       "      <td>../tiles_768/66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>91</td>\n",
       "      <td>HGSC</td>\n",
       "      <td>3388</td>\n",
       "      <td>3388</td>\n",
       "      <td>True</td>\n",
       "      <td>../tiles_768/91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>281</td>\n",
       "      <td>LGSC</td>\n",
       "      <td>42309</td>\n",
       "      <td>15545</td>\n",
       "      <td>False</td>\n",
       "      <td>../tiles_768/281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>286</td>\n",
       "      <td>EC</td>\n",
       "      <td>37204</td>\n",
       "      <td>30020</td>\n",
       "      <td>False</td>\n",
       "      <td>../tiles_768/286</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   image_id label  image_width  image_height  is_tma         tile_path\n",
       "0         4  HGSC        23785         20008   False    ../tiles_768/4\n",
       "1        66  LGSC        48871         48195   False   ../tiles_768/66\n",
       "2        91  HGSC         3388          3388    True   ../tiles_768/91\n",
       "3       281  LGSC        42309         15545   False  ../tiles_768/281\n",
       "4       286    EC        37204         30020   False  ../tiles_768/286"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def get_image_path(image_id:int):\n",
    "    return os.path.join('../tiles_768', str(image_id))\n",
    "\n",
    "train = pd.read_csv(f\"../data/train.csv\")\n",
    "\n",
    "train['tile_path'] = train['image_id'].apply(lambda x: get_image_path(x))\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d782638-2c6c-4b63-87b3-c7bf291bbde4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda and model timm/eva02_base_patch14_448.mim_in22k_ft_in22k_in1k\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import timm\n",
    "from timm.models.layers import DropPath\n",
    "import copy\n",
    "from itertools import cycle\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_name = \"timm/eva02_base_patch14_448.mim_in22k_ft_in22k_in1k\"\n",
    "\n",
    "print(f\"Using device {device} and model {model_name}\")\n",
    "\n",
    "model = timm.create_model(model_name, pretrained=True)\n",
    "\n",
    "# 25000 cutmix_mixup was 0.3 and 0.1\n",
    "drop_path_rate = 0.5\n",
    "dropout_rate = 0.\n",
    "head_dropout_rate = 0.3\n",
    "drop_path_rates = [x.item() for x in torch.linspace(0, drop_path_rate, len(model.blocks))]\n",
    "\n",
    "# Assign drop path rates\n",
    "for i, block in enumerate(model.blocks):\n",
    "    block.drop_path1 = DropPath(drop_prob=drop_path_rates[i])\n",
    "    block.drop_path2 = DropPath(drop_prob=drop_path_rates[i])\n",
    "    block.attn.attn_drop = nn.Dropout(p=dropout_rate, inplace=False)\n",
    "    block.attn.proj_drop = nn.Dropout(p=dropout_rate, inplace=False)\n",
    "    block.mlp.drop1 = nn.Dropout(p=dropout_rate, inplace=False)\n",
    "    block.mlp.drop2 = nn.Dropout(p=dropout_rate, inplace=False)\n",
    "\n",
    "model.head = nn.Linear(model.head.in_features, 5)\n",
    "model.pos_drop = nn.Dropout(dropout_rate)\n",
    "model.head_drop = nn.Dropout(head_dropout_rate)\n",
    "\n",
    "# model_location = 'eva02_base_models_tma_special_pt_2/ema_0.9999_step_50000.pth' # chosen because this is 1 epoch's worth of data\n",
    "# state_dict = torch.load(model_location, map_location=device)\n",
    "# model.load_state_dict(state_dict, strict=False)\n",
    "model = model.to(device)\n",
    "\n",
    "# Initialize EMA model\n",
    "ema_decays = [0.999, 0.9995, 0.9998, 0.9999, 0.99995, 0.99998, 0.99999]\n",
    "ema_models = [copy.deepcopy(model) for _ in range(len(ema_decays))]\n",
    "for i_ema, ema_model in enumerate(ema_models):\n",
    "    ema_model = ema_model.to(device)\n",
    "    ema_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82668844-c38f-42a1-8ed9-17098321ad5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "integer_to_label = {\n",
    "    0: 'HGSC',\n",
    "    1: 'CC',\n",
    "    2: 'EC',\n",
    "    3: 'LGSC',\n",
    "    4: 'MC',\n",
    "}\n",
    "\n",
    "label_to_integer = {\n",
    "    'HGSC': 0,\n",
    "    'CC': 1,\n",
    "    'EC': 2,\n",
    "    'LGSC': 3,\n",
    "    'MC': 4,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9613dea-4edf-47cd-a042-5f3468b19d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.transform = transform\n",
    "        self.all_images = []  # Store all images in an interlaced fashion\n",
    "        self.wsi_label_images = [[] for _ in range(5)]\n",
    "        self.tma_label_images = [[] for _ in range(5)]\n",
    "\n",
    "        # Step 1: Collect all images from each folder\n",
    "        for index, row in dataframe.iterrows():\n",
    "            folder_path = row['tile_path']\n",
    "            label = row['label']\n",
    "            image_id = row['image_id']\n",
    "            is_tma = row['is_tma']\n",
    "            if os.path.isdir(folder_path):\n",
    "                image_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.lower().endswith('.png')]\n",
    "                if is_tma:\n",
    "                    self.tma_label_images[label_to_integer[label]].extend([(image_file, label, image_id, is_tma) for image_file in image_files])\n",
    "                else:\n",
    "                    self.wsi_label_images[label_to_integer[label]].extend([(image_file, label, image_id, is_tma) for image_file in image_files])\n",
    "\n",
    "        for i in range(5):\n",
    "            random.shuffle(self.tma_label_images[i])\n",
    "            random.shuffle(self.wsi_label_images[i])\n",
    "\n",
    "        # Step 3: Interlace the images, repeating data as needed\n",
    "        max_length = max(max(len(tma) for tma in self.tma_label_images), max(len(wsi) for wsi in self.wsi_label_images))\n",
    "        for i in range(max_length):\n",
    "            for label in range(5):\n",
    "                if len(self.tma_label_images[label]) > 0:\n",
    "                    tma_index = i % len(self.tma_label_images[label])  # Repeat TMA data\n",
    "                    self.all_images.append(self.tma_label_images[label][tma_index])\n",
    "                if len(self.wsi_label_images[label]) > 0:\n",
    "                    wsi_index = i % len(self.wsi_label_images[label])  # Repeat WSI data\n",
    "                    self.all_images.append(self.wsi_label_images[label][wsi_index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return 1_000_000_000\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path, label, image_id, is_tma = self.all_images[idx]\n",
    "        image = Image.open(image_path)\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label_to_integer[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "daab3db2-8fff-4b14-83f9-608c256dea11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.v2 as v2\n",
    "from torch.utils.data import default_collate\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(size=448, scale=(0.5, 1.0), ratio=(0.75, 1.33)),\n",
    "    transforms.RandAugment(9, 15, 31),\n",
    "    transforms.Resize(448),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[\n",
    "        0.48145466,\n",
    "        0.4578275,\n",
    "        0.40821073\n",
    "    ], std=[\n",
    "        0.26862954,\n",
    "        0.26130258,\n",
    "        0.27577711\n",
    "    ]),\n",
    "    # transforms.RandomErasing(p=0.5, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=0, inplace=False),\n",
    "    # transforms.RandomErasing(p=0.5, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=0, inplace=False),\n",
    "    # transforms.RandomErasing(p=0.5, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=0, inplace=False),\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize(448),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[\n",
    "        0.48145466,\n",
    "        0.4578275,\n",
    "        0.40821073\n",
    "    ], std=[\n",
    "        0.26862954,\n",
    "        0.26130258,\n",
    "        0.27577711\n",
    "    ]),\n",
    "])\n",
    "\n",
    "train_dataset = ImageDataset(dataframe=train, transform=train_transform)\n",
    "\n",
    "cutmix = v2.CutMix(num_classes=5)\n",
    "mixup = v2.MixUp(num_classes=5)\n",
    "cutmix_or_mixup = v2.RandomChoice([cutmix, mixup])\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return cutmix_or_mixup(*default_collate(batch))\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, num_workers=7, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0aa0e2bd-4c21-48a1-a0fe-5eb15fb556b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "# Get the root logger\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# Optional: Remove all existing handlers from the logger\n",
    "for handler in logger.handlers[:]:\n",
    "    logger.removeHandler(handler)\n",
    "\n",
    "# Set the logging level\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Create a FileHandler and add it to the logger\n",
    "file_handler = logging.FileHandler(f'logs/eva02_base_train_{MODEL_NAME}.txt')\n",
    "file_handler.setLevel(logging.INFO)\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "# Create a StreamHandler for stderr and add it to the logger\n",
    "stream_handler = logging.StreamHandler(sys.stderr)\n",
    "stream_handler.setLevel(logging.ERROR)  # Only log ERROR and CRITICAL messages to stderr\n",
    "logger.addHandler(stream_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f9b938-9860-4147-be7e-f6f36938f212",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import logging\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "import random\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "initial_lr = 3e-6\n",
    "final_lr = 3e-8\n",
    "num_epochs = 10000\n",
    "\n",
    "# Function for linear warmup\n",
    "def learning_rate(step, warmup_steps=500, max_steps=50000):\n",
    "    if step < warmup_steps:\n",
    "        return initial_lr * (float(step) / float(max(1, warmup_steps)))\n",
    "    elif step < max_steps:\n",
    "        progress = (float(step - warmup_steps) / float(max(1, max_steps - warmup_steps)))\n",
    "        cos_component = 0.5 * (1 + math.cos(math.pi * progress))\n",
    "        return final_lr + (initial_lr - final_lr) * cos_component\n",
    "    else:\n",
    "        return final_lr\n",
    "\n",
    "def update_ema_variables(model, ema_model, alpha, global_step):\n",
    "    # Update the EMA model parameters\n",
    "    with torch.no_grad():\n",
    "        for ema_param, param in zip(ema_model.parameters(), model.parameters()):\n",
    "            ema_param.data.mul_(alpha).add_(param.data, alpha=1 - alpha)\n",
    "\n",
    "scaler = GradScaler()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=initial_lr, weight_decay=1e-7)\n",
    "\n",
    "# Define the weighted loss function\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "best_val_accuracy = 0.0\n",
    "step = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # set the model to training mode\n",
    "    \n",
    "    for i, (images, labels) in enumerate(train_dataloader, 0):\n",
    "        # Convert images to PIL format\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Linearly increase the learning rate\n",
    "        lr = learning_rate(step)\n",
    "        for g in optimizer.param_groups:\n",
    "            # g['lr'] = g['lr'] * lr / initial_lr\n",
    "            g['lr'] = lr\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass with autocast\n",
    "        with autocast():\n",
    "            outputs = model(images)\n",
    "            logits_per_image = outputs\n",
    "            loss = criterion(logits_per_image, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        for i_ema, ema_model in enumerate(ema_models):\n",
    "            update_ema_variables(model, ema_model, ema_decays[i_ema], step)\n",
    "\n",
    "        logging.info('[%d, %5d] loss: %.3f' % (epoch + 1, step, loss.item()))\n",
    "\n",
    "        if step % 1000 == 0:\n",
    "            for i_ema, ema_model in enumerate(ema_models):\n",
    "                torch.save(ema_model.state_dict(), f'eva02_base_models_{MODEL_NAME}/ema_{ema_decays[i_ema]}_step_{step}.pth')\n",
    "            logging.info(f'Models saved after epoch {epoch} and step {step}')\\\n",
    "\n",
    "            model.train()\n",
    "\n",
    "        if step == 50000:\n",
    "            break\n",
    "\n",
    "        step += 1\n",
    "\n",
    "    if step >= 50000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02074fd9-314d-47c1-97be-b73dcdb26d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "def duplicate_ipynb_with_new_name(src_file_path, dest_dir, new_name):\n",
    "    \"\"\"\n",
    "    Duplicate an IPython notebook file to a new directory with a new file name.\n",
    "\n",
    "    Parameters:\n",
    "    src_file_path (str): The path of the source IPython notebook file.\n",
    "    dest_dir (str): The destination directory where the file will be copied.\n",
    "    new_name (str): The new file name for the duplicated notebook.\n",
    "\n",
    "    Returns:\n",
    "    str: The path of the duplicated file with the new name.\n",
    "    \"\"\"\n",
    "    # Check if the new name contains the '.ipynb' extension, add if not\n",
    "    if not new_name.endswith('.ipynb'):\n",
    "        new_name += '.ipynb'\n",
    "\n",
    "    # Creating the destination file path with the new name\n",
    "    dest_file_path = os.path.join(dest_dir, new_name)\n",
    "\n",
    "    # Copying the file to the new directory\n",
    "    shutil.copy(src_file_path, dest_file_path)\n",
    "\n",
    "    return dest_file_path\n",
    "\n",
    "src_file = \"eva02-base-finetune.ipynb\"\n",
    "dest_directory = \"notebook_history\"\n",
    "new_filename = f\"{MODEL_NAME}.ipynb\"\n",
    "duplicate_ipynb_with_new_name(src_file, dest_directory, new_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e262eef-93e9-46c1-a243-7b8ace18dbc2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd395355-ce58-405c-b6d8-bd889ca98fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "66\n",
      "91\n",
      "281\n",
      "286\n",
      "431\n",
      "706\n",
      "970\n",
      "1020\n",
      "1080\n",
      "1101\n",
      "1252\n",
      "1289\n",
      "1295\n",
      "1660\n",
      "1666\n",
      "1774\n",
      "1925\n",
      "1943\n",
      "1952\n",
      "2097\n",
      "2227\n",
      "2391\n",
      "2666\n",
      "2706\n",
      "2906\n",
      "3055\n",
      "3084\n",
      "3092\n",
      "3098\n",
      "3191\n",
      "3222\n",
      "3264\n",
      "3511\n",
      "3672\n",
      "3881\n",
      "3997\n",
      "4134\n",
      "4211\n",
      "4608\n",
      "4797\n",
      "4827\n",
      "4877\n",
      "4963\n",
      "5015\n",
      "5114\n",
      "5251\n",
      "5264\n",
      "5265\n",
      "5307\n",
      "5456\n",
      "5851\n",
      "5852\n",
      "5970\n",
      "5992\n",
      "6140\n",
      "6175\n",
      "6281\n",
      "6359\n",
      "6363\n",
      "6449\n",
      "6558\n",
      "6582\n",
      "6793\n",
      "6843\n",
      "6898\n",
      "6951\n",
      "7204\n",
      "7329\n",
      "7482\n",
      "7490\n",
      "7955\n",
      "8130\n",
      "8213\n",
      "8240\n",
      "8279\n",
      "8280\n",
      "8531\n",
      "8713\n",
      "8805\n",
      "8985\n",
      "9154\n",
      "9183\n",
      "9200\n",
      "9254\n",
      "9341\n",
      "9509\n",
      "9658\n",
      "9697\n",
      "10077\n",
      "10143\n",
      "10246\n",
      "10252\n",
      "10469\n",
      "10548\n",
      "10642\n",
      "10800\n",
      "10896\n",
      "11263\n",
      "11417\n",
      "11431\n",
      "11557\n",
      "11559\n",
      "12159\n",
      "12222\n",
      "12244\n",
      "12442\n",
      "12522\n",
      "12902\n",
      "13364\n",
      "13387\n",
      "13526\n",
      "13568\n",
      "13987\n",
      "14039\n",
      "14051\n",
      "14127\n",
      "14312\n",
      "14401\n",
      "14424\n",
      "14532\n",
      "14542\n",
      "14617\n",
      "15139\n",
      "15188\n",
      "15209\n",
      "15221\n",
      "15231\n",
      "15293\n",
      "15470\n",
      "15486\n",
      "15583\n",
      "15671\n",
      "15742\n",
      "15871\n",
      "15912\n",
      "16042\n",
      "16064\n",
      "16209\n",
      "16325\n",
      "16494\n",
      "16876\n",
      "16986\n",
      "17067\n",
      "17174\n",
      "17291\n",
      "17365\n",
      "17416\n",
      "17487\n",
      "17637\n",
      "17738\n",
      "17854\n",
      "18014\n",
      "18138\n",
      "18196\n",
      "18547\n",
      "18568\n",
      "18607\n",
      "18810\n",
      "18813\n",
      "18896\n",
      "18914\n",
      "18981\n",
      "19030\n",
      "19157\n",
      "19255\n",
      "19512\n",
      "19569\n",
      "20205\n",
      "20312\n",
      "20316\n",
      "20329\n",
      "20463\n",
      "20670\n",
      "20858\n",
      "20882\n",
      "21020\n",
      "21232\n",
      "21260\n",
      "21303\n",
      "21373\n",
      "21432\n",
      "21445\n",
      "21910\n",
      "21929\n",
      "22155\n",
      "22221\n",
      "22290\n",
      "22425\n",
      "22489\n",
      "22654\n",
      "22740\n",
      "22924\n",
      "23523\n",
      "23629\n",
      "23796\n",
      "24023\n",
      "24507\n",
      "24563\n",
      "24617\n",
      "24759\n",
      "24991\n",
      "25256\n",
      "25331\n",
      "25561\n",
      "25604\n",
      "25792\n",
      "25923\n",
      "25928\n",
      "26025\n",
      "26124\n",
      "26190\n",
      "26219\n",
      "26533\n",
      "26603\n",
      "26644\n",
      "26862\n",
      "26939\n",
      "26950\n",
      "27245\n",
      "27249\n",
      "27315\n",
      "27739\n",
      "27747\n",
      "27851\n",
      "27950\n",
      "28028\n",
      "28066\n",
      "28121\n",
      "28393\n",
      "28519\n",
      "28562\n",
      "28603\n",
      "28736\n",
      "28821\n",
      "28922\n",
      "29084\n",
      "29147\n",
      "29200\n",
      "29240\n",
      "29331\n",
      "29615\n",
      "29888\n",
      "29904\n",
      "29915\n",
      "30203\n",
      "30272\n",
      "30369\n",
      "30508\n",
      "30515\n",
      "30539\n",
      "30712\n",
      "30738\n",
      "30792\n",
      "30794\n",
      "30868\n",
      "30986\n",
      "31033\n",
      "31297\n",
      "31300\n",
      "31333\n",
      "31383\n",
      "31473\n",
      "31594\n",
      "31793\n",
      "32032\n",
      "32035\n",
      "32042\n",
      "32112\n",
      "32192\n",
      "32432\n",
      "32596\n",
      "32636\n",
      "33708\n",
      "33839\n",
      "33976\n",
      "33984\n",
      "34247\n",
      "34277\n",
      "34508\n",
      "34649\n",
      "34688\n",
      "34690\n",
      "34720\n",
      "34822\n",
      "34845\n",
      "35197\n",
      "35239\n",
      "35565\n",
      "35592\n",
      "35652\n",
      "35754\n",
      "35792\n",
      "35909\n",
      "35953\n",
      "35998\n",
      "36008\n",
      "36063\n",
      "36204\n",
      "36302\n",
      "36499\n",
      "36513\n",
      "36583\n",
      "36639\n",
      "36678\n",
      "36783\n",
      "37190\n",
      "37307\n",
      "37367\n",
      "37385\n",
      "37655\n",
      "38018\n",
      "38019\n",
      "38041\n",
      "38048\n",
      "38097\n",
      "38118\n",
      "38349\n",
      "38366\n",
      "38479\n",
      "38535\n",
      "38585\n",
      "38647\n",
      "38669\n",
      "38687\n",
      "38849\n",
      "38901\n",
      "38959\n",
      "39144\n",
      "39146\n",
      "39172\n",
      "39208\n",
      "39252\n",
      "39255\n",
      "39258\n",
      "39269\n",
      "39297\n",
      "39365\n",
      "39425\n",
      "39466\n",
      "39728\n",
      "39872\n",
      "39880\n",
      "39893\n",
      "39990\n",
      "40079\n",
      "40124\n",
      "40129\n",
      "40503\n",
      "40639\n",
      "40864\n",
      "40888\n",
      "41099\n",
      "41361\n",
      "41368\n",
      "41586\n",
      "41801\n",
      "42125\n",
      "42260\n",
      "42296\n",
      "42549\n",
      "42857\n",
      "43280\n",
      "43390\n",
      "43432\n",
      "43671\n",
      "43796\n",
      "43815\n",
      "43875\n",
      "43998\n",
      "44232\n",
      "44283\n",
      "44432\n",
      "44530\n",
      "44581\n",
      "44603\n",
      "44700\n",
      "44804\n",
      "44962\n",
      "44976\n",
      "45104\n",
      "45185\n",
      "45254\n",
      "45578\n",
      "45630\n",
      "45725\n",
      "45990\n",
      "46139\n",
      "46172\n",
      "46435\n",
      "46444\n",
      "46469\n",
      "46543\n",
      "46688\n",
      "46736\n",
      "46769\n",
      "46793\n",
      "46815\n",
      "47020\n",
      "47035\n",
      "47105\n",
      "47431\n",
      "47837\n",
      "47911\n",
      "47960\n",
      "47984\n",
      "48502\n",
      "48506\n",
      "48550\n",
      "48734\n",
      "48861\n",
      "48973\n",
      "49281\n",
      "49587\n",
      "49872\n",
      "49942\n",
      "49995\n",
      "50048\n",
      "50246\n",
      "50304\n",
      "50589\n",
      "50712\n",
      "50878\n",
      "50932\n",
      "50962\n",
      "51021\n",
      "51032\n",
      "51128\n",
      "51215\n",
      "51346\n",
      "51499\n",
      "51679\n",
      "51832\n",
      "51893\n",
      "52108\n",
      "52259\n",
      "52275\n",
      "52308\n",
      "52375\n",
      "52420\n",
      "52438\n",
      "52461\n",
      "52612\n",
      "52752\n",
      "52784\n",
      "52836\n",
      "52846\n",
      "52931\n",
      "53059\n",
      "53098\n",
      "53377\n",
      "53402\n",
      "53655\n",
      "53688\n",
      "53859\n",
      "53900\n",
      "54007\n",
      "54408\n",
      "54473\n",
      "54506\n",
      "54590\n",
      "54825\n",
      "54928\n",
      "54949\n",
      "54990\n",
      "55279\n",
      "55281\n",
      "55287\n",
      "55876\n",
      "56117\n",
      "56221\n",
      "56351\n",
      "56500\n",
      "56799\n",
      "56843\n",
      "56861\n",
      "56875\n",
      "56947\n",
      "56993\n",
      "57100\n",
      "57162\n",
      "57265\n",
      "57468\n",
      "57598\n",
      "57696\n",
      "57711\n",
      "58895\n",
      "58947\n",
      "58974\n",
      "59002\n",
      "59031\n",
      "59383\n",
      "59515\n",
      "59589\n",
      "59760\n",
      "59900\n",
      "60058\n",
      "60287\n",
      "60685\n",
      "60928\n",
      "60936\n",
      "60988\n",
      "61033\n",
      "61089\n",
      "61100\n",
      "61280\n",
      "61320\n",
      "61424\n",
      "61493\n",
      "61689\n",
      "61797\n",
      "61823\n",
      "61852\n",
      "61961\n",
      "62476\n",
      "62641\n",
      "62828\n",
      "63015\n",
      "63121\n",
      "63165\n",
      "63289\n",
      "63298\n",
      "63367\n",
      "63429\n",
      "63836\n",
      "63897\n",
      "63941\n",
      "64111\n",
      "64188\n",
      "64629\n",
      "64771\n",
      "64824\n",
      "64950\n",
      "65022\n",
      "65094\n",
      "65300\n",
      "65371\n",
      "65533\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "ema_model = model\n",
    "# state_dict = torch.load('eva02_base_models_tma_special/ema_0.9999_step_10000.pth', map_location=device)\n",
    "state_dict = torch.load('eva02_base_models_tma_special_pt_2/ema_0.9999_step_50000.pth', map_location=device)\n",
    "ema_model.load_state_dict(state_dict, strict=False)\n",
    "ema_model = ema_model.to(device)\n",
    "ema_model.eval()\n",
    "\n",
    "# Maximum number of tiles per image\n",
    "MAX_TILES_PER_IMAGE = 1\n",
    "\n",
    "# Maximum number of images per batch\n",
    "MAX_IMAGES_PER_BATCH = 8  # Adjust based on model capacity and memory constraints\n",
    "\n",
    "image_ids = []\n",
    "logits = []\n",
    "labels = []\n",
    "is_tmas = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Temporary storage for the current batch\n",
    "    batch_tiles = []\n",
    "    batch_image_ids = []\n",
    "    batch_labels = []\n",
    "    batch_is_tmas = []\n",
    "\n",
    "    for _, row in train.iterrows():\n",
    "        print(row['image_id'])\n",
    "        path = row['tile_path']\n",
    "        all_files = [f for f in os.listdir(path) if f.lower().endswith('.png')]\n",
    "        \n",
    "        # Randomly sample tiles from this image\n",
    "        sample_size = min(MAX_TILES_PER_IMAGE, len(all_files))\n",
    "        sampled_files = random.sample(all_files, sample_size)\n",
    "\n",
    "        image_tiles = []\n",
    "        for image_name in sampled_files:\n",
    "            image_path = os.path.join(path, image_name)\n",
    "            sub_image = Image.open(image_path)\n",
    "            tile = val_transform(sub_image).unsqueeze(0)\n",
    "            image_tiles.append(tile)\n",
    "\n",
    "        # Add this image's tiles to the batch\n",
    "        batch_tiles.append(torch.concat(image_tiles, dim=0))\n",
    "        batch_image_ids.append(row['image_id'])\n",
    "        batch_labels.append(row['label'])\n",
    "        batch_is_tmas.append(row['is_tma'])\n",
    "\n",
    "        # Process the batch if it's full or this is the last row\n",
    "        if len(batch_tiles) == MAX_IMAGES_PER_BATCH or row.equals(train.iloc[-1]):\n",
    "            # Concatenate tiles from each image in the batch\n",
    "            batch_input = torch.concat(batch_tiles, dim=0).to(device)\n",
    "\n",
    "            # Run the batch through the model\n",
    "            batch_output = ema_model(batch_input)\n",
    "\n",
    "            # Split the outputs back into per-image groups and store them\n",
    "            start = 0\n",
    "            for i, tiles in enumerate(batch_tiles):\n",
    "                end = start + tiles.shape[0]\n",
    "                logits.append(batch_output[start:end])\n",
    "                start = end\n",
    "\n",
    "            image_ids.extend(batch_image_ids)\n",
    "            labels.extend(batch_labels)\n",
    "            is_tmas.extend(batch_is_tmas)\n",
    "\n",
    "            # Reset the batch\n",
    "            batch_tiles = []\n",
    "            batch_image_ids = []\n",
    "            batch_labels = []\n",
    "            batch_is_tmas = []\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dabb5781-3306-452e-a3d7-eedbd54160a1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 0.9574833130600616, 0.9787416565300309)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 50000 tma pt 2 0.9999 16\n",
    "\n",
    "predictions = []\n",
    "for image_logits in logits:\n",
    "    summed_logits = image_logits.sum(dim=0)\n",
    "    \n",
    "    max_vote_key = summed_logits.argmax().cpu().item()\n",
    "    predictions.append(integer_to_label[max_vote_key])\n",
    "\n",
    "tma_accuracy, non_tma_accuracy = balanced_accuracy_score([label for i, label in enumerate(labels) if is_tmas[i]], [prediction for i, prediction in enumerate(predictions) if is_tmas[i]]), balanced_accuracy_score([label for i, label in enumerate(labels) if not is_tmas[i]], [prediction for i, prediction in enumerate(predictions) if not is_tmas[i]])\n",
    "tma_accuracy, non_tma_accuracy, (tma_accuracy + non_tma_accuracy) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c74271f-e8b3-43f8-ad0f-c2b2e6ee833c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 0.9579579847406763, 0.9789789923703381)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 50000 tma pt 2 0.9999 8\n",
    "\n",
    "predictions = []\n",
    "for image_logits in logits:\n",
    "    summed_logits = image_logits.sum(dim=0)\n",
    "    \n",
    "    max_vote_key = summed_logits.argmax().cpu().item()\n",
    "    predictions.append(integer_to_label[max_vote_key])\n",
    "\n",
    "tma_accuracy, non_tma_accuracy = balanced_accuracy_score([label for i, label in enumerate(labels) if is_tmas[i]], [prediction for i, prediction in enumerate(predictions) if is_tmas[i]]), balanced_accuracy_score([label for i, label in enumerate(labels) if not is_tmas[i]], [prediction for i, prediction in enumerate(predictions) if not is_tmas[i]])\n",
    "tma_accuracy, non_tma_accuracy, (tma_accuracy + non_tma_accuracy) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d890e1d5-9fe2-4531-a8df-761d7d3d4641",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 0.8604749089484631, 0.9302374544742316)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 50000 tma pt 2 0.9999 1\n",
    "\n",
    "predictions = []\n",
    "for image_logits in logits:\n",
    "    summed_logits = image_logits.sum(dim=0)\n",
    "    \n",
    "    max_vote_key = summed_logits.argmax().cpu().item()\n",
    "    predictions.append(integer_to_label[max_vote_key])\n",
    "\n",
    "tma_accuracy, non_tma_accuracy = balanced_accuracy_score([label for i, label in enumerate(labels) if is_tmas[i]], [prediction for i, prediction in enumerate(predictions) if is_tmas[i]]), balanced_accuracy_score([label for i, label in enumerate(labels) if not is_tmas[i]], [prediction for i, prediction in enumerate(predictions) if not is_tmas[i]])\n",
    "tma_accuracy, non_tma_accuracy, (tma_accuracy + non_tma_accuracy) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86beab4a-5c2f-4fd7-836d-f7a2446ca0ff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 0.8632343340268964, 0.9316171670134482)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 50000 tma from start 0.9999\n",
    "\n",
    "predictions = []\n",
    "for image_logits in logits:\n",
    "    summed_logits = image_logits.sum(dim=0)\n",
    "    \n",
    "    max_vote_key = summed_logits.argmax().cpu().item()\n",
    "    predictions.append(integer_to_label[max_vote_key])\n",
    "\n",
    "tma_accuracy, non_tma_accuracy = balanced_accuracy_score([label for i, label in enumerate(labels) if is_tmas[i]], [prediction for i, prediction in enumerate(predictions) if is_tmas[i]]), balanced_accuracy_score([label for i, label in enumerate(labels) if not is_tmas[i]], [prediction for i, prediction in enumerate(predictions) if not is_tmas[i]])\n",
    "tma_accuracy, non_tma_accuracy, (tma_accuracy + non_tma_accuracy) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8d9000db-2fce-499c-beb8-0f101bbfc34e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12902_False: Guessed EC was really EC | tensor([-0.4551, -1.7309,  3.3703, -2.5687,  1.3749])\n",
      "18014_False: Guessed CC was really CC | tensor([-1.4731,  5.9430,  1.7641, -3.1101, -2.2189])\n",
      "24617_False: Guessed HGSC was really HGSC | tensor([ 5.7943, -0.0595, -1.1560, -2.6102, -0.0116])\n",
      "39990_False: Guessed HGSC was really HGSC | tensor([ 4.9722, -0.5116, -0.9595, -0.1688, -0.7827])\n",
      "1925_False: Guessed HGSC was really HGSC | tensor([ 2.7392e+00,  1.9131e-01, -5.7958e-04, -1.4853e+00, -1.9090e-02])\n",
      "21232_False: Guessed HGSC was really EC | tensor([ 1.3298, -1.8386,  1.1597,  0.8315, -0.3904])\n",
      "6175_False: Guessed HGSC was really HGSC | tensor([ 5.6998,  1.1837, -2.0748, -1.7332, -0.4406])\n",
      "47960_False: Guessed CC was really CC | tensor([ 0.8279,  5.6552,  1.1812, -3.1329, -3.1216])\n",
      "36063_False: Guessed CC was really CC | tensor([-1.2599,  6.5254,  1.7078, -3.0790, -3.0787])\n",
      "53688_False: Guessed HGSC was really HGSC | tensor([ 4.3264e+00, -1.5734e-03, -2.6865e-01, -8.7101e-01, -1.1869e+00])\n",
      "7329_False: Guessed EC was really EC | tensor([-0.0094,  0.1123,  3.4997, -1.6264, -1.5716])\n",
      "31300_False: Guessed LGSC was really LGSC | tensor([-0.0232, -2.9085,  1.7838,  4.6537, -1.5790])\n",
      "36204_False: Guessed HGSC was really HGSC | tensor([ 2.7890,  0.1075,  0.5047, -0.6214, -1.2167])\n",
      "6359_False: Guessed CC was really CC | tensor([-1.4657,  6.3545,  1.9562, -2.8143, -2.9984])\n",
      "15871_False: Guessed HGSC was really HGSC | tensor([ 4.3236, -1.8712,  0.8329, -0.5143, -0.8942])\n",
      "40888_False: Guessed CC was really CC | tensor([-0.4074,  5.3314,  1.5320, -3.2243, -2.3633])\n",
      "23796_False: Guessed LGSC was really LGSC | tensor([ 0.9525, -1.7524, -0.0981,  3.7795, -0.7482])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 232\u001b[0m\n\u001b[1;32m    230\u001b[0m         image_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(image_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    231\u001b[0m \u001b[38;5;66;03m#         t0 = time.time()\u001b[39;00m\n\u001b[0;32m--> 232\u001b[0m         results \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    234\u001b[0m         average_result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m    235\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m method \u001b[38;5;129;01min\u001b[39;00m results:\n",
      "Cell \u001b[0;32mIn[36], line 171\u001b[0m, in \u001b[0;36mprocess_image\u001b[0;34m(image_path, batch_size, total_samples)\u001b[0m\n\u001b[1;32m    168\u001b[0m j, i \u001b[38;5;241m=\u001b[39m uniform_tiles[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m uniform_tiles[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 171\u001b[0m tile \u001b[38;5;241m=\u001b[39m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m768\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m768\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tile\u001b[38;5;241m.\u001b[39msize[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m768\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m tile\u001b[38;5;241m.\u001b[39msize[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m768\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m calculate_entropy(tile) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[1;32m    174\u001b[0m     tile \u001b[38;5;241m=\u001b[39m transform(tile)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/PIL/Image.py:1211\u001b[0m, in \u001b[0;36mImage.crop\u001b[0;34m(self, box)\u001b[0m\n\u001b[1;32m   1208\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m box[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m<\u001b[39m box[\u001b[38;5;241m1\u001b[39m]:\n\u001b[1;32m   1209\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCoordinate \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlower\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is less than \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mupper\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1211\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1212\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_crop(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mim, box))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/PIL/ImageFile.py:260\u001b[0m, in \u001b[0;36mImageFile.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m    255\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage file is truncated \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    256\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(b)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m bytes not processed)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    257\u001b[0m         )\n\u001b[1;32m    259\u001b[0m b \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m+\u001b[39m s\n\u001b[0;32m--> 260\u001b[0m n, err_code \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image, ImageFile\n",
    "import torch\n",
    "import timm\n",
    "from timm.models.layers import DropPath\n",
    "import pandas as pd\n",
    "import torchvision.transforms as transforms\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "from timm.layers import RotaryEmbeddingCat\n",
    "\n",
    "Image.MAX_IMAGE_PIXELS = None\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "# Set up the device and the model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load the CSV file\n",
    "test = pd.read_csv(\"../data/train.csv\")\n",
    "\n",
    "# Define the paths\n",
    "image_dir = \"../data/train_images\"\n",
    "model_locations = {\n",
    "    'all_the_data': \"eva02_base_models_tma_special_pt_2/ema_0.9999_step_50000.pth\",\n",
    "#     'all_the_data': \"/kaggle/input/eva02-cutmix/eva02-cutmix/cutmix_mixup_all_the_data/ema_0.9999_step_115000.pth\",\n",
    "}\n",
    "\n",
    "model_name = \"timm/eva02_base_patch14_448.mim_in22k_ft_in22k_in1k\"\n",
    "models = { fold: timm.create_model(model_name, pretrained=False) for fold in model_locations }\n",
    "NEW_IMG_SIZE = 672\n",
    "\n",
    "for fold in models:\n",
    "    \n",
    "    drop_path_rate = 0.5\n",
    "    dropout_rate = 0.\n",
    "    head_dropout_rate = 0.3\n",
    "    drop_path_rates = [x.item() for x in torch.linspace(0, drop_path_rate, len(models[fold].blocks))]\n",
    "\n",
    "    # Assign drop path rates\n",
    "    for i, block in enumerate(models[fold].blocks):\n",
    "        block.drop_path1 = DropPath(drop_prob=drop_path_rates[i])\n",
    "        block.drop_path2 = DropPath(drop_prob=drop_path_rates[i])\n",
    "        block.attn.attn_drop = nn.Dropout(p=dropout_rate, inplace=False)\n",
    "        block.attn.proj_drop = nn.Dropout(p=dropout_rate, inplace=False)\n",
    "        block.mlp.drop1 = nn.Dropout(p=dropout_rate, inplace=False)\n",
    "        block.mlp.drop2 = nn.Dropout(p=dropout_rate, inplace=False)\n",
    "\n",
    "    models[fold].head = nn.Linear(models[fold] .head.in_features, 5)\n",
    "    models[fold].pos_drop = nn.Dropout(dropout_rate)\n",
    "    models[fold].head_drop = nn.Dropout(head_dropout_rate)\n",
    "\n",
    "    # Load the weights\n",
    "    state_dict = torch.load(model_locations[fold], map_location=device)\n",
    "    models[fold].load_state_dict(state_dict, strict=False)\n",
    "    \n",
    "    ########################################################################################################################################\n",
    "\n",
    "    # Calculate the size of the grid of patches\n",
    "    num_patches_side = NEW_IMG_SIZE // 14\n",
    "    num_patches = num_patches_side ** 2\n",
    "\n",
    "    # Extract the original positional embeddings, excluding the class token\n",
    "    pos_embed = models[fold].pos_embed\n",
    "    old_num_patches_side = int((pos_embed.size(1) - 1) ** 0.5)\n",
    "    pos_grid = pos_embed[:, 1:].reshape(1, old_num_patches_side, old_num_patches_side, -1)\n",
    "    pos_grid = pos_grid.permute(0, 3, 1, 2).contiguous()\n",
    "\n",
    "    # Resize using bilinear interpolation (make sure to keep the embedding dimension unchanged)\n",
    "    new_pos_grid = F.interpolate(pos_grid, size=(num_patches_side, num_patches_side), mode='bilinear', align_corners=False)\n",
    "\n",
    "    # Flatten the grid back to a sequence and re-add the class token\n",
    "    new_pos_embed = torch.cat([pos_embed[:, :1], new_pos_grid.permute(0, 2, 3, 1).contiguous().view(1, num_patches_side * num_patches_side, -1)], dim=1)\n",
    "\n",
    "    # Update the positional embeddings\n",
    "    models[fold].pos_embed = nn.Parameter(new_pos_embed)\n",
    "    models[fold].patch_embed.img_size = (NEW_IMG_SIZE, NEW_IMG_SIZE)\n",
    "    models[fold].patch_embed.grid_size = (NEW_IMG_SIZE // 14, NEW_IMG_SIZE // 14)\n",
    "    \n",
    "    models[fold].rope = RotaryEmbeddingCat(\n",
    "        768 // 12,\n",
    "        in_pixels=False,\n",
    "        feat_shape=models[fold].patch_embed.grid_size,\n",
    "        ref_feat_shape=(16, 16),\n",
    "    )\n",
    "\n",
    "    ########################################################################################################################################\n",
    "    \n",
    "    models[fold] = models[fold].to(device)\n",
    "    models[fold] = models[fold].eval()\n",
    "\n",
    "    \n",
    "\n",
    "def calculate_entropy(tile):\n",
    "    # Convert to grayscale if the image is RGB\n",
    "    if tile.mode == 'RGB':\n",
    "        tile = tile.convert('L')\n",
    "\n",
    "    # Flatten the tile and calculate histogram\n",
    "    pixel_counts = np.histogram(tile, bins=range(256))[0]\n",
    "\n",
    "    # Normalize to get probabilities\n",
    "    probabilities = pixel_counts / np.sum(pixel_counts)\n",
    "\n",
    "    # Filter out zero probabilities and calculate entropy\n",
    "    probabilities = probabilities[probabilities > 0]\n",
    "    entropy = -np.sum(probabilities * np.log2(probabilities))\n",
    "\n",
    "    return entropy\n",
    "\n",
    "integer_to_label = {\n",
    "    0: 'HGSC',\n",
    "    1: 'CC',\n",
    "    2: 'EC',\n",
    "    3: 'LGSC',\n",
    "    4: 'MC',\n",
    "}\n",
    "\n",
    "label_to_integer = {\n",
    "    'HGSC': 0,\n",
    "    'CC': 1,\n",
    "    'EC': 2,\n",
    "    'LGSC': 3,\n",
    "    'MC': 4,\n",
    "}\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(NEW_IMG_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[\n",
    "        0.48145466,\n",
    "        0.4578275,\n",
    "        0.40821073\n",
    "    ], std=[\n",
    "        0.26862954,\n",
    "        0.26130258,\n",
    "        0.27577711\n",
    "    ]),\n",
    "])\n",
    "\n",
    "def process_image(image_path, batch_size=16, total_samples=64):\n",
    "    random.seed(0)\n",
    "    with Image.open(image_path) as img:\n",
    "        width, height = img.size\n",
    "\n",
    "        # Define uniform tiles\n",
    "        total_tiles_to_choose_from = 512\n",
    "        uniform_tiles = [(j, i) for j in range(0, width - 768, int((((width - 768) ** 2) / total_tiles_to_choose_from) ** 0.5)) for i in range(0, height - 768, int((((height - 768) ** 2) / total_tiles_to_choose_from) ** 0.5))]\n",
    "        # uniform_tiles = [(j, i) for j in range(0, width, int((((width - 768) ** 2) / total_tiles_to_choose_from) ** 0.5)) for i in range(0, (height - 768), int(((width ** 2) / total_tiles_to_choose_from) ** 0.5))]\n",
    "        random.shuffle(uniform_tiles)\n",
    "\n",
    "        # Initialize statistics tensors\n",
    "        sum_logits = torch.zeros(5).to(device)\n",
    "        sum_probabilities = torch.zeros(5).to(device)\n",
    "        sum_log_probabilities = torch.zeros(5).to(device)\n",
    "        sum_log_neg_probabilities = torch.zeros(5).to(device)\n",
    "        max_logits = torch.full((5,), float('-inf')).to(device)\n",
    "        max_probabilities = torch.full((5,), float('-inf')).to(device)\n",
    "        total_tiles_processed = 0\n",
    "\n",
    "        # Prepare a list to hold image tiles\n",
    "        batch_tiles = []\n",
    "\n",
    "        while total_tiles_processed < total_samples and len(uniform_tiles) > 0:\n",
    "            if len(batch_tiles) < batch_size:\n",
    "                j, i = uniform_tiles[0]\n",
    "                del uniform_tiles[0]\n",
    "\n",
    "                tile = img.crop((j, i, j + 768, i + 768))\n",
    "\n",
    "                print(tile.size)\n",
    "                if tile.size[0] == 768 and tile.size[1] == 768 and calculate_entropy(tile) > 3:\n",
    "                    tile = transform(tile).unsqueeze(0)\n",
    "                    batch_tiles.append(tile)\n",
    "\n",
    "#             # Process tiles in batches\n",
    "            if len(batch_tiles) == batch_size or total_tiles_processed + len(batch_tiles) == total_samples or len(uniform_tiles) == 0:\n",
    "                batch_tiles_tensor = torch.cat(batch_tiles, dim=0).to(device)\n",
    "                \n",
    "                for fold in models:\n",
    "                    outputs = models[fold](batch_tiles_tensor)\n",
    "                    logits = outputs\n",
    "                    probs = logits.softmax(dim=1)\n",
    "                    sum_logits += logits.sum(dim=0) / len(models)\n",
    "                    sum_probabilities += probs.sum(dim=0) / len(models)\n",
    "                    sum_log_probabilities += torch.log(probs).sum(dim=0) / len(models)\n",
    "                    sum_log_neg_probabilities += torch.log(1 - probs).sum(dim=0) / len(models)\n",
    "                    max_logits = torch.max(max_logits, logits.max(dim=0)[0])\n",
    "                    max_probabilities = torch.max(max_probabilities, probs.max(dim=0)[0])\n",
    "                \n",
    "                # Reset the batch\n",
    "                total_tiles_processed += len(batch_tiles)\n",
    "                batch_tiles = []\n",
    "\n",
    "    return {\n",
    "        'average_logits': sum_logits / total_tiles_processed, \n",
    "        'average_probabilities': sum_probabilities / total_tiles_processed, \n",
    "        'average_log_probabilities': sum_log_probabilities / total_tiles_processed, \n",
    "        'average_log_neg_probabilities': sum_log_neg_probabilities / total_tiles_processed, \n",
    "        'max_logits': max_logits, \n",
    "        'max_probabilities': max_probabilities,\n",
    "    }\n",
    "\n",
    "\n",
    "# Initialize a DataFrame to store the results\n",
    "submission = pd.DataFrame(columns=['image_id', 'label'])\n",
    "\n",
    "total = 0\n",
    "total_correct = 0\n",
    "# Loop over the image IDs and process the images\n",
    "shuffled_image_ids = test['image_id'].tolist()\n",
    "\n",
    "random.seed(0)\n",
    "random.shuffle(shuffled_image_ids)\n",
    "\n",
    "# Dictionary to store probabilities\n",
    "probabilities_dict = {}\n",
    "labels_dict = {}\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Process images and store their probabilities\n",
    "#     do_it = False\n",
    "    \n",
    "    for image_id in shuffled_image_ids:\n",
    "#         if not do_it:\n",
    "#             if image_id == 52308:\n",
    "#                 do_it = True\n",
    "#             continue\n",
    "        image_path = os.path.join(image_dir, f\"{image_id}.png\")\n",
    "#         t0 = time.time()\n",
    "        results = process_image(image_path)\n",
    "        \n",
    "        average_result = torch.zeros(5)\n",
    "        for method in results:\n",
    "            results[method] = results[method].cpu()\n",
    "            if method == 'average_log_neg_probabilities':\n",
    "                mean = (-results[method]).mean()\n",
    "                std = (-results[method]).std()\n",
    "                average_result += (-results[method] - mean) / std\n",
    "            else:\n",
    "                mean = results[method].mean()\n",
    "                std = results[method].std()\n",
    "                average_result += (results[method] - mean) / std\n",
    "\n",
    "        average_result = average_result / len(results)\n",
    "#         print(time.time() - t0)\n",
    "        \n",
    "        probabilities_dict[image_id] = (results['average_logits']).max().item()\n",
    "        labels_dict[image_id] = integer_to_label[(results['average_logits']).argmax().item()]\n",
    "        print(f\"{image_id}_{test[test['image_id'] == image_id]['is_tma'].item()}: Guessed {labels_dict[image_id]} was really {test[test['image_id'] == image_id]['label'].item()} | {results['average_logits']}\")\n",
    "\n",
    "sorted_probabilities = sorted(probabilities_dict.items(), key=lambda x: x[1])\n",
    "\n",
    "threshold_prob = sorted_probabilities[int(len(sorted_probabilities) * 0.25)][1]\n",
    "\n",
    "# Initialize a list to store tuples of (image_id, label)\n",
    "modified_labels = []\n",
    "\n",
    "# Iterate over sorted probabilities and create tuples of image_id and label\n",
    "for index, (image_id, probability) in enumerate(sorted_probabilities):\n",
    "    label = 'Other' if probability < threshold_prob else labels_dict[image_id]\n",
    "    modified_labels.append((image_id, label))\n",
    "\n",
    "# Create a new DataFrame from the list of tuples\n",
    "new_submission = pd.DataFrame(modified_labels, columns=['image_id', 'label'])\n",
    "\n",
    "# Concatenate this new DataFrame with the original submission DataFrame\n",
    "submission = pd.concat([submission, new_submission])\n",
    "\n",
    "# Ensure there are no duplicate entries and reset the index\n",
    "submission = submission.drop_duplicates(subset='image_id').reset_index(drop=True)\n",
    "\n",
    "# submission.to_csv(\"/kaggle/working/submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7f584ad7-23c7-4b3d-8cd0-1ad11ca44eb6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "total_tiles_to_choose_from = 512\n",
    "for width in range(0, 10000, 100):\n",
    "    for height in range(0, 10000, 100):\n",
    "        if width < 1000 or height < 1000:\n",
    "            continue\n",
    "        # print(int((((width - 768) ** 2) / total_tiles_to_choose_from) ** 0.5), int((((height - 768) ** 2) / total_tiles_to_choose_from) ** 0.5))\n",
    "        uniform_tiles = [(j, i) for j in range(0, width - 768, int((((width - 768) ** 2) / total_tiles_to_choose_from) ** 0.5)) for i in range(0, height - 768, int((((height - 768) ** 2) / total_tiles_to_choose_from) ** 0.5))]\n",
    "        # uniform_tiles = [(j, i) for j in range(0, width, int((((width - 768) ** 2) / total_tiles_to_choose_from) ** 0.5)) for i in range(0, (height - 768), int(((width ** 2) / total_tiles_to_choose_from) ** 0.5))]\n",
    "        # print(len(uniform_tiles))\n",
    "        max_0 = 0\n",
    "        max_1 = 0\n",
    "        for thing in uniform_tiles:\n",
    "            if thing[0] > max_0:\n",
    "                max_0 = thing[0]\n",
    "            if thing[1] > max_1:\n",
    "                max_1 = thing[1]\n",
    "        if len(uniform_tiles) < total_tiles_to_choose_from:\n",
    "            print(len(uniform_tiles), width, height)\n",
    "        if max_0 + 768 >= width or max_1 + 768 > height:\n",
    "            print(max_0, max_1, width, height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad49c447-36b2-4d9b-a3ee-ad519f43dd62",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- \n",
      "+++ \n",
      "@@ -25,7 +25,8 @@\n",
      " # Define the paths\n",
      " image_dir = \"/kaggle/input/UBC-OCEAN/test_images\"\n",
      " model_locations = {\n",
      "-    'all_the_data': \"/kaggle/input/eva02-tma-special/eva02-tma-special/ema_0.9999_step_10000.pth\",\n",
      "+    'all_the_data': \"/kaggle/input/eva02-tma-special/eva02-tma-special/pt_2/ema_0.9999_step_50000.pth\",\n",
      "+#     'all_the_data': \"/kaggle/input/eva02-cutmix/eva02-cutmix/cutmix_mixup_all_the_data/ema_0.9999_step_115000.pth\",\n",
      " }\n",
      " \n",
      " model_name = \"timm/eva02_base_patch14_448.mim_in22k_ft_in22k_in1k\"\n",
      "@@ -144,9 +145,9 @@\n",
      "     random.seed(0)\n",
      "     with Image.open(image_path) as img:\n",
      "         width, height = img.size\n",
      "-        \n",
      "+\n",
      "         # Define uniform tiles\n",
      "-        total_tiles_to_choose_from = 128\n",
      "+        total_tiles_to_choose_from = 512\n",
      "         uniform_tiles = [(j, i) for j in range(0, width, int((((width - 768) ** 2) / total_tiles_to_choose_from) ** 0.5)) for i in range(0, (height - 768), int(((width ** 2) / total_tiles_to_choose_from) ** 0.5))]\n",
      "         random.shuffle(uniform_tiles)\n",
      " \n",
      "@@ -169,7 +170,7 @@\n",
      " \n",
      "                 tile = img.crop((j, i, j + 768, i + 768))\n",
      " \n",
      "-                if tile.size[0] == 768 or tile.size[1] == 768 or calculate_entropy(tile) > 3:\n",
      "+                if tile.size[0] == 768 and tile.size[1] == 768 and calculate_entropy(tile) > 3:\n",
      "                     tile = transform(tile).unsqueeze(0)\n",
      "                     batch_tiles.append(tile)\n",
      "             \n",
      "@@ -191,7 +192,7 @@\n",
      "                 # Reset the batch\n",
      "                 total_tiles_processed += len(batch_tiles)\n",
      "                 batch_tiles = []\n",
      "-    \n",
      "+\n",
      "     return {\n",
      "         'average_logits': sum_logits / total_tiles_processed, \n",
      "         'average_probabilities': sum_probabilities / total_tiles_processed, \n",
      "@@ -247,7 +248,7 @@\n",
      "         \n",
      "         probabilities_dict[image_id] = (results['average_logits']).max().item()\n",
      "         labels_dict[image_id] = integer_to_label[(results['average_logits']).argmax().item()]\n",
      "-#         print(f\"{image_id}: Guessed {labels_dict[image_id]} was really {test[test['image_id'] == image_id]['label'].item()} | {results['average_logits']}\")\n",
      "+#         print(f\"{image_id}_{test[test['image_id'] == image_id]['is_tma'].item()}: Guessed {labels_dict[image_id]} was really {test[test['image_id'] == image_id]['label'].item()} | {results['average_logits']}\")\n",
      " \n",
      " sorted_probabilities = sorted(probabilities_dict.items(), key=lambda x: x[1])\n",
      " \n"
     ]
    }
   ],
   "source": [
    "import difflib\n",
    "\n",
    "def compare_strings(string1, string2):\n",
    "    lines1 = string1.splitlines()\n",
    "    lines2 = string2.splitlines()\n",
    "\n",
    "    diff = difflib.unified_diff(lines1, lines2, lineterm='')\n",
    "\n",
    "    return list(diff)\n",
    "\n",
    "differences = compare_strings(string_a, string_b)\n",
    "for diff in differences:\n",
    "    print(diff)\n"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": ".m114",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/:m114"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
