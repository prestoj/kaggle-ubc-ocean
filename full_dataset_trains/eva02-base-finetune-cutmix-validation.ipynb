{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82500098-49ea-4a7e-bb63-ce5130f954cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>label</th>\n",
       "      <th>image_width</th>\n",
       "      <th>image_height</th>\n",
       "      <th>is_tma</th>\n",
       "      <th>tile_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>HGSC</td>\n",
       "      <td>23785</td>\n",
       "      <td>20008</td>\n",
       "      <td>False</td>\n",
       "      <td>../tiles_768/4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>66</td>\n",
       "      <td>LGSC</td>\n",
       "      <td>48871</td>\n",
       "      <td>48195</td>\n",
       "      <td>False</td>\n",
       "      <td>../tiles_768/66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>91</td>\n",
       "      <td>HGSC</td>\n",
       "      <td>3388</td>\n",
       "      <td>3388</td>\n",
       "      <td>True</td>\n",
       "      <td>../tiles_768/91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>281</td>\n",
       "      <td>LGSC</td>\n",
       "      <td>42309</td>\n",
       "      <td>15545</td>\n",
       "      <td>False</td>\n",
       "      <td>../tiles_768/281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>286</td>\n",
       "      <td>EC</td>\n",
       "      <td>37204</td>\n",
       "      <td>30020</td>\n",
       "      <td>False</td>\n",
       "      <td>../tiles_768/286</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   image_id label  image_width  image_height  is_tma         tile_path\n",
       "0         4  HGSC        23785         20008   False    ../tiles_768/4\n",
       "1        66  LGSC        48871         48195   False   ../tiles_768/66\n",
       "2        91  HGSC         3388          3388    True   ../tiles_768/91\n",
       "3       281  LGSC        42309         15545   False  ../tiles_768/281\n",
       "4       286    EC        37204         30020   False  ../tiles_768/286"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def get_image_path(image_id:int):\n",
    "    return os.path.join('../tiles_768', str(image_id))\n",
    "\n",
    "# I_FOLD = 1\n",
    "# validation_df = pd.read_csv(f\"../folds/val_fold_{I_FOLD}.csv\")\n",
    "validation_df = pd.read_csv(f\"../data/train.csv\")\n",
    "\n",
    "validation_df['tile_path'] = validation_df['image_id'].apply(lambda x: get_image_path(x))\n",
    "validation_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d782638-2c6c-4b63-87b3-c7bf291bbde4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cpu and model timm/eva02_base_patch14_448.mim_in22k_ft_in22k_in1k\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import timm\n",
    "from timm.models.layers import DropPath\n",
    "import copy\n",
    "from itertools import cycle\n",
    "\n",
    "# device = \"cuda\"\n",
    "device = \"cpu\"\n",
    "model_name = \"timm/eva02_base_patch14_448.mim_in22k_ft_in22k_in1k\"\n",
    "\n",
    "print(f\"Using device {device} and model {model_name}\")\n",
    "\n",
    "model = timm.create_model(model_name, pretrained=True)\n",
    "\n",
    "drop_path_rate = 0.5\n",
    "dropout_rate = 0.\n",
    "head_dropout_rate = 0.3\n",
    "drop_path_rates = [x.item() for x in torch.linspace(0, drop_path_rate, len(model.blocks))]\n",
    "\n",
    "# Assign drop path rates\n",
    "for i, block in enumerate(model.blocks):\n",
    "    block.drop_path1 = DropPath(drop_prob=drop_path_rates[i])\n",
    "    block.drop_path2 = DropPath(drop_prob=drop_path_rates[i])\n",
    "    block.attn.attn_drop = nn.Dropout(p=dropout_rate, inplace=False)\n",
    "    block.attn.proj_drop = nn.Dropout(p=dropout_rate, inplace=False)\n",
    "    block.mlp.drop1 = nn.Dropout(p=dropout_rate, inplace=False)\n",
    "    block.mlp.drop2 = nn.Dropout(p=dropout_rate, inplace=False)\n",
    "\n",
    "model.head = nn.Linear(model.head.in_features, 5)\n",
    "model.pos_drop = nn.Dropout(dropout_rate)\n",
    "model.head_drop = nn.Dropout(head_dropout_rate)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Initialize EMA model\n",
    "ema_decays = [0.999, 0.9995, 0.9998, 0.9999, 0.99995, 0.99998, 0.99999]\n",
    "# ema_decays = [0.99, 0.995, 0.998, 0.999, 0.9995, 0.9998, 0.9999]\n",
    "# model_name = \"cutmix_mixup_different_sampling\"\n",
    "# model_name = \"cutmix_mixup_final_try_all_the_data\"\n",
    "# model_name = \"cutmix_mixup_final_try_fold\"\n",
    "# model_name = \"cutmix_mixup_high_reg\"\n",
    "model_name = \"tma_special_pt_2\"\n",
    "model_step = 26000\n",
    "model_locations = [\n",
    "    f'eva02_base_models_{model_name}/ema_{ema_decays[0]}_step_{model_step}.pth',\n",
    "    f'eva02_base_models_{model_name}/ema_{ema_decays[1]}_step_{model_step}.pth',\n",
    "    f'eva02_base_models_{model_name}/ema_{ema_decays[2]}_step_{model_step}.pth',\n",
    "    f'eva02_base_models_{model_name}/ema_{ema_decays[3]}_step_{model_step}.pth',\n",
    "    f'eva02_base_models_{model_name}/ema_{ema_decays[4]}_step_{model_step}.pth',\n",
    "    f'eva02_base_models_{model_name}/ema_{ema_decays[5]}_step_{model_step}.pth',\n",
    "    f'eva02_base_models_{model_name}/ema_{ema_decays[6]}_step_{model_step}.pth',\n",
    "]\n",
    "# ema_decay = 0.9995\n",
    "# model_locations = [\n",
    "#     f'eva02_base_models_{model_name}/ema_{ema_decay}_step_53000.pth',\n",
    "#     f'eva02_base_models_{model_name}/ema_{ema_decay}_step_43000.pth',\n",
    "#     f'eva02_base_models_{model_name}/ema_{ema_decay}_step_33000.pth',\n",
    "#     f'eva02_base_models_{model_name}/ema_{ema_decay}_step_23000.pth',\n",
    "#     f'eva02_base_models_{model_name}/ema_{ema_decay}_step_13000.pth',\n",
    "#     f'eva02_base_models_{model_name}/ema_{ema_decay}_step_3000.pth',\n",
    "# ]\n",
    "# model_name = \"cutmix_mixup_third_try\"\n",
    "# model_locations = [\n",
    "#     f'eva02_base_models_{model_name}/ema_0.9998_step_10000.pth',\n",
    "#     f'eva02_base_models_{model_name}/ema_0.9999_step_20000.pth',\n",
    "#     f'eva02_base_models_{model_name}/ema_0.99995_step_30000.pth',\n",
    "#     f'eva02_base_models_{model_name}/ema_0.99998_step_50000.pth',\n",
    "# ]\n",
    "ema_models = [copy.deepcopy(model) for _ in range(len(model_locations))]\n",
    "for i, ema_model in enumerate(ema_models):\n",
    "    state_dict = torch.load(model_locations[i], map_location=device)\n",
    "    ema_model.load_state_dict(state_dict, strict=False)\n",
    "    ema_model = ema_model.to(device)\n",
    "    ema_model.eval()\n",
    "\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82668844-c38f-42a1-8ed9-17098321ad5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "integer_to_label = {\n",
    "    0: 'HGSC',\n",
    "    1: 'CC',\n",
    "    2: 'EC',\n",
    "    3: 'LGSC',\n",
    "    4: 'MC',\n",
    "}\n",
    "\n",
    "label_to_integer = {\n",
    "    'HGSC': 0,\n",
    "    'CC': 1,\n",
    "    'EC': 2,\n",
    "    'LGSC': 3,\n",
    "    'MC': 4,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9613dea-4edf-47cd-a042-5f3468b19d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.transform = transform\n",
    "        self.all_images = []  # Store all images in an interlaced fashion\n",
    "        self.wsi_label_images = [[] for _ in range(5)]\n",
    "        self.tma_label_images = [[] for _ in range(5)]\n",
    "\n",
    "        # Step 1: Collect all images from each folder\n",
    "        for index, row in dataframe.iterrows():\n",
    "            folder_path = row['tile_path']\n",
    "            label = row['label']\n",
    "            image_id = row['image_id']\n",
    "            is_tma = row['is_tma']\n",
    "            if os.path.isdir(folder_path):\n",
    "                image_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.lower().endswith('.png')]\n",
    "                if is_tma:\n",
    "                    self.tma_label_images[label_to_integer[label]].extend([(image_file, label, image_id, is_tma) for image_file in image_files])\n",
    "                else:\n",
    "                    self.wsi_label_images[label_to_integer[label]].extend([(image_file, label, image_id, is_tma) for image_file in image_files])\n",
    "\n",
    "        for i in range(5):\n",
    "            random.shuffle(self.tma_label_images[i])\n",
    "            random.shuffle(self.wsi_label_images[i])\n",
    "\n",
    "        # Step 3: Interlace the images, repeating data as needed\n",
    "        max_length = max(max(len(tma) for tma in self.tma_label_images), max(len(wsi) for wsi in self.wsi_label_images))\n",
    "        for i in range(max_length):\n",
    "            for label in range(5):\n",
    "                if len(self.tma_label_images[label]) > 0:\n",
    "                    tma_index = i % len(self.tma_label_images[label])  # Repeat TMA data\n",
    "                    self.all_images.append(self.tma_label_images[label][tma_index])\n",
    "                if len(self.wsi_label_images[label]) > 0:\n",
    "                    wsi_index = i % len(self.wsi_label_images[label])  # Repeat WSI data\n",
    "                    self.all_images.append(self.wsi_label_images[label][wsi_index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return 1_000_000_000\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path, label, image_id, is_tma = self.all_images[idx]\n",
    "        image = Image.open(image_path)\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label_to_integer[label], image_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "daab3db2-8fff-4b14-83f9-608c256dea11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.v2 as v2\n",
    "from torch.utils.data import default_collate\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "validation_transform = transforms.Compose([\n",
    "    transforms.Resize(448),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[\n",
    "        0.48145466,\n",
    "        0.4578275,\n",
    "        0.40821073\n",
    "    ], std=[\n",
    "        0.26862954,\n",
    "        0.26130258,\n",
    "        0.27577711\n",
    "    ]),\n",
    "])\n",
    "\n",
    "validation_dataset = ImageDataset(dataframe=validation_df, transform=validation_transform)\n",
    "\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=BATCH_SIZE, num_workers=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10bc31f8-540f-4af3-91a3-b56725dc9b3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def class_wise_accuracy_score(actual_classes, predicted_classes):\n",
    "    # Identify all unique classes in both actual and predicted classes\n",
    "    all_classes = sorted(set(actual_classes) | set(predicted_classes))\n",
    "\n",
    "    # Compute confusion matrix with all classes included\n",
    "    matrix = confusion_matrix(actual_classes, predicted_classes, labels=all_classes)\n",
    "    \n",
    "    # Convert to DataFrame for better readability\n",
    "    matrix_df = pd.DataFrame(matrix, index=all_classes, columns=all_classes)\n",
    "    \n",
    "    # Calculate class-wise accuracy and round to the nearest thousandth\n",
    "    # Use np.nan_to_num to handle division by zero\n",
    "    class_accuracies = np.round(np.nan_to_num(matrix.diagonal() / matrix.sum(axis=1)), 3)\n",
    "    \n",
    "    # Create a dictionary to hold class and its corresponding accuracy\n",
    "    accuracy_dict = dict(zip(all_classes, class_accuracies))            \n",
    "    \n",
    "    return accuracy_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08f9b938-9860-4147-be7e-f6f36938f212",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Model 0 | 0.111 | 1.000 | {0: 1.0, 1: 1.0, 2: 1.0, 3: 1.0, 4: 1.0}\n",
      "Model 1 | 0.111 | 1.000 | {0: 1.0, 1: 1.0, 2: 1.0, 3: 1.0, 4: 1.0}\n",
      "Model 2 | 0.111 | 1.000 | {0: 1.0, 1: 1.0, 2: 1.0, 3: 1.0, 4: 1.0}\n",
      "Model 3 | 0.118 | 1.000 | {0: 1.0, 1: 1.0, 2: 1.0, 3: 1.0, 4: 1.0}\n",
      "Model 4 | 0.173 | 1.000 | {0: 1.0, 1: 1.0, 2: 1.0, 3: 1.0, 4: 1.0}\n",
      "Model 5 | 0.538 | 0.850 | {0: 0.25, 1: 1.0, 2: 1.0, 3: 1.0, 4: 1.0}\n",
      "Model 6 | 0.720 | 0.750 | {0: 0.25, 1: 1.0, 2: 1.0, 3: 0.5, 4: 1.0}\n",
      "1\n",
      "Model 0 | 0.084 | 1.000 | {0: 1.0, 1: 1.0, 2: 1.0, 3: 1.0, 4: 1.0}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 39\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast():\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m model_index, model \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(ema_models):\n\u001b[0;32m---> 39\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m         logits_per_image \u001b[38;5;241m=\u001b[39m outputs\n\u001b[1;32m     41\u001b[0m         loss \u001b[38;5;241m=\u001b[39m criterion(logits_per_image, labels)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/timm/models/eva.py:580\u001b[0m, in \u001b[0;36mEva.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 580\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    581\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_head(x)\n\u001b[1;32m    582\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/timm/models/eva.py:568\u001b[0m, in \u001b[0;36mEva.forward_features\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    566\u001b[0m         x \u001b[38;5;241m=\u001b[39m checkpoint(blk, x, rope\u001b[38;5;241m=\u001b[39mrot_pos_embed)\n\u001b[1;32m    567\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 568\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mblk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrope\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrot_pos_embed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    569\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(x)\n\u001b[1;32m    570\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/timm/models/eva.py:237\u001b[0m, in \u001b[0;36mEvaBlock.forward\u001b[0;34m(self, x, rope, attn_mask)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma_1 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    236\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_path1(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x), rope\u001b[38;5;241m=\u001b[39mrope, attn_mask\u001b[38;5;241m=\u001b[39mattn_mask))\n\u001b[0;32m--> 237\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_path2(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    239\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_path1(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma_1 \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x), rope\u001b[38;5;241m=\u001b[39mrope, attn_mask\u001b[38;5;241m=\u001b[39mattn_mask))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/timm/layers/mlp.py:140\u001b[0m, in \u001b[0;36mSwiGLU.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    139\u001b[0m     x_gate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1_g(x)\n\u001b[0;32m--> 140\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1_x\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(x_gate) \u001b[38;5;241m*\u001b[39m x\n\u001b[1;32m    142\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop1(x)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import logging\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "import random\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "# # Assuming the existence of 'ema_models', 'validation_dataloader', and 'device' from your context\n",
    "# # Class counts: {'CC': 146576, 'EC': 165037, 'HGSC': 272477, 'LGSC': 46346, 'MC': 79036}\n",
    "# class_counts = torch.tensor([272477, 146576, 165037, 46346, 79036], dtype=torch.float32)\n",
    "\n",
    "# # Calculate weights: Inverse of class frequencies\n",
    "# weights = 1.0 / class_counts\n",
    "# weights = weights / weights.sum()  # Normalize to make the sum of weights equal to 1\n",
    "# weights = weights.to(device)  # Move weights to the device (CPU/GPU)\n",
    "\n",
    "# Define the weighted loss function\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "step = 0\n",
    "cumulative_losses = [0 for _ in range(len(ema_models))]  # Cumulative loss for each model\n",
    "model_steps = [0 for _ in range(len(ema_models))]  # Number of steps for each model\n",
    "\n",
    "# Initialize lists for all predictions and true labels\n",
    "all_predictions = [[] for _ in range(len(ema_models))]\n",
    "all_labels = [[] for _ in range(len(ema_models))]\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (images, labels, _) in enumerate(validation_dataloader, 0):\n",
    "        print(i)\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass with autocast\n",
    "        with autocast():\n",
    "            for model_index, model in enumerate(ema_models):\n",
    "                outputs = model(images)\n",
    "                logits_per_image = outputs\n",
    "                loss = criterion(logits_per_image, labels)\n",
    "\n",
    "                # Predictions for balanced accuracy\n",
    "                predictions = torch.argmax(logits_per_image, dim=1)\n",
    "\n",
    "                cumulative_losses[model_index] += loss.item()\n",
    "                model_steps[model_index] += 1\n",
    "\n",
    "                # Store predictions and labels for balanced accuracy calculation\n",
    "                all_predictions[model_index].extend(predictions.cpu().numpy())\n",
    "                all_labels[model_index].extend(labels.cpu().numpy())\n",
    "\n",
    "                # Calculate running average loss for this model\n",
    "                running_avg_loss = cumulative_losses[model_index] / model_steps[model_index]\n",
    "\n",
    "                # Calculate current balanced accuracy score\n",
    "                current_balanced_acc_score = balanced_accuracy_score(all_labels[model_index], all_predictions[model_index])\n",
    "                current_class_acc_score = class_wise_accuracy_score(all_labels[model_index], all_predictions[model_index])\n",
    "                print(f\"Model {model_index} | {running_avg_loss:.3f} | {current_balanced_acc_score:.3f} | {current_class_acc_score}\")\n",
    "\n",
    "        step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cee5356-b5b3-4771-981f-95a4ffce7dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 50 10000 tma\n",
    "# Model 0 | 0.304 | 0.888 | {0: 0.811, 1: 0.902, 2: 0.823, 3: 0.938, 4: 0.963}\n",
    "# Model 1 | 0.304 | 0.888 | {0: 0.811, 1: 0.902, 2: 0.823, 3: 0.938, 4: 0.963}\n",
    "# Model 2 | 0.304 | 0.888 | {0: 0.811, 1: 0.902, 2: 0.823, 3: 0.938, 4: 0.963}\n",
    "# Model 3 | 0.305 | 0.888 | {0: 0.811, 1: 0.902, 2: 0.823, 3: 0.938, 4: 0.963}\n",
    "# Model 4 | 0.305 | 0.885 | {0: 0.811, 1: 0.902, 2: 0.811, 3: 0.938, 4: 0.963}\n",
    "# Model 5 | 0.299 | 0.885 | {0: 0.811, 1: 0.896, 2: 0.817, 3: 0.938, 4: 0.963}\n",
    "# Model 6 | 0.303 | 0.891 | {0: 0.817, 1: 0.896, 2: 0.829, 3: 0.957, 4: 0.957}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c677422-e1d3-40bc-9f41-82c2a5e015ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 50 5000 tma\n",
    "# Model 0 | 0.320 | 0.895 | {0: 0.811, 1: 0.927, 2: 0.848, 3: 0.944, 4: 0.944}\n",
    "# Model 1 | 0.319 | 0.899 | {0: 0.811, 1: 0.927, 2: 0.86, 3: 0.951, 4: 0.944}\n",
    "# Model 2 | 0.318 | 0.895 | {0: 0.793, 1: 0.927, 2: 0.86, 3: 0.951, 4: 0.944}\n",
    "# Model 3 | 0.317 | 0.895 | {0: 0.787, 1: 0.927, 2: 0.866, 3: 0.951, 4: 0.944}\n",
    "# Model 4 | 0.314 | 0.889 | {0: 0.787, 1: 0.915, 2: 0.841, 3: 0.951, 4: 0.951}\n",
    "# Model 5 | 0.318 | 0.888 | {0: 0.75, 1: 0.915, 2: 0.872, 3: 0.951, 4: 0.951}\n",
    "# Model 6 | 0.349 | 0.883 | {0: 0.756, 1: 0.921, 2: 0.872, 3: 0.914, 4: 0.951}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2e3d30-6993-49ca-8457-527c89868454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 50 1000 tma\n",
    "# Model 0 | 0.343 | 0.872 | {0: 0.829, 1: 0.89, 2: 0.701, 3: 0.969, 4: 0.969}\n",
    "# Model 1 | 0.346 | 0.878 | {0: 0.866, 1: 0.89, 2: 0.701, 3: 0.963, 4: 0.969}\n",
    "# Model 2 | 0.339 | 0.879 | {0: 0.86, 1: 0.909, 2: 0.713, 3: 0.963, 4: 0.951}\n",
    "# Model 3 | 0.338 | 0.874 | {0: 0.799, 1: 0.909, 2: 0.774, 3: 0.938, 4: 0.951}\n",
    "# Model 4 | 0.361 | 0.883 | {0: 0.762, 1: 0.915, 2: 0.841, 3: 0.938, 4: 0.957}\n",
    "# Model 5 | 0.408 | 0.830 | {0: 0.524, 1: 0.878, 2: 0.854, 3: 0.938, 4: 0.957}\n",
    "# Model 6 | 0.434 | 0.819 | {0: 0.463, 1: 0.878, 2: 0.854, 3: 0.944, 4: 0.957}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cc61c7-17ac-4cdf-9912-dde2469806f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 50 44000 all the data\n",
    "# Model 0 | 0.484 | 0.818 | {0: 0.445, 1: 0.878, 2: 0.86, 3: 0.944, 4: 0.963}\n",
    "# Model 1 | 0.489 | 0.812 | {0: 0.421, 1: 0.872, 2: 0.86, 3: 0.944, 4: 0.963}\n",
    "# Model 2 | 0.491 | 0.812 | {0: 0.421, 1: 0.878, 2: 0.86, 3: 0.938, 4: 0.963}\n",
    "# Model 3 | 0.451 | 0.820 | {0: 0.506, 1: 0.884, 2: 0.848, 3: 0.907, 4: 0.957}\n",
    "# Model 4 | 0.407 | 0.869 | {0: 0.756, 1: 0.921, 2: 0.799, 3: 0.901, 4: 0.969}\n",
    "# Model 5 | 0.522 | 0.804 | {0: 0.811, 1: 0.915, 2: 0.671, 3: 0.821, 4: 0.802}\n",
    "# Model 6 | 0.825 | 0.694 | {0: 0.713, 1: 0.915, 2: 0.683, 3: 0.506, 4: 0.654}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be43bca5-0f83-46de-87d0-aac2d7b05ffc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 72 20000 all the data\n",
    "# Model 0 | 0.529 | 0.833 | {0: 0.808, 1: 0.923, 2: 0.615, 3: 0.859, 4: 0.961}\n",
    "# Model 1 | 0.524 | 0.834 | {0: 0.812, 1: 0.927, 2: 0.615, 3: 0.855, 4: 0.961}\n",
    "# Model 2 | 0.529 | 0.834 | {0: 0.846, 1: 0.923, 2: 0.59, 3: 0.85, 4: 0.961}\n",
    "# Model 3 | 0.561 | 0.807 | {0: 0.88, 1: 0.906, 2: 0.585, 3: 0.812, 4: 0.853}\n",
    "# Model 4 | 0.687 | 0.735 | {0: 0.795, 1: 0.902, 2: 0.607, 3: 0.65, 4: 0.72}\n",
    "# Model 5 | 1.063 | 0.619 | {0: 0.577, 1: 0.906, 2: 0.62, 3: 0.376, 4: 0.616}\n",
    "# Model 6 | 1.412 | 0.438 | {0: 0.064, 1: 0.803, 2: 0.509, 3: 0.265, 4: 0.547}"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": ".m114",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/:m114"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
