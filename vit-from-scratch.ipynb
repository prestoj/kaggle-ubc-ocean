{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "099870d7-e05f-48cc-aa6d-f7492207a5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "config = {\n",
    "    'image_size': 224,\n",
    "    'patch_size': 32,\n",
    "    'num_classes': 5,\n",
    "    'dim': 768,\n",
    "    'depth': 12,\n",
    "    'heads': 12,\n",
    "    'mlp_dim': 3072\n",
    "}\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, in_channels=3, patch_size=16, emb_size=768):\n",
    "        super().__init__()\n",
    "        self.patch_embed = nn.Conv2d(in_channels, emb_size, kernel_size=patch_size, stride=patch_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.patch_embed(x)  # [B, C, H, W]\n",
    "        x = x.flatten(2)  # [B, C, H*W]\n",
    "        x = x.transpose(1, 2)  # [B, H*W, C]\n",
    "        return x\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads=12):\n",
    "        super().__init__()\n",
    "        self.heads = heads\n",
    "        self.scale = dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.heads, C // self.heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv.unbind(0)   # make torchscript happy (cannot use tensor as tuple)\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        return x\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, out_features, p=0.1):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, heads, mlp_ratio=4., qkv_bias=True, p=0., attn_p=0.):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim, eps=1e-6)\n",
    "        self.attn = Attention(dim, heads=heads)\n",
    "        self.drop_path = nn.Dropout(p)\n",
    "        self.norm2 = nn.LayerNorm(dim, eps=1e-6)\n",
    "        self.mlp = MLP(in_features=dim, hidden_features=int(dim * mlp_ratio), out_features=dim)\n",
    "        self.mlp_drop = nn.Dropout(p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.drop_path(self.attn(self.norm1(x)))\n",
    "        x = x + self.mlp_drop(self.mlp(self.norm2(x)))\n",
    "        return x\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.patch_embed = PatchEmbedding(patch_size=config['patch_size'], emb_size=config['dim'])\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, config['dim']))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, (config['image_size'] // config['patch_size']) ** 2 + 1, config['dim']))\n",
    "        self.pos_drop = nn.Dropout(p=0.1)\n",
    "\n",
    "        self.blocks = nn.Sequential(*[Block(dim=config['dim'], heads=config['heads']) for _ in range(config['depth'])])\n",
    "        self.norm = nn.LayerNorm(config['dim'], eps=1e-6)\n",
    "        self.head = nn.Linear(config['dim'], config['num_classes'])\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.shape[0]\n",
    "        x = self.patch_embed(x)\n",
    "\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x = x + self.pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        x = self.blocks(x)\n",
    "        x = self.norm(x)\n",
    "\n",
    "        cls_token_final = x[:, 0]\n",
    "        x = self.head(cls_token_final)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0870dae1-56bb-477a-b8b5-9149971ab704",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>label</th>\n",
       "      <th>image_width</th>\n",
       "      <th>image_height</th>\n",
       "      <th>is_tma</th>\n",
       "      <th>tile_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>38366</td>\n",
       "      <td>LGSC</td>\n",
       "      <td>31951</td>\n",
       "      <td>21718</td>\n",
       "      <td>False</td>\n",
       "      <td>tiles/38366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>63298</td>\n",
       "      <td>HGSC</td>\n",
       "      <td>26067</td>\n",
       "      <td>20341</td>\n",
       "      <td>False</td>\n",
       "      <td>tiles/63298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>54928</td>\n",
       "      <td>CC</td>\n",
       "      <td>36166</td>\n",
       "      <td>31487</td>\n",
       "      <td>False</td>\n",
       "      <td>tiles/54928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18813</td>\n",
       "      <td>CC</td>\n",
       "      <td>54671</td>\n",
       "      <td>32443</td>\n",
       "      <td>False</td>\n",
       "      <td>tiles/18813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>63429</td>\n",
       "      <td>EC</td>\n",
       "      <td>67783</td>\n",
       "      <td>29066</td>\n",
       "      <td>False</td>\n",
       "      <td>tiles/63429</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   image_id label  image_width  image_height  is_tma    tile_path\n",
       "0     38366  LGSC        31951         21718   False  tiles/38366\n",
       "1     63298  HGSC        26067         20341   False  tiles/63298\n",
       "2     54928    CC        36166         31487   False  tiles/54928\n",
       "3     18813    CC        54671         32443   False  tiles/18813\n",
       "4     63429    EC        67783         29066   False  tiles/63429"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def get_image_path(image_id:int):\n",
    "    return os.path.join(\"tiles\", str(image_id))\n",
    "\n",
    "train = pd.read_csv(\"train-no-tma.csv\")\n",
    "val = pd.read_csv(\"validation-no-tma.csv\")\n",
    "\n",
    "train['tile_path'] = train['image_id'].apply(lambda x: get_image_path(x))\n",
    "val['tile_path'] = val['image_id'].apply(lambda x: get_image_path(x))\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f20a3425-6b7e-4096-9d03-1c96842d50e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import autoaugment\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image, ImageOps\n",
    "\n",
    "integer_to_label = {\n",
    "    0: 'HGSC',\n",
    "    1: 'CC',\n",
    "    2: 'EC',\n",
    "    3: 'LGSC',\n",
    "    4: 'MC',\n",
    "}\n",
    "\n",
    "label_to_integer = {\n",
    "    'HGSC': 0,\n",
    "    'CC': 1,\n",
    "    'EC': 2,\n",
    "    'LGSC': 3,\n",
    "    'MC': 4,\n",
    "}\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        for index, row in dataframe.iterrows():\n",
    "            folder_path = row['tile_path']\n",
    "            label = row['label']\n",
    "            if os.path.isdir(folder_path):  # Check if the folder_path is a valid directory\n",
    "                for image_name in os.listdir(folder_path):\n",
    "                    if image_name.lower().endswith('.png'):  # Check if the file is a PNG\n",
    "                        image_path = os.path.join(folder_path, image_name)\n",
    "                        self.image_paths.append(image_path)\n",
    "                        self.labels.append(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_paths[idx])\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label_to_integer[label]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3904119f-b3a3-4118-9db1-c84f1b385917",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torch.utils.data import DataLoader\n",
    "# from torchvision import transforms\n",
    "# from PIL import Image\n",
    "# import os\n",
    "\n",
    "# def calculate_dataset_stats(dataset, num_samples=10000):\n",
    "#     loader = DataLoader(dataset, batch_size=1, num_workers=4, shuffle=True)\n",
    "    \n",
    "#     mean = 0.\n",
    "#     std = 0.\n",
    "#     nb_samples = 0.\n",
    "    \n",
    "#     for i, (data, _) in enumerate(loader):\n",
    "#         data = data.view(data.size(0), data.size(1), -1)\n",
    "#         mean += data.mean(2).sum(0)\n",
    "#         std += data.var(2).sum(0)\n",
    "#         nb_samples += data.size(0)\n",
    "        \n",
    "#         if i >= num_samples:\n",
    "#             break\n",
    "\n",
    "#     mean /= nb_samples\n",
    "#     std /= nb_samples\n",
    "#     std = torch.sqrt(std)\n",
    "    \n",
    "#     return mean, std\n",
    "\n",
    "# mean, std = calculate_dataset_stats(train_dataset)\n",
    "# print(\"Mean:\", mean)\n",
    "# print(\"Standard Deviation:\", std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d91b165-596d-4b29-8d98-b391354a0820",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "\n",
    "\"\"\"# FOR YES TMA\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    autoaugment.RandAugment(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.6152, 0.5353, 0.5934], std=[0.2387, 0.2385, 0.2317]), # FOR YES TMA. calculated above\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.6152, 0.5353, 0.5934], std=[0.2387, 0.2385, 0.2317]), # FOR YES TMA. calculated above\n",
    "])\"\"\"\n",
    "\n",
    "\n",
    "# FOR NO TMA\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.8265, 0.7217, 0.8247], std=[0.1133, 0.1265, 0.0960]), # FOR NO TMA. calculated above\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.8265, 0.7217, 0.8247], std=[0.1133, 0.1265, 0.0960]), # FOR NO TMA. calculated above\n",
    "])\n",
    "\n",
    "train_dataset = ImageDataset(dataframe=train, transform=train_transform)\n",
    "val_dataset = ImageDataset(dataframe=val, transform=val_transform)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=128, num_workers=8, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=128, num_workers=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffa2b9e6-2f85-4bb1-a5bc-8f887c0ef29f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Total number of parameters: 87459077\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = VisionTransformer(config)\n",
    "model.to(device)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print('Total number of parameters:', total_params)\n",
    "\n",
    "# Calculate class weights\n",
    "class_counts = np.array([3521456, 1876772, 2126428, 589002, 1053114], dtype=np.float32) # These were derived by looking at the number of files in tile_path for each label\n",
    "# class_counts = np.array([703, 690, 631, 581, 706], dtype=np.float32) # These were derived by looking at the number of files in tile_path for each label\n",
    "class_weights = 1. / class_counts\n",
    "class_weights /= class_weights.sum()\n",
    "\n",
    "# Convert class weights to tensor\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "# Define the loss function with class weights\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5, weight_decay=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "737fb4b4-3cae-47dd-bf14-19849d60c95f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from torchsummary import summary\n",
    "\n",
    "# summary(model, input_size=(3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45af2131-3f19-4e7a-a0be-abd821aef060",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "# Get the root logger\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# Optional: Remove all existing handlers from the logger\n",
    "for handler in logger.handlers[:]:\n",
    "    logger.removeHandler(handler)\n",
    "\n",
    "# Set the logging level\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Create a FileHandler and add it to the logger\n",
    "file_handler = logging.FileHandler('training_log_vit_non_tma_pt_3.txt')\n",
    "file_handler.setLevel(logging.INFO)\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "# Create a StreamHandler for stderr and add it to the logger\n",
    "stream_handler = logging.StreamHandler(sys.stderr)\n",
    "stream_handler.setLevel(logging.ERROR)  # Only log ERROR and CRITICAL messages to stderr\n",
    "logger.addHandler(stream_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f78a2d09-da82-4a79-a482-4e87eeff0616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint = torch.load('vit-non-tma-models/model_step_16000.pt')\n",
    "# model.load_state_dict(checkpoint['model_state_dict'])\n",
    "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e323e20e-8868-43dc-921f-7c3d243b408e",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     27\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 28\u001b[0m current_loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m current_loss\n\u001b[1;32m     31\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, step: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Specify the directory to save the model checkpoints\n",
    "checkpoint_dir = \"vit-non-tma-models-pt-2/\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# Number of epochs\n",
    "num_epochs = 100\n",
    "step = 0\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for images, labels in train_dataloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        current_loss = loss.item()\n",
    "        \n",
    "        running_loss += current_loss\n",
    "        logging.info(f\"epoch: {epoch}, step: {step}, loss: {current_loss}\")\n",
    "        \n",
    "        if step % 1000 == 0:\n",
    "            checkpoint_filename = f\"{checkpoint_dir}model_step_{step}.pt\"\n",
    "            torch.save({\n",
    "                'step': step,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, checkpoint_filename)\n",
    "            \n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                correct = 0\n",
    "                total = 0\n",
    "                for images, labels in val_dataloader:\n",
    "                    images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "                    # Forward pass\n",
    "                    outputs = model(images)\n",
    "\n",
    "                    # Get predictions from the maximum value\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "                    # Total number of labels\n",
    "                    total += labels.size(0)\n",
    "\n",
    "                    # Total correct predictions\n",
    "                    correct += (predicted == labels).sum().item()\n",
    "                    if total >= 10000:\n",
    "                        break\n",
    "\n",
    "            # Calculate validation accuracy\n",
    "            val_accuracy = 100 * correct / total\n",
    "            logging.info(f\"epoch: {epoch}, step: {step}, validation accuracy: {val_accuracy:.4f}%\")\n",
    "            model.train()\n",
    "        \n",
    "        step += 1\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in val_dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "\n",
    "            # Get predictions from the maximum value\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            # Total number of labels\n",
    "            total += labels.size(0)\n",
    "\n",
    "            # Total correct predictions\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            if total >= 10000:\n",
    "                break\n",
    "\n",
    "    # Calculate validation accuracy\n",
    "    val_accuracy = 100 * correct / total\n",
    "    # logging.info(f\"epoch: {epoch}, step: {step}, validation accuracy: {val_accuracy:.4f}%\")\n",
    "    model.train()\n",
    "    epoch_loss = running_loss / len(train_dataloader)\n",
    "#     checkpoint_filename = f\"{checkpoint_dir}model_epoch{epoch}.pt\"\n",
    "#     torch.save({\n",
    "#         'epoch': epoch,\n",
    "#         'model_state_dict': model.state_dict(),\n",
    "#         'optimizer_state_dict': optimizer.state_dict(),\n",
    "#         'loss': epoch_loss,\n",
    "#     }, checkpoint_filename)\n",
    "    \n",
    "#     Validation step\n",
    "    logging.info(f'Epoch {epoch+1}/{num_epochs}, Steps: {step}, Loss: {epoch_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6b1ab4-191e-433e-b0f6-44865b6557d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_filename = f\"{checkpoint_dir}final.pt\"\n",
    "torch.save({\n",
    "    'step': step,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "}, checkpoint_filename)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.2-0.m112",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.2-0:m112"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
